
:======== Few-shot Seg. with CHM ========
|             datapath: ../Datasets_HPN         
|            benchmark: pascal                  
|              logpath: res101_pas_fold3        
|                  bsz: 20                      
|                   lr: 0.001                   
|                niter: 2000                    
|              nworker: 8                       
|                 load:                         
|                 fold: 3                       
|             backbone: resnet101               
:================================================

Backbone # param.: 44654608
Learner # param.: 2587394
Total # param.: 47242002
Using 4 GPUs (DataParallel)
[Epoch: 00] [Batch: 0001/0580] Loss: 0.68987  Avg Loss: 0.68987  Avg mIoU:   5.37  
[Epoch: 00] [Batch: 0051/0580] Loss: 0.44067  Avg Loss: 0.53127  Avg mIoU:   0.34  
[Epoch: 00] [Batch: 0101/0580] Loss: 0.39738  Avg Loss: 0.48041  Avg mIoU:  13.10  
[Epoch: 00] [Batch: 0151/0580] Loss: 0.48091  Avg Loss: 0.45586  Avg mIoU:  21.40  
[Epoch: 00] [Batch: 0201/0580] Loss: 0.44389  Avg Loss: 0.43806  Avg mIoU:  26.21  
[Epoch: 00] [Batch: 0251/0580] Loss: 0.38589  Avg Loss: 0.42633  Avg mIoU:  28.98  
[Epoch: 00] [Batch: 0301/0580] Loss: 0.38131  Avg Loss: 0.41684  Avg mIoU:  30.87  
[Epoch: 00] [Batch: 0351/0580] Loss: 0.37832  Avg Loss: 0.41095  Avg mIoU:  32.28  
[Epoch: 00] [Batch: 0401/0580] Loss: 0.52478  Avg Loss: 0.40513  Avg mIoU:  33.26  
[Epoch: 00] [Batch: 0451/0580] Loss: 0.33105  Avg Loss: 0.40161  Avg mIoU:  34.57  
[Epoch: 00] [Batch: 0501/0580] Loss: 0.46459  Avg Loss: 0.39698  Avg mIoU:  35.76  
[Epoch: 00] [Batch: 0551/0580] Loss: 0.23492  Avg Loss: 0.39181  Avg mIoU:  37.02  

*** Training [@Epoch 00] Avg Loss: 0.38876  Avg mIoU:  37.67  ***

[Epoch: 00] [Batch: 0001/0050] Loss: 0.26742  Avg Loss: 0.26742  Avg mIoU:  49.32  

*** Validation [@Epoch 00] Avg Loss: 0.36817  Avg mIoU:  48.30  ***

Model saved @0 w/ val. mIoU: 48.30.

[Epoch: 01] [Batch: 0001/0580] Loss: 0.30464  Avg Loss: 0.30464  Avg mIoU:  29.78  
[Epoch: 01] [Batch: 0051/0580] Loss: 0.32565  Avg Loss: 0.33795  Avg mIoU:  50.48  
[Epoch: 01] [Batch: 0101/0580] Loss: 0.32348  Avg Loss: 0.33106  Avg mIoU:  50.48  
[Epoch: 01] [Batch: 0151/0580] Loss: 0.28699  Avg Loss: 0.33064  Avg mIoU:  50.37  
[Epoch: 01] [Batch: 0201/0580] Loss: 0.35868  Avg Loss: 0.32747  Avg mIoU:  50.34  
[Epoch: 01] [Batch: 0251/0580] Loss: 0.33538  Avg Loss: 0.32576  Avg mIoU:  50.74  
[Epoch: 01] [Batch: 0301/0580] Loss: 0.34786  Avg Loss: 0.32347  Avg mIoU:  51.34  
[Epoch: 01] [Batch: 0351/0580] Loss: 0.32182  Avg Loss: 0.32051  Avg mIoU:  51.76  
[Epoch: 01] [Batch: 0401/0580] Loss: 0.28787  Avg Loss: 0.31992  Avg mIoU:  51.77  
[Epoch: 01] [Batch: 0451/0580] Loss: 0.31039  Avg Loss: 0.31984  Avg mIoU:  51.93  
[Epoch: 01] [Batch: 0501/0580] Loss: 0.33710  Avg Loss: 0.31764  Avg mIoU:  52.04  
[Epoch: 01] [Batch: 0551/0580] Loss: 0.31965  Avg Loss: 0.31565  Avg mIoU:  52.25  

*** Training [@Epoch 01] Avg Loss: 0.31510  Avg mIoU:  52.26  ***

[Epoch: 01] [Batch: 0001/0050] Loss: 0.28305  Avg Loss: 0.28305  Avg mIoU:  54.16  

*** Validation [@Epoch 01] Avg Loss: 0.34900  Avg mIoU:  51.43  ***

Model saved @1 w/ val. mIoU: 51.43.

[Epoch: 02] [Batch: 0001/0580] Loss: 0.32279  Avg Loss: 0.32279  Avg mIoU:  45.74  
[Epoch: 02] [Batch: 0051/0580] Loss: 0.29820  Avg Loss: 0.31964  Avg mIoU:  51.60  
[Epoch: 02] [Batch: 0101/0580] Loss: 0.28052  Avg Loss: 0.31039  Avg mIoU:  53.08  
[Epoch: 02] [Batch: 0151/0580] Loss: 0.21540  Avg Loss: 0.30718  Avg mIoU:  53.77  
[Epoch: 02] [Batch: 0201/0580] Loss: 0.28496  Avg Loss: 0.30548  Avg mIoU:  53.67  
[Epoch: 02] [Batch: 0251/0580] Loss: 0.26141  Avg Loss: 0.30430  Avg mIoU:  53.82  
[Epoch: 02] [Batch: 0301/0580] Loss: 0.33942  Avg Loss: 0.30122  Avg mIoU:  54.33  
[Epoch: 02] [Batch: 0351/0580] Loss: 0.29163  Avg Loss: 0.30186  Avg mIoU:  54.49  
[Epoch: 02] [Batch: 0401/0580] Loss: 0.28433  Avg Loss: 0.30265  Avg mIoU:  54.45  
[Epoch: 02] [Batch: 0451/0580] Loss: 0.34871  Avg Loss: 0.30155  Avg mIoU:  54.40  
[Epoch: 02] [Batch: 0501/0580] Loss: 0.26281  Avg Loss: 0.30138  Avg mIoU:  54.42  
[Epoch: 02] [Batch: 0551/0580] Loss: 0.27148  Avg Loss: 0.30064  Avg mIoU:  54.38  

*** Training [@Epoch 02] Avg Loss: 0.30032  Avg mIoU:  54.46  ***

[Epoch: 02] [Batch: 0001/0050] Loss: 0.25326  Avg Loss: 0.25326  Avg mIoU:  50.43  

*** Validation [@Epoch 02] Avg Loss: 0.32370  Avg mIoU:  49.75  ***

[Epoch: 03] [Batch: 0001/0580] Loss: 0.39280  Avg Loss: 0.39280  Avg mIoU:  37.15  
[Epoch: 03] [Batch: 0051/0580] Loss: 0.28694  Avg Loss: 0.29085  Avg mIoU:  56.86  
[Epoch: 03] [Batch: 0101/0580] Loss: 0.31070  Avg Loss: 0.28417  Avg mIoU:  56.31  
[Epoch: 03] [Batch: 0151/0580] Loss: 0.21502  Avg Loss: 0.28478  Avg mIoU:  55.97  
[Epoch: 03] [Batch: 0201/0580] Loss: 0.26707  Avg Loss: 0.28652  Avg mIoU:  56.28  
[Epoch: 03] [Batch: 0251/0580] Loss: 0.25925  Avg Loss: 0.28616  Avg mIoU:  56.35  
[Epoch: 03] [Batch: 0301/0580] Loss: 0.31178  Avg Loss: 0.28462  Avg mIoU:  56.69  
[Epoch: 03] [Batch: 0351/0580] Loss: 0.39895  Avg Loss: 0.28503  Avg mIoU:  56.50  
[Epoch: 03] [Batch: 0401/0580] Loss: 0.26454  Avg Loss: 0.28492  Avg mIoU:  56.49  
[Epoch: 03] [Batch: 0451/0580] Loss: 0.34655  Avg Loss: 0.28683  Avg mIoU:  56.55  
[Epoch: 03] [Batch: 0501/0580] Loss: 0.20558  Avg Loss: 0.28599  Avg mIoU:  56.38  
[Epoch: 03] [Batch: 0551/0580] Loss: 0.29612  Avg Loss: 0.28481  Avg mIoU:  56.44  

*** Training [@Epoch 03] Avg Loss: 0.28436  Avg mIoU:  56.54  ***

[Epoch: 03] [Batch: 0001/0050] Loss: 0.23192  Avg Loss: 0.23192  Avg mIoU:  53.16  

*** Validation [@Epoch 03] Avg Loss: 0.31049  Avg mIoU:  51.26  ***

[Epoch: 04] [Batch: 0001/0580] Loss: 0.24140  Avg Loss: 0.24140  Avg mIoU:  37.46  
[Epoch: 04] [Batch: 0051/0580] Loss: 0.34831  Avg Loss: 0.26560  Avg mIoU:  59.42  
[Epoch: 04] [Batch: 0101/0580] Loss: 0.26259  Avg Loss: 0.27446  Avg mIoU:  59.13  
[Epoch: 04] [Batch: 0151/0580] Loss: 0.28027  Avg Loss: 0.27351  Avg mIoU:  58.72  
[Epoch: 04] [Batch: 0201/0580] Loss: 0.20710  Avg Loss: 0.27377  Avg mIoU:  58.05  
[Epoch: 04] [Batch: 0251/0580] Loss: 0.24786  Avg Loss: 0.27311  Avg mIoU:  57.76  
[Epoch: 04] [Batch: 0301/0580] Loss: 0.30143  Avg Loss: 0.27277  Avg mIoU:  57.68  
[Epoch: 04] [Batch: 0351/0580] Loss: 0.40391  Avg Loss: 0.27237  Avg mIoU:  57.62  
[Epoch: 04] [Batch: 0401/0580] Loss: 0.17711  Avg Loss: 0.27454  Avg mIoU:  57.52  
[Epoch: 04] [Batch: 0451/0580] Loss: 0.20886  Avg Loss: 0.27260  Avg mIoU:  57.87  
[Epoch: 04] [Batch: 0501/0580] Loss: 0.19723  Avg Loss: 0.27364  Avg mIoU:  57.88  
[Epoch: 04] [Batch: 0551/0580] Loss: 0.21887  Avg Loss: 0.27381  Avg mIoU:  57.87  

*** Training [@Epoch 04] Avg Loss: 0.27337  Avg mIoU:  57.86  ***

[Epoch: 04] [Batch: 0001/0050] Loss: 0.23373  Avg Loss: 0.23373  Avg mIoU:  48.91  

*** Validation [@Epoch 04] Avg Loss: 0.29669  Avg mIoU:  49.33  ***

[Epoch: 05] [Batch: 0001/0580] Loss: 0.36670  Avg Loss: 0.36670  Avg mIoU:  30.02  
[Epoch: 05] [Batch: 0051/0580] Loss: 0.24121  Avg Loss: 0.27180  Avg mIoU:  58.52  
[Epoch: 05] [Batch: 0101/0580] Loss: 0.20676  Avg Loss: 0.27586  Avg mIoU:  57.70  
[Epoch: 05] [Batch: 0151/0580] Loss: 0.24720  Avg Loss: 0.26988  Avg mIoU:  57.82  
[Epoch: 05] [Batch: 0201/0580] Loss: 0.18946  Avg Loss: 0.26815  Avg mIoU:  58.17  
[Epoch: 05] [Batch: 0251/0580] Loss: 0.28443  Avg Loss: 0.26687  Avg mIoU:  58.63  
[Epoch: 05] [Batch: 0301/0580] Loss: 0.28126  Avg Loss: 0.26591  Avg mIoU:  58.50  
[Epoch: 05] [Batch: 0351/0580] Loss: 0.31180  Avg Loss: 0.26443  Avg mIoU:  58.69  
[Epoch: 05] [Batch: 0401/0580] Loss: 0.36959  Avg Loss: 0.26471  Avg mIoU:  58.82  
[Epoch: 05] [Batch: 0451/0580] Loss: 0.24272  Avg Loss: 0.26615  Avg mIoU:  58.45  
[Epoch: 05] [Batch: 0501/0580] Loss: 0.23613  Avg Loss: 0.26807  Avg mIoU:  58.22  
[Epoch: 05] [Batch: 0551/0580] Loss: 0.40551  Avg Loss: 0.26751  Avg mIoU:  58.50  

*** Training [@Epoch 05] Avg Loss: 0.26602  Avg mIoU:  58.71  ***

[Epoch: 05] [Batch: 0001/0050] Loss: 0.22611  Avg Loss: 0.22611  Avg mIoU:  56.45  

*** Validation [@Epoch 05] Avg Loss: 0.31200  Avg mIoU:  53.96  ***

Model saved @5 w/ val. mIoU: 53.96.

[Epoch: 06] [Batch: 0001/0580] Loss: 0.30500  Avg Loss: 0.30500  Avg mIoU:  28.97  
[Epoch: 06] [Batch: 0051/0580] Loss: 0.30342  Avg Loss: 0.26907  Avg mIoU:  56.56  
[Epoch: 06] [Batch: 0101/0580] Loss: 0.14383  Avg Loss: 0.26009  Avg mIoU:  57.79  
[Epoch: 06] [Batch: 0151/0580] Loss: 0.29665  Avg Loss: 0.25916  Avg mIoU:  58.40  
[Epoch: 06] [Batch: 0201/0580] Loss: 0.26283  Avg Loss: 0.25689  Avg mIoU:  58.63  
[Epoch: 06] [Batch: 0251/0580] Loss: 0.26143  Avg Loss: 0.25822  Avg mIoU:  58.85  
[Epoch: 06] [Batch: 0301/0580] Loss: 0.29995  Avg Loss: 0.25887  Avg mIoU:  59.04  
[Epoch: 06] [Batch: 0351/0580] Loss: 0.29767  Avg Loss: 0.25914  Avg mIoU:  59.10  
[Epoch: 06] [Batch: 0401/0580] Loss: 0.24740  Avg Loss: 0.25713  Avg mIoU:  59.41  
[Epoch: 06] [Batch: 0451/0580] Loss: 0.35875  Avg Loss: 0.25819  Avg mIoU:  59.28  
[Epoch: 06] [Batch: 0501/0580] Loss: 0.37889  Avg Loss: 0.25816  Avg mIoU:  59.48  
[Epoch: 06] [Batch: 0551/0580] Loss: 0.29975  Avg Loss: 0.25861  Avg mIoU:  59.40  

*** Training [@Epoch 06] Avg Loss: 0.25851  Avg mIoU:  59.40  ***

[Epoch: 06] [Batch: 0001/0050] Loss: 0.21026  Avg Loss: 0.21026  Avg mIoU:  57.09  

*** Validation [@Epoch 06] Avg Loss: 0.30525  Avg mIoU:  54.70  ***

Model saved @6 w/ val. mIoU: 54.70.

[Epoch: 07] [Batch: 0001/0580] Loss: 0.25493  Avg Loss: 0.25493  Avg mIoU:  35.63  
[Epoch: 07] [Batch: 0051/0580] Loss: 0.22568  Avg Loss: 0.24978  Avg mIoU:  58.99  
[Epoch: 07] [Batch: 0101/0580] Loss: 0.27259  Avg Loss: 0.24821  Avg mIoU:  59.69  
[Epoch: 07] [Batch: 0151/0580] Loss: 0.40027  Avg Loss: 0.25261  Avg mIoU:  58.89  
[Epoch: 07] [Batch: 0201/0580] Loss: 0.23767  Avg Loss: 0.25529  Avg mIoU:  58.63  
[Epoch: 07] [Batch: 0251/0580] Loss: 0.21930  Avg Loss: 0.25558  Avg mIoU:  58.95  
[Epoch: 07] [Batch: 0301/0580] Loss: 0.23399  Avg Loss: 0.25467  Avg mIoU:  59.02  
[Epoch: 07] [Batch: 0351/0580] Loss: 0.27694  Avg Loss: 0.25432  Avg mIoU:  59.05  
[Epoch: 07] [Batch: 0401/0580] Loss: 0.35004  Avg Loss: 0.25390  Avg mIoU:  59.41  
[Epoch: 07] [Batch: 0451/0580] Loss: 0.21788  Avg Loss: 0.25516  Avg mIoU:  59.36  
[Epoch: 07] [Batch: 0501/0580] Loss: 0.30696  Avg Loss: 0.25513  Avg mIoU:  59.43  
[Epoch: 07] [Batch: 0551/0580] Loss: 0.23345  Avg Loss: 0.25493  Avg mIoU:  59.63  

*** Training [@Epoch 07] Avg Loss: 0.25434  Avg mIoU:  59.73  ***

[Epoch: 07] [Batch: 0001/0050] Loss: 0.21607  Avg Loss: 0.21607  Avg mIoU:  54.75  

*** Validation [@Epoch 07] Avg Loss: 0.28866  Avg mIoU:  53.87  ***

[Epoch: 08] [Batch: 0001/0580] Loss: 0.24622  Avg Loss: 0.24622  Avg mIoU:  33.97  
[Epoch: 08] [Batch: 0051/0580] Loss: 0.23375  Avg Loss: 0.25046  Avg mIoU:  61.31  
[Epoch: 08] [Batch: 0101/0580] Loss: 0.17560  Avg Loss: 0.24580  Avg mIoU:  61.41  
[Epoch: 08] [Batch: 0151/0580] Loss: 0.33400  Avg Loss: 0.24934  Avg mIoU:  60.68  
[Epoch: 08] [Batch: 0201/0580] Loss: 0.19693  Avg Loss: 0.24933  Avg mIoU:  60.28  
[Epoch: 08] [Batch: 0251/0580] Loss: 0.25579  Avg Loss: 0.24909  Avg mIoU:  60.53  
[Epoch: 08] [Batch: 0301/0580] Loss: 0.31085  Avg Loss: 0.24806  Avg mIoU:  60.48  
[Epoch: 08] [Batch: 0351/0580] Loss: 0.20549  Avg Loss: 0.24806  Avg mIoU:  60.62  
[Epoch: 08] [Batch: 0401/0580] Loss: 0.31970  Avg Loss: 0.24986  Avg mIoU:  60.59  
[Epoch: 08] [Batch: 0451/0580] Loss: 0.17506  Avg Loss: 0.25038  Avg mIoU:  60.63  
[Epoch: 08] [Batch: 0501/0580] Loss: 0.26890  Avg Loss: 0.25139  Avg mIoU:  60.55  
[Epoch: 08] [Batch: 0551/0580] Loss: 0.24492  Avg Loss: 0.25136  Avg mIoU:  60.63  

*** Training [@Epoch 08] Avg Loss: 0.25171  Avg mIoU:  60.70  ***

[Epoch: 08] [Batch: 0001/0050] Loss: 0.21813  Avg Loss: 0.21813  Avg mIoU:  54.84  

*** Validation [@Epoch 08] Avg Loss: 0.28574  Avg mIoU:  53.02  ***

[Epoch: 09] [Batch: 0001/0580] Loss: 0.23377  Avg Loss: 0.23377  Avg mIoU:  29.38  
[Epoch: 09] [Batch: 0051/0580] Loss: 0.28000  Avg Loss: 0.26186  Avg mIoU:  60.16  
[Epoch: 09] [Batch: 0101/0580] Loss: 0.24932  Avg Loss: 0.25021  Avg mIoU:  61.40  
[Epoch: 09] [Batch: 0151/0580] Loss: 0.26954  Avg Loss: 0.24646  Avg mIoU:  61.47  
[Epoch: 09] [Batch: 0201/0580] Loss: 0.28004  Avg Loss: 0.24376  Avg mIoU:  61.91  
[Epoch: 09] [Batch: 0251/0580] Loss: 0.28496  Avg Loss: 0.24339  Avg mIoU:  62.11  
[Epoch: 09] [Batch: 0301/0580] Loss: 0.24468  Avg Loss: 0.24620  Avg mIoU:  61.71  
[Epoch: 09] [Batch: 0351/0580] Loss: 0.21742  Avg Loss: 0.24659  Avg mIoU:  61.71  
[Epoch: 09] [Batch: 0401/0580] Loss: 0.23610  Avg Loss: 0.24742  Avg mIoU:  61.71  
[Epoch: 09] [Batch: 0451/0580] Loss: 0.22213  Avg Loss: 0.24651  Avg mIoU:  61.77  
[Epoch: 09] [Batch: 0501/0580] Loss: 0.23550  Avg Loss: 0.24593  Avg mIoU:  61.73  
[Epoch: 09] [Batch: 0551/0580] Loss: 0.15366  Avg Loss: 0.24636  Avg mIoU:  61.41  

*** Training [@Epoch 09] Avg Loss: 0.24520  Avg mIoU:  61.36  ***

[Epoch: 09] [Batch: 0001/0050] Loss: 0.20911  Avg Loss: 0.20911  Avg mIoU:  57.31  

*** Validation [@Epoch 09] Avg Loss: 0.29398  Avg mIoU:  54.71  ***

Model saved @9 w/ val. mIoU: 54.71.

[Epoch: 10] [Batch: 0001/0580] Loss: 0.18621  Avg Loss: 0.18621  Avg mIoU:  47.46  
[Epoch: 10] [Batch: 0051/0580] Loss: 0.25552  Avg Loss: 0.23681  Avg mIoU:  59.75  
[Epoch: 10] [Batch: 0101/0580] Loss: 0.20978  Avg Loss: 0.23846  Avg mIoU:  59.59  
[Epoch: 10] [Batch: 0151/0580] Loss: 0.19915  Avg Loss: 0.24116  Avg mIoU:  60.38  
[Epoch: 10] [Batch: 0201/0580] Loss: 0.24563  Avg Loss: 0.24047  Avg mIoU:  61.02  
[Epoch: 10] [Batch: 0251/0580] Loss: 0.25059  Avg Loss: 0.24232  Avg mIoU:  61.20  
[Epoch: 10] [Batch: 0301/0580] Loss: 0.31093  Avg Loss: 0.24544  Avg mIoU:  61.03  
[Epoch: 10] [Batch: 0351/0580] Loss: 0.27859  Avg Loss: 0.24525  Avg mIoU:  61.12  
[Epoch: 10] [Batch: 0401/0580] Loss: 0.21772  Avg Loss: 0.24493  Avg mIoU:  61.14  
[Epoch: 10] [Batch: 0451/0580] Loss: 0.27913  Avg Loss: 0.24499  Avg mIoU:  61.28  
[Epoch: 10] [Batch: 0501/0580] Loss: 0.22917  Avg Loss: 0.24496  Avg mIoU:  61.36  
[Epoch: 10] [Batch: 0551/0580] Loss: 0.20561  Avg Loss: 0.24441  Avg mIoU:  61.43  

*** Training [@Epoch 10] Avg Loss: 0.24442  Avg mIoU:  61.44  ***

[Epoch: 10] [Batch: 0001/0050] Loss: 0.26263  Avg Loss: 0.26263  Avg mIoU:  57.42  

*** Validation [@Epoch 10] Avg Loss: 0.32774  Avg mIoU:  55.97  ***

Model saved @10 w/ val. mIoU: 55.97.

[Epoch: 11] [Batch: 0001/0580] Loss: 0.27003  Avg Loss: 0.27003  Avg mIoU:  28.33  
[Epoch: 11] [Batch: 0051/0580] Loss: 0.22480  Avg Loss: 0.24181  Avg mIoU:  61.49  
[Epoch: 11] [Batch: 0101/0580] Loss: 0.31164  Avg Loss: 0.24251  Avg mIoU:  61.02  
[Epoch: 11] [Batch: 0151/0580] Loss: 0.28463  Avg Loss: 0.23737  Avg mIoU:  62.16  
[Epoch: 11] [Batch: 0201/0580] Loss: 0.14512  Avg Loss: 0.23884  Avg mIoU:  61.90  
[Epoch: 11] [Batch: 0251/0580] Loss: 0.20419  Avg Loss: 0.23711  Avg mIoU:  62.04  
[Epoch: 11] [Batch: 0301/0580] Loss: 0.21802  Avg Loss: 0.23926  Avg mIoU:  61.73  
[Epoch: 11] [Batch: 0351/0580] Loss: 0.19064  Avg Loss: 0.24029  Avg mIoU:  61.76  
[Epoch: 11] [Batch: 0401/0580] Loss: 0.19420  Avg Loss: 0.24145  Avg mIoU:  61.75  
[Epoch: 11] [Batch: 0451/0580] Loss: 0.17901  Avg Loss: 0.24131  Avg mIoU:  61.74  
[Epoch: 11] [Batch: 0501/0580] Loss: 0.27343  Avg Loss: 0.24077  Avg mIoU:  61.67  
[Epoch: 11] [Batch: 0551/0580] Loss: 0.17557  Avg Loss: 0.23974  Avg mIoU:  61.89  

*** Training [@Epoch 11] Avg Loss: 0.24034  Avg mIoU:  61.80  ***

[Epoch: 11] [Batch: 0001/0050] Loss: 0.22064  Avg Loss: 0.22064  Avg mIoU:  57.03  

*** Validation [@Epoch 11] Avg Loss: 0.29263  Avg mIoU:  56.15  ***

Model saved @11 w/ val. mIoU: 56.15.

[Epoch: 12] [Batch: 0001/0580] Loss: 0.20188  Avg Loss: 0.20188  Avg mIoU:  40.79  
[Epoch: 12] [Batch: 0051/0580] Loss: 0.32915  Avg Loss: 0.22947  Avg mIoU:  63.37  
[Epoch: 12] [Batch: 0101/0580] Loss: 0.22314  Avg Loss: 0.23744  Avg mIoU:  61.98  
[Epoch: 12] [Batch: 0151/0580] Loss: 0.27647  Avg Loss: 0.24061  Avg mIoU:  61.77  
[Epoch: 12] [Batch: 0201/0580] Loss: 0.29345  Avg Loss: 0.24051  Avg mIoU:  62.30  
[Epoch: 12] [Batch: 0251/0580] Loss: 0.18403  Avg Loss: 0.23909  Avg mIoU:  62.05  
[Epoch: 12] [Batch: 0301/0580] Loss: 0.14224  Avg Loss: 0.24070  Avg mIoU:  61.87  
[Epoch: 12] [Batch: 0351/0580] Loss: 0.24089  Avg Loss: 0.24183  Avg mIoU:  62.19  
[Epoch: 12] [Batch: 0401/0580] Loss: 0.26127  Avg Loss: 0.24059  Avg mIoU:  62.26  
[Epoch: 12] [Batch: 0451/0580] Loss: 0.19584  Avg Loss: 0.23850  Avg mIoU:  62.33  
[Epoch: 12] [Batch: 0501/0580] Loss: 0.23377  Avg Loss: 0.23767  Avg mIoU:  62.38  
[Epoch: 12] [Batch: 0551/0580] Loss: 0.14759  Avg Loss: 0.23682  Avg mIoU:  62.44  

*** Training [@Epoch 12] Avg Loss: 0.23627  Avg mIoU:  62.39  ***

[Epoch: 12] [Batch: 0001/0050] Loss: 0.21111  Avg Loss: 0.21111  Avg mIoU:  53.69  

*** Validation [@Epoch 12] Avg Loss: 0.27309  Avg mIoU:  52.80  ***

[Epoch: 13] [Batch: 0001/0580] Loss: 0.34453  Avg Loss: 0.34453  Avg mIoU:  29.78  
[Epoch: 13] [Batch: 0051/0580] Loss: 0.26425  Avg Loss: 0.23512  Avg mIoU:  63.51  
[Epoch: 13] [Batch: 0101/0580] Loss: 0.25600  Avg Loss: 0.22869  Avg mIoU:  62.41  
[Epoch: 13] [Batch: 0151/0580] Loss: 0.16451  Avg Loss: 0.23324  Avg mIoU:  61.70  
[Epoch: 13] [Batch: 0201/0580] Loss: 0.21017  Avg Loss: 0.22991  Avg mIoU:  62.50  
[Epoch: 13] [Batch: 0251/0580] Loss: 0.26104  Avg Loss: 0.23151  Avg mIoU:  62.58  
[Epoch: 13] [Batch: 0301/0580] Loss: 0.27937  Avg Loss: 0.23200  Avg mIoU:  62.77  
[Epoch: 13] [Batch: 0351/0580] Loss: 0.17798  Avg Loss: 0.23276  Avg mIoU:  62.53  
[Epoch: 13] [Batch: 0401/0580] Loss: 0.29095  Avg Loss: 0.23219  Avg mIoU:  62.52  
[Epoch: 13] [Batch: 0451/0580] Loss: 0.25428  Avg Loss: 0.23377  Avg mIoU:  62.46  
[Epoch: 13] [Batch: 0501/0580] Loss: 0.15206  Avg Loss: 0.23423  Avg mIoU:  62.49  
[Epoch: 13] [Batch: 0551/0580] Loss: 0.21995  Avg Loss: 0.23446  Avg mIoU:  62.40  

*** Training [@Epoch 13] Avg Loss: 0.23384  Avg mIoU:  62.39  ***

[Epoch: 13] [Batch: 0001/0050] Loss: 0.21857  Avg Loss: 0.21857  Avg mIoU:  55.27  

*** Validation [@Epoch 13] Avg Loss: 0.28591  Avg mIoU:  55.86  ***

[Epoch: 14] [Batch: 0001/0580] Loss: 0.27544  Avg Loss: 0.27544  Avg mIoU:  31.94  
[Epoch: 14] [Batch: 0051/0580] Loss: 0.19441  Avg Loss: 0.22965  Avg mIoU:  62.79  
[Epoch: 14] [Batch: 0101/0580] Loss: 0.17519  Avg Loss: 0.23543  Avg mIoU:  62.83  
[Epoch: 14] [Batch: 0151/0580] Loss: 0.18646  Avg Loss: 0.23908  Avg mIoU:  62.56  
[Epoch: 14] [Batch: 0201/0580] Loss: 0.24172  Avg Loss: 0.23768  Avg mIoU:  63.20  
[Epoch: 14] [Batch: 0251/0580] Loss: 0.46195  Avg Loss: 0.23593  Avg mIoU:  63.34  
[Epoch: 14] [Batch: 0301/0580] Loss: 0.20237  Avg Loss: 0.23480  Avg mIoU:  63.20  
[Epoch: 14] [Batch: 0351/0580] Loss: 0.19172  Avg Loss: 0.23229  Avg mIoU:  63.30  
[Epoch: 14] [Batch: 0401/0580] Loss: 0.18252  Avg Loss: 0.23221  Avg mIoU:  63.24  
[Epoch: 14] [Batch: 0451/0580] Loss: 0.19606  Avg Loss: 0.23184  Avg mIoU:  63.28  
[Epoch: 14] [Batch: 0501/0580] Loss: 0.26603  Avg Loss: 0.23121  Avg mIoU:  63.19  
[Epoch: 14] [Batch: 0551/0580] Loss: 0.24568  Avg Loss: 0.23160  Avg mIoU:  63.11  

*** Training [@Epoch 14] Avg Loss: 0.23169  Avg mIoU:  62.97  ***

[Epoch: 14] [Batch: 0001/0050] Loss: 0.20959  Avg Loss: 0.20959  Avg mIoU:  59.17  

*** Validation [@Epoch 14] Avg Loss: 0.28496  Avg mIoU:  55.43  ***

[Epoch: 15] [Batch: 0001/0580] Loss: 0.25160  Avg Loss: 0.25160  Avg mIoU:  52.91  
[Epoch: 15] [Batch: 0051/0580] Loss: 0.22424  Avg Loss: 0.23572  Avg mIoU:  61.16  
[Epoch: 15] [Batch: 0101/0580] Loss: 0.22765  Avg Loss: 0.24072  Avg mIoU:  61.19  
[Epoch: 15] [Batch: 0151/0580] Loss: 0.19440  Avg Loss: 0.23573  Avg mIoU:  61.95  
[Epoch: 15] [Batch: 0201/0580] Loss: 0.21832  Avg Loss: 0.23443  Avg mIoU:  62.32  
[Epoch: 15] [Batch: 0251/0580] Loss: 0.24470  Avg Loss: 0.23201  Avg mIoU:  62.42  
[Epoch: 15] [Batch: 0301/0580] Loss: 0.17294  Avg Loss: 0.23094  Avg mIoU:  62.58  
[Epoch: 15] [Batch: 0351/0580] Loss: 0.27166  Avg Loss: 0.23133  Avg mIoU:  62.48  
[Epoch: 15] [Batch: 0401/0580] Loss: 0.30045  Avg Loss: 0.23139  Avg mIoU:  62.55  
[Epoch: 15] [Batch: 0451/0580] Loss: 0.20878  Avg Loss: 0.23133  Avg mIoU:  62.64  
[Epoch: 15] [Batch: 0501/0580] Loss: 0.19199  Avg Loss: 0.23169  Avg mIoU:  62.78  
[Epoch: 15] [Batch: 0551/0580] Loss: 0.26645  Avg Loss: 0.23126  Avg mIoU:  62.81  

*** Training [@Epoch 15] Avg Loss: 0.23023  Avg mIoU:  62.97  ***

[Epoch: 15] [Batch: 0001/0050] Loss: 0.22629  Avg Loss: 0.22629  Avg mIoU:  55.69  

*** Validation [@Epoch 15] Avg Loss: 0.28669  Avg mIoU:  54.70  ***

[Epoch: 16] [Batch: 0001/0580] Loss: 0.26347  Avg Loss: 0.26347  Avg mIoU:  23.92  
[Epoch: 16] [Batch: 0051/0580] Loss: 0.23027  Avg Loss: 0.22497  Avg mIoU:  63.70  
[Epoch: 16] [Batch: 0101/0580] Loss: 0.35305  Avg Loss: 0.21981  Avg mIoU:  63.77  
[Epoch: 16] [Batch: 0151/0580] Loss: 0.30183  Avg Loss: 0.22166  Avg mIoU:  64.00  
[Epoch: 16] [Batch: 0201/0580] Loss: 0.31646  Avg Loss: 0.22318  Avg mIoU:  63.71  
[Epoch: 16] [Batch: 0251/0580] Loss: 0.27814  Avg Loss: 0.22463  Avg mIoU:  63.61  
[Epoch: 16] [Batch: 0301/0580] Loss: 0.14515  Avg Loss: 0.22614  Avg mIoU:  63.56  
[Epoch: 16] [Batch: 0351/0580] Loss: 0.19785  Avg Loss: 0.22634  Avg mIoU:  63.68  
[Epoch: 16] [Batch: 0401/0580] Loss: 0.21698  Avg Loss: 0.22535  Avg mIoU:  63.97  
[Epoch: 16] [Batch: 0451/0580] Loss: 0.20190  Avg Loss: 0.22680  Avg mIoU:  63.71  
[Epoch: 16] [Batch: 0501/0580] Loss: 0.25049  Avg Loss: 0.22686  Avg mIoU:  63.64  
[Epoch: 16] [Batch: 0551/0580] Loss: 0.30275  Avg Loss: 0.22659  Avg mIoU:  63.64  

*** Training [@Epoch 16] Avg Loss: 0.22639  Avg mIoU:  63.67  ***

[Epoch: 16] [Batch: 0001/0050] Loss: 0.19657  Avg Loss: 0.19657  Avg mIoU:  59.66  

*** Validation [@Epoch 16] Avg Loss: 0.26687  Avg mIoU:  55.20  ***

[Epoch: 17] [Batch: 0001/0580] Loss: 0.23263  Avg Loss: 0.23263  Avg mIoU:  32.13  
[Epoch: 17] [Batch: 0051/0580] Loss: 0.16424  Avg Loss: 0.22950  Avg mIoU:  64.62  
[Epoch: 17] [Batch: 0101/0580] Loss: 0.28992  Avg Loss: 0.22633  Avg mIoU:  63.65  
[Epoch: 17] [Batch: 0151/0580] Loss: 0.14465  Avg Loss: 0.22734  Avg mIoU:  63.21  
[Epoch: 17] [Batch: 0201/0580] Loss: 0.19667  Avg Loss: 0.22863  Avg mIoU:  63.01  
[Epoch: 17] [Batch: 0251/0580] Loss: 0.21173  Avg Loss: 0.22929  Avg mIoU:  63.21  
[Epoch: 17] [Batch: 0301/0580] Loss: 0.16354  Avg Loss: 0.22911  Avg mIoU:  63.54  
[Epoch: 17] [Batch: 0351/0580] Loss: 0.20101  Avg Loss: 0.23045  Avg mIoU:  63.42  
[Epoch: 17] [Batch: 0401/0580] Loss: 0.22795  Avg Loss: 0.22927  Avg mIoU:  63.52  
[Epoch: 17] [Batch: 0451/0580] Loss: 0.16300  Avg Loss: 0.22748  Avg mIoU:  63.47  
[Epoch: 17] [Batch: 0501/0580] Loss: 0.21330  Avg Loss: 0.22606  Avg mIoU:  63.55  
[Epoch: 17] [Batch: 0551/0580] Loss: 0.18906  Avg Loss: 0.22538  Avg mIoU:  63.68  

*** Training [@Epoch 17] Avg Loss: 0.22522  Avg mIoU:  63.62  ***

[Epoch: 17] [Batch: 0001/0050] Loss: 0.21021  Avg Loss: 0.21021  Avg mIoU:  58.46  

*** Validation [@Epoch 17] Avg Loss: 0.29426  Avg mIoU:  58.19  ***

Model saved @17 w/ val. mIoU: 58.19.

[Epoch: 18] [Batch: 0001/0580] Loss: 0.17729  Avg Loss: 0.17729  Avg mIoU:  50.15  
[Epoch: 18] [Batch: 0051/0580] Loss: 0.25965  Avg Loss: 0.22099  Avg mIoU:  65.20  
[Epoch: 18] [Batch: 0101/0580] Loss: 0.26270  Avg Loss: 0.22163  Avg mIoU:  63.89  
[Epoch: 18] [Batch: 0151/0580] Loss: 0.15637  Avg Loss: 0.22376  Avg mIoU:  63.49  
[Epoch: 18] [Batch: 0201/0580] Loss: 0.18106  Avg Loss: 0.22337  Avg mIoU:  63.88  
[Epoch: 18] [Batch: 0251/0580] Loss: 0.22444  Avg Loss: 0.22653  Avg mIoU:  63.71  
[Epoch: 18] [Batch: 0301/0580] Loss: 0.22879  Avg Loss: 0.22605  Avg mIoU:  63.66  
[Epoch: 18] [Batch: 0351/0580] Loss: 0.17943  Avg Loss: 0.22527  Avg mIoU:  63.67  
[Epoch: 18] [Batch: 0401/0580] Loss: 0.23315  Avg Loss: 0.22497  Avg mIoU:  63.66  
[Epoch: 18] [Batch: 0451/0580] Loss: 0.30282  Avg Loss: 0.22482  Avg mIoU:  63.40  
[Epoch: 18] [Batch: 0501/0580] Loss: 0.29774  Avg Loss: 0.22598  Avg mIoU:  63.41  
[Epoch: 18] [Batch: 0551/0580] Loss: 0.25187  Avg Loss: 0.22507  Avg mIoU:  63.56  

*** Training [@Epoch 18] Avg Loss: 0.22540  Avg mIoU:  63.62  ***

[Epoch: 18] [Batch: 0001/0050] Loss: 0.19199  Avg Loss: 0.19199  Avg mIoU:  60.52  

*** Validation [@Epoch 18] Avg Loss: 0.26499  Avg mIoU:  57.05  ***

[Epoch: 19] [Batch: 0001/0580] Loss: 0.19342  Avg Loss: 0.19342  Avg mIoU:  28.62  
[Epoch: 19] [Batch: 0051/0580] Loss: 0.27312  Avg Loss: 0.22281  Avg mIoU:  63.58  
[Epoch: 19] [Batch: 0101/0580] Loss: 0.18805  Avg Loss: 0.22005  Avg mIoU:  63.94  
[Epoch: 19] [Batch: 0151/0580] Loss: 0.22902  Avg Loss: 0.22105  Avg mIoU:  63.76  
[Epoch: 19] [Batch: 0201/0580] Loss: 0.19856  Avg Loss: 0.22335  Avg mIoU:  63.67  
[Epoch: 19] [Batch: 0251/0580] Loss: 0.16280  Avg Loss: 0.22317  Avg mIoU:  63.89  
[Epoch: 19] [Batch: 0301/0580] Loss: 0.20117  Avg Loss: 0.22471  Avg mIoU:  63.63  
[Epoch: 19] [Batch: 0351/0580] Loss: 0.18419  Avg Loss: 0.22413  Avg mIoU:  63.76  
[Epoch: 19] [Batch: 0401/0580] Loss: 0.20309  Avg Loss: 0.22262  Avg mIoU:  63.89  
[Epoch: 19] [Batch: 0451/0580] Loss: 0.21726  Avg Loss: 0.22392  Avg mIoU:  63.93  
[Epoch: 19] [Batch: 0501/0580] Loss: 0.18316  Avg Loss: 0.22403  Avg mIoU:  64.06  
[Epoch: 19] [Batch: 0551/0580] Loss: 0.27210  Avg Loss: 0.22379  Avg mIoU:  64.18  

*** Training [@Epoch 19] Avg Loss: 0.22328  Avg mIoU:  64.14  ***

[Epoch: 19] [Batch: 0001/0050] Loss: 0.20272  Avg Loss: 0.20272  Avg mIoU:  58.86  

*** Validation [@Epoch 19] Avg Loss: 0.27315  Avg mIoU:  56.87  ***

[Epoch: 20] [Batch: 0001/0580] Loss: 0.21024  Avg Loss: 0.21024  Avg mIoU:  50.06  
[Epoch: 20] [Batch: 0051/0580] Loss: 0.20181  Avg Loss: 0.21376  Avg mIoU:  63.27  
[Epoch: 20] [Batch: 0101/0580] Loss: 0.24534  Avg Loss: 0.22485  Avg mIoU:  63.17  
[Epoch: 20] [Batch: 0151/0580] Loss: 0.17863  Avg Loss: 0.22292  Avg mIoU:  63.56  
[Epoch: 20] [Batch: 0201/0580] Loss: 0.16885  Avg Loss: 0.22160  Avg mIoU:  64.07  
[Epoch: 20] [Batch: 0251/0580] Loss: 0.17321  Avg Loss: 0.21920  Avg mIoU:  64.50  
[Epoch: 20] [Batch: 0301/0580] Loss: 0.23679  Avg Loss: 0.22051  Avg mIoU:  64.13  
[Epoch: 20] [Batch: 0351/0580] Loss: 0.13568  Avg Loss: 0.21873  Avg mIoU:  64.39  
[Epoch: 20] [Batch: 0401/0580] Loss: 0.19228  Avg Loss: 0.21968  Avg mIoU:  64.26  
[Epoch: 20] [Batch: 0451/0580] Loss: 0.17965  Avg Loss: 0.21944  Avg mIoU:  64.32  
[Epoch: 20] [Batch: 0501/0580] Loss: 0.33792  Avg Loss: 0.21875  Avg mIoU:  64.57  
[Epoch: 20] [Batch: 0551/0580] Loss: 0.22444  Avg Loss: 0.21869  Avg mIoU:  64.78  

*** Training [@Epoch 20] Avg Loss: 0.21806  Avg mIoU:  64.71  ***

[Epoch: 20] [Batch: 0001/0050] Loss: 0.22299  Avg Loss: 0.22299  Avg mIoU:  55.16  

*** Validation [@Epoch 20] Avg Loss: 0.29593  Avg mIoU:  57.88  ***

[Epoch: 21] [Batch: 0001/0580] Loss: 0.20303  Avg Loss: 0.20303  Avg mIoU:  42.82  
[Epoch: 21] [Batch: 0051/0580] Loss: 0.23371  Avg Loss: 0.22165  Avg mIoU:  63.26  
[Epoch: 21] [Batch: 0101/0580] Loss: 0.22960  Avg Loss: 0.22009  Avg mIoU:  63.68  
[Epoch: 21] [Batch: 0151/0580] Loss: 0.20225  Avg Loss: 0.22049  Avg mIoU:  64.35  
[Epoch: 21] [Batch: 0201/0580] Loss: 0.21821  Avg Loss: 0.22116  Avg mIoU:  64.63  
[Epoch: 21] [Batch: 0251/0580] Loss: 0.29475  Avg Loss: 0.22004  Avg mIoU:  64.35  
[Epoch: 21] [Batch: 0301/0580] Loss: 0.19110  Avg Loss: 0.21708  Avg mIoU:  64.47  
[Epoch: 21] [Batch: 0351/0580] Loss: 0.22739  Avg Loss: 0.21691  Avg mIoU:  64.52  
[Epoch: 21] [Batch: 0401/0580] Loss: 0.32059  Avg Loss: 0.21696  Avg mIoU:  64.61  
[Epoch: 21] [Batch: 0451/0580] Loss: 0.21773  Avg Loss: 0.21773  Avg mIoU:  64.55  
[Epoch: 21] [Batch: 0501/0580] Loss: 0.23353  Avg Loss: 0.21812  Avg mIoU:  64.47  
[Epoch: 21] [Batch: 0551/0580] Loss: 0.29683  Avg Loss: 0.21809  Avg mIoU:  64.46  

*** Training [@Epoch 21] Avg Loss: 0.21769  Avg mIoU:  64.55  ***

[Epoch: 21] [Batch: 0001/0050] Loss: 0.18066  Avg Loss: 0.18066  Avg mIoU:  63.22  

*** Validation [@Epoch 21] Avg Loss: 0.28526  Avg mIoU:  52.38  ***

[Epoch: 22] [Batch: 0001/0580] Loss: 0.20818  Avg Loss: 0.20818  Avg mIoU:  40.56  
[Epoch: 22] [Batch: 0051/0580] Loss: 0.22364  Avg Loss: 0.21139  Avg mIoU:  64.16  
[Epoch: 22] [Batch: 0101/0580] Loss: 0.14409  Avg Loss: 0.20843  Avg mIoU:  65.15  
[Epoch: 22] [Batch: 0151/0580] Loss: 0.19591  Avg Loss: 0.20594  Avg mIoU:  65.57  
[Epoch: 22] [Batch: 0201/0580] Loss: 0.13190  Avg Loss: 0.21083  Avg mIoU:  65.07  
[Epoch: 22] [Batch: 0251/0580] Loss: 0.19330  Avg Loss: 0.21243  Avg mIoU:  64.58  
[Epoch: 22] [Batch: 0301/0580] Loss: 0.25462  Avg Loss: 0.21362  Avg mIoU:  64.65  
[Epoch: 22] [Batch: 0351/0580] Loss: 0.22295  Avg Loss: 0.21590  Avg mIoU:  64.57  
[Epoch: 22] [Batch: 0401/0580] Loss: 0.30354  Avg Loss: 0.21714  Avg mIoU:  64.42  
[Epoch: 22] [Batch: 0451/0580] Loss: 0.19750  Avg Loss: 0.21735  Avg mIoU:  64.45  
[Epoch: 22] [Batch: 0501/0580] Loss: 0.22303  Avg Loss: 0.21710  Avg mIoU:  64.50  
[Epoch: 22] [Batch: 0551/0580] Loss: 0.22931  Avg Loss: 0.21667  Avg mIoU:  64.50  

*** Training [@Epoch 22] Avg Loss: 0.21695  Avg mIoU:  64.49  ***

[Epoch: 22] [Batch: 0001/0050] Loss: 0.18786  Avg Loss: 0.18786  Avg mIoU:  62.21  

*** Validation [@Epoch 22] Avg Loss: 0.26026  Avg mIoU:  58.26  ***

Model saved @22 w/ val. mIoU: 58.26.

[Epoch: 23] [Batch: 0001/0580] Loss: 0.17752  Avg Loss: 0.17752  Avg mIoU:  43.17  
[Epoch: 23] [Batch: 0051/0580] Loss: 0.15440  Avg Loss: 0.21392  Avg mIoU:  67.23  
[Epoch: 23] [Batch: 0101/0580] Loss: 0.17865  Avg Loss: 0.21190  Avg mIoU:  66.13  
[Epoch: 23] [Batch: 0151/0580] Loss: 0.21379  Avg Loss: 0.21302  Avg mIoU:  65.26  
[Epoch: 23] [Batch: 0201/0580] Loss: 0.14180  Avg Loss: 0.21172  Avg mIoU:  64.76  
[Epoch: 23] [Batch: 0251/0580] Loss: 0.15364  Avg Loss: 0.21120  Avg mIoU:  64.96  
[Epoch: 23] [Batch: 0301/0580] Loss: 0.18570  Avg Loss: 0.21229  Avg mIoU:  64.74  
[Epoch: 23] [Batch: 0351/0580] Loss: 0.36281  Avg Loss: 0.21391  Avg mIoU:  64.88  
[Epoch: 23] [Batch: 0401/0580] Loss: 0.23577  Avg Loss: 0.21396  Avg mIoU:  65.19  
[Epoch: 23] [Batch: 0451/0580] Loss: 0.23140  Avg Loss: 0.21434  Avg mIoU:  65.09  
[Epoch: 23] [Batch: 0501/0580] Loss: 0.18083  Avg Loss: 0.21439  Avg mIoU:  65.05  
[Epoch: 23] [Batch: 0551/0580] Loss: 0.25312  Avg Loss: 0.21462  Avg mIoU:  64.99  

*** Training [@Epoch 23] Avg Loss: 0.21503  Avg mIoU:  64.92  ***

[Epoch: 23] [Batch: 0001/0050] Loss: 0.20408  Avg Loss: 0.20408  Avg mIoU:  60.29  

*** Validation [@Epoch 23] Avg Loss: 0.27804  Avg mIoU:  56.57  ***

[Epoch: 24] [Batch: 0001/0580] Loss: 0.23400  Avg Loss: 0.23400  Avg mIoU:  41.27  
[Epoch: 24] [Batch: 0051/0580] Loss: 0.13742  Avg Loss: 0.20911  Avg mIoU:  64.16  
[Epoch: 24] [Batch: 0101/0580] Loss: 0.22382  Avg Loss: 0.21442  Avg mIoU:  64.11  
[Epoch: 24] [Batch: 0151/0580] Loss: 0.17788  Avg Loss: 0.21352  Avg mIoU:  64.52  
[Epoch: 24] [Batch: 0201/0580] Loss: 0.24789  Avg Loss: 0.21478  Avg mIoU:  65.05  
[Epoch: 24] [Batch: 0251/0580] Loss: 0.23709  Avg Loss: 0.21520  Avg mIoU:  65.03  
[Epoch: 24] [Batch: 0301/0580] Loss: 0.23328  Avg Loss: 0.21589  Avg mIoU:  65.11  
[Epoch: 24] [Batch: 0351/0580] Loss: 0.19898  Avg Loss: 0.21492  Avg mIoU:  65.35  
[Epoch: 24] [Batch: 0401/0580] Loss: 0.20058  Avg Loss: 0.21477  Avg mIoU:  65.36  
[Epoch: 24] [Batch: 0451/0580] Loss: 0.21955  Avg Loss: 0.21504  Avg mIoU:  65.25  
[Epoch: 24] [Batch: 0501/0580] Loss: 0.18144  Avg Loss: 0.21474  Avg mIoU:  65.13  
[Epoch: 24] [Batch: 0551/0580] Loss: 0.18767  Avg Loss: 0.21453  Avg mIoU:  65.04  

*** Training [@Epoch 24] Avg Loss: 0.21513  Avg mIoU:  65.00  ***

[Epoch: 24] [Batch: 0001/0050] Loss: 0.20559  Avg Loss: 0.20559  Avg mIoU:  60.23  

*** Validation [@Epoch 24] Avg Loss: 0.26897  Avg mIoU:  58.75  ***

Model saved @24 w/ val. mIoU: 58.75.

[Epoch: 25] [Batch: 0001/0580] Loss: 0.21284  Avg Loss: 0.21284  Avg mIoU:  53.38  
[Epoch: 25] [Batch: 0051/0580] Loss: 0.30153  Avg Loss: 0.21815  Avg mIoU:  65.44  
[Epoch: 25] [Batch: 0101/0580] Loss: 0.16628  Avg Loss: 0.21647  Avg mIoU:  64.86  
[Epoch: 25] [Batch: 0151/0580] Loss: 0.17507  Avg Loss: 0.21381  Avg mIoU:  64.71  
[Epoch: 25] [Batch: 0201/0580] Loss: 0.14053  Avg Loss: 0.21352  Avg mIoU:  65.06  
[Epoch: 25] [Batch: 0251/0580] Loss: 0.24229  Avg Loss: 0.21281  Avg mIoU:  65.02  
[Epoch: 25] [Batch: 0301/0580] Loss: 0.20774  Avg Loss: 0.21250  Avg mIoU:  65.09  
[Epoch: 25] [Batch: 0351/0580] Loss: 0.20620  Avg Loss: 0.21170  Avg mIoU:  65.22  
[Epoch: 25] [Batch: 0401/0580] Loss: 0.27925  Avg Loss: 0.21208  Avg mIoU:  65.05  
[Epoch: 25] [Batch: 0451/0580] Loss: 0.22647  Avg Loss: 0.21326  Avg mIoU:  64.88  
[Epoch: 25] [Batch: 0501/0580] Loss: 0.20737  Avg Loss: 0.21374  Avg mIoU:  65.02  
[Epoch: 25] [Batch: 0551/0580] Loss: 0.36639  Avg Loss: 0.21353  Avg mIoU:  65.07  

*** Training [@Epoch 25] Avg Loss: 0.21355  Avg mIoU:  65.13  ***

[Epoch: 25] [Batch: 0001/0050] Loss: 0.19478  Avg Loss: 0.19478  Avg mIoU:  60.83  

*** Validation [@Epoch 25] Avg Loss: 0.28382  Avg mIoU:  58.83  ***

Model saved @25 w/ val. mIoU: 58.83.

[Epoch: 26] [Batch: 0001/0580] Loss: 0.17044  Avg Loss: 0.17044  Avg mIoU:  49.41  
[Epoch: 26] [Batch: 0051/0580] Loss: 0.14244  Avg Loss: 0.21201  Avg mIoU:  65.79  
[Epoch: 26] [Batch: 0101/0580] Loss: 0.18407  Avg Loss: 0.20711  Avg mIoU:  65.61  
[Epoch: 26] [Batch: 0151/0580] Loss: 0.19556  Avg Loss: 0.20401  Avg mIoU:  66.05  
[Epoch: 26] [Batch: 0201/0580] Loss: 0.21781  Avg Loss: 0.20736  Avg mIoU:  66.00  
[Epoch: 26] [Batch: 0251/0580] Loss: 0.33044  Avg Loss: 0.20846  Avg mIoU:  65.61  
[Epoch: 26] [Batch: 0301/0580] Loss: 0.25230  Avg Loss: 0.20730  Avg mIoU:  65.70  
[Epoch: 26] [Batch: 0351/0580] Loss: 0.14483  Avg Loss: 0.20827  Avg mIoU:  65.79  
[Epoch: 26] [Batch: 0401/0580] Loss: 0.25249  Avg Loss: 0.20978  Avg mIoU:  65.71  
[Epoch: 26] [Batch: 0451/0580] Loss: 0.26777  Avg Loss: 0.21010  Avg mIoU:  65.80  
[Epoch: 26] [Batch: 0501/0580] Loss: 0.18857  Avg Loss: 0.20920  Avg mIoU:  65.85  
[Epoch: 26] [Batch: 0551/0580] Loss: 0.19059  Avg Loss: 0.20934  Avg mIoU:  65.94  

*** Training [@Epoch 26] Avg Loss: 0.20915  Avg mIoU:  66.01  ***

[Epoch: 26] [Batch: 0001/0050] Loss: 0.18415  Avg Loss: 0.18415  Avg mIoU:  63.18  

*** Validation [@Epoch 26] Avg Loss: 0.27482  Avg mIoU:  57.31  ***

[Epoch: 27] [Batch: 0001/0580] Loss: 0.22464  Avg Loss: 0.22464  Avg mIoU:  42.16  
[Epoch: 27] [Batch: 0051/0580] Loss: 0.21391  Avg Loss: 0.21244  Avg mIoU:  68.01  
[Epoch: 27] [Batch: 0101/0580] Loss: 0.26043  Avg Loss: 0.20974  Avg mIoU:  66.22  
[Epoch: 27] [Batch: 0151/0580] Loss: 0.21649  Avg Loss: 0.20947  Avg mIoU:  66.05  
[Epoch: 27] [Batch: 0201/0580] Loss: 0.19729  Avg Loss: 0.20889  Avg mIoU:  65.78  
[Epoch: 27] [Batch: 0251/0580] Loss: 0.15406  Avg Loss: 0.20779  Avg mIoU:  65.85  
[Epoch: 27] [Batch: 0301/0580] Loss: 0.16800  Avg Loss: 0.20873  Avg mIoU:  65.45  
[Epoch: 27] [Batch: 0351/0580] Loss: 0.18568  Avg Loss: 0.20853  Avg mIoU:  65.33  
[Epoch: 27] [Batch: 0401/0580] Loss: 0.20430  Avg Loss: 0.20885  Avg mIoU:  65.22  
[Epoch: 27] [Batch: 0451/0580] Loss: 0.24965  Avg Loss: 0.20932  Avg mIoU:  65.37  
[Epoch: 27] [Batch: 0501/0580] Loss: 0.18715  Avg Loss: 0.20880  Avg mIoU:  65.52  
[Epoch: 27] [Batch: 0551/0580] Loss: 0.14212  Avg Loss: 0.20981  Avg mIoU:  65.61  

*** Training [@Epoch 27] Avg Loss: 0.20964  Avg mIoU:  65.60  ***

[Epoch: 27] [Batch: 0001/0050] Loss: 0.19893  Avg Loss: 0.19893  Avg mIoU:  54.89  

*** Validation [@Epoch 27] Avg Loss: 0.26699  Avg mIoU:  50.61  ***

[Epoch: 28] [Batch: 0001/0580] Loss: 0.21658  Avg Loss: 0.21658  Avg mIoU:  44.11  
[Epoch: 28] [Batch: 0051/0580] Loss: 0.21518  Avg Loss: 0.21258  Avg mIoU:  65.44  
[Epoch: 28] [Batch: 0101/0580] Loss: 0.22417  Avg Loss: 0.20611  Avg mIoU:  66.44  
[Epoch: 28] [Batch: 0151/0580] Loss: 0.26714  Avg Loss: 0.20884  Avg mIoU:  66.18  
[Epoch: 28] [Batch: 0201/0580] Loss: 0.16095  Avg Loss: 0.20986  Avg mIoU:  66.08  
[Epoch: 28] [Batch: 0251/0580] Loss: 0.21800  Avg Loss: 0.20929  Avg mIoU:  65.87  
[Epoch: 28] [Batch: 0301/0580] Loss: 0.24148  Avg Loss: 0.21015  Avg mIoU:  65.41  
[Epoch: 28] [Batch: 0351/0580] Loss: 0.22281  Avg Loss: 0.20756  Avg mIoU:  65.51  
[Epoch: 28] [Batch: 0401/0580] Loss: 0.25943  Avg Loss: 0.20690  Avg mIoU:  65.79  
[Epoch: 28] [Batch: 0451/0580] Loss: 0.25166  Avg Loss: 0.20641  Avg mIoU:  65.84  
[Epoch: 28] [Batch: 0501/0580] Loss: 0.20974  Avg Loss: 0.20601  Avg mIoU:  66.05  
[Epoch: 28] [Batch: 0551/0580] Loss: 0.14193  Avg Loss: 0.20621  Avg mIoU:  65.98  

*** Training [@Epoch 28] Avg Loss: 0.20609  Avg mIoU:  65.89  ***

[Epoch: 28] [Batch: 0001/0050] Loss: 0.19060  Avg Loss: 0.19060  Avg mIoU:  58.25  

*** Validation [@Epoch 28] Avg Loss: 0.26802  Avg mIoU:  55.63  ***

[Epoch: 29] [Batch: 0001/0580] Loss: 0.21215  Avg Loss: 0.21215  Avg mIoU:  37.53  
[Epoch: 29] [Batch: 0051/0580] Loss: 0.13648  Avg Loss: 0.21340  Avg mIoU:  64.73  
[Epoch: 29] [Batch: 0101/0580] Loss: 0.14898  Avg Loss: 0.21412  Avg mIoU:  64.37  
[Epoch: 29] [Batch: 0151/0580] Loss: 0.16594  Avg Loss: 0.21432  Avg mIoU:  64.56  
[Epoch: 29] [Batch: 0201/0580] Loss: 0.21104  Avg Loss: 0.21215  Avg mIoU:  64.22  
[Epoch: 29] [Batch: 0251/0580] Loss: 0.20186  Avg Loss: 0.21394  Avg mIoU:  64.64  
[Epoch: 29] [Batch: 0301/0580] Loss: 0.17166  Avg Loss: 0.21230  Avg mIoU:  64.89  
[Epoch: 29] [Batch: 0351/0580] Loss: 0.28956  Avg Loss: 0.21193  Avg mIoU:  64.84  
[Epoch: 29] [Batch: 0401/0580] Loss: 0.19758  Avg Loss: 0.21108  Avg mIoU:  64.92  
[Epoch: 29] [Batch: 0451/0580] Loss: 0.14459  Avg Loss: 0.20979  Avg mIoU:  64.97  
[Epoch: 29] [Batch: 0501/0580] Loss: 0.12839  Avg Loss: 0.20926  Avg mIoU:  65.17  
[Epoch: 29] [Batch: 0551/0580] Loss: 0.11899  Avg Loss: 0.20866  Avg mIoU:  65.31  

*** Training [@Epoch 29] Avg Loss: 0.20878  Avg mIoU:  65.31  ***

[Epoch: 29] [Batch: 0001/0050] Loss: 0.20438  Avg Loss: 0.20438  Avg mIoU:  61.88  

*** Validation [@Epoch 29] Avg Loss: 0.27811  Avg mIoU:  58.16  ***

[Epoch: 30] [Batch: 0001/0580] Loss: 0.19927  Avg Loss: 0.19927  Avg mIoU:  48.06  
[Epoch: 30] [Batch: 0051/0580] Loss: 0.26918  Avg Loss: 0.20507  Avg mIoU:  65.95  
[Epoch: 30] [Batch: 0101/0580] Loss: 0.14747  Avg Loss: 0.20292  Avg mIoU:  66.03  
[Epoch: 30] [Batch: 0151/0580] Loss: 0.20897  Avg Loss: 0.20594  Avg mIoU:  65.84  
[Epoch: 30] [Batch: 0201/0580] Loss: 0.17197  Avg Loss: 0.20720  Avg mIoU:  65.56  
[Epoch: 30] [Batch: 0251/0580] Loss: 0.28701  Avg Loss: 0.20739  Avg mIoU:  66.04  
[Epoch: 30] [Batch: 0301/0580] Loss: 0.23671  Avg Loss: 0.20740  Avg mIoU:  65.96  
[Epoch: 30] [Batch: 0351/0580] Loss: 0.21758  Avg Loss: 0.20732  Avg mIoU:  65.98  
[Epoch: 30] [Batch: 0401/0580] Loss: 0.15118  Avg Loss: 0.20710  Avg mIoU:  65.87  
[Epoch: 30] [Batch: 0451/0580] Loss: 0.17456  Avg Loss: 0.20651  Avg mIoU:  65.90  
[Epoch: 30] [Batch: 0501/0580] Loss: 0.21309  Avg Loss: 0.20677  Avg mIoU:  65.90  
[Epoch: 30] [Batch: 0551/0580] Loss: 0.22042  Avg Loss: 0.20700  Avg mIoU:  65.93  

*** Training [@Epoch 30] Avg Loss: 0.20673  Avg mIoU:  65.88  ***

[Epoch: 30] [Batch: 0001/0050] Loss: 0.20828  Avg Loss: 0.20828  Avg mIoU:  58.87  

*** Validation [@Epoch 30] Avg Loss: 0.28137  Avg mIoU:  58.89  ***

Model saved @30 w/ val. mIoU: 58.89.

[Epoch: 31] [Batch: 0001/0580] Loss: 0.23462  Avg Loss: 0.23462  Avg mIoU:  33.07  
[Epoch: 31] [Batch: 0051/0580] Loss: 0.16930  Avg Loss: 0.20956  Avg mIoU:  67.55  
[Epoch: 31] [Batch: 0101/0580] Loss: 0.18642  Avg Loss: 0.20970  Avg mIoU:  67.47  
[Epoch: 31] [Batch: 0151/0580] Loss: 0.22695  Avg Loss: 0.20752  Avg mIoU:  67.09  
[Epoch: 31] [Batch: 0201/0580] Loss: 0.21947  Avg Loss: 0.20872  Avg mIoU:  66.41  
[Epoch: 31] [Batch: 0251/0580] Loss: 0.21540  Avg Loss: 0.20606  Avg mIoU:  66.64  
[Epoch: 31] [Batch: 0301/0580] Loss: 0.17109  Avg Loss: 0.20471  Avg mIoU:  66.59  
[Epoch: 31] [Batch: 0351/0580] Loss: 0.22818  Avg Loss: 0.20527  Avg mIoU:  66.48  
[Epoch: 31] [Batch: 0401/0580] Loss: 0.31435  Avg Loss: 0.20527  Avg mIoU:  66.50  
[Epoch: 31] [Batch: 0451/0580] Loss: 0.22953  Avg Loss: 0.20493  Avg mIoU:  66.36  
[Epoch: 31] [Batch: 0501/0580] Loss: 0.20374  Avg Loss: 0.20505  Avg mIoU:  66.28  
[Epoch: 31] [Batch: 0551/0580] Loss: 0.13750  Avg Loss: 0.20523  Avg mIoU:  66.14  

*** Training [@Epoch 31] Avg Loss: 0.20557  Avg mIoU:  66.10  ***

[Epoch: 31] [Batch: 0001/0050] Loss: 0.18482  Avg Loss: 0.18482  Avg mIoU:  59.98  

*** Validation [@Epoch 31] Avg Loss: 0.26122  Avg mIoU:  53.05  ***

[Epoch: 32] [Batch: 0001/0580] Loss: 0.16374  Avg Loss: 0.16374  Avg mIoU:  44.05  
[Epoch: 32] [Batch: 0051/0580] Loss: 0.16679  Avg Loss: 0.20624  Avg mIoU:  65.59  
[Epoch: 32] [Batch: 0101/0580] Loss: 0.19468  Avg Loss: 0.20473  Avg mIoU:  66.67  
[Epoch: 32] [Batch: 0151/0580] Loss: 0.26166  Avg Loss: 0.20410  Avg mIoU:  66.82  
[Epoch: 32] [Batch: 0201/0580] Loss: 0.21944  Avg Loss: 0.20509  Avg mIoU:  67.01  
[Epoch: 32] [Batch: 0251/0580] Loss: 0.25971  Avg Loss: 0.20466  Avg mIoU:  66.61  
[Epoch: 32] [Batch: 0301/0580] Loss: 0.18858  Avg Loss: 0.20276  Avg mIoU:  66.97  
[Epoch: 32] [Batch: 0351/0580] Loss: 0.21427  Avg Loss: 0.20329  Avg mIoU:  66.61  
[Epoch: 32] [Batch: 0401/0580] Loss: 0.21974  Avg Loss: 0.20455  Avg mIoU:  66.54  
[Epoch: 32] [Batch: 0451/0580] Loss: 0.18215  Avg Loss: 0.20331  Avg mIoU:  66.52  
[Epoch: 32] [Batch: 0501/0580] Loss: 0.12563  Avg Loss: 0.20456  Avg mIoU:  66.43  
[Epoch: 32] [Batch: 0551/0580] Loss: 0.14443  Avg Loss: 0.20394  Avg mIoU:  66.37  

*** Training [@Epoch 32] Avg Loss: 0.20437  Avg mIoU:  66.48  ***

[Epoch: 32] [Batch: 0001/0050] Loss: 0.18753  Avg Loss: 0.18753  Avg mIoU:  61.45  

*** Validation [@Epoch 32] Avg Loss: 0.27697  Avg mIoU:  59.81  ***

Model saved @32 w/ val. mIoU: 59.81.

[Epoch: 33] [Batch: 0001/0580] Loss: 0.19051  Avg Loss: 0.19051  Avg mIoU:  47.17  
[Epoch: 33] [Batch: 0051/0580] Loss: 0.16858  Avg Loss: 0.20914  Avg mIoU:  64.49  
[Epoch: 33] [Batch: 0101/0580] Loss: 0.17318  Avg Loss: 0.20586  Avg mIoU:  65.81  
[Epoch: 33] [Batch: 0151/0580] Loss: 0.21421  Avg Loss: 0.20509  Avg mIoU:  66.38  
[Epoch: 33] [Batch: 0201/0580] Loss: 0.13064  Avg Loss: 0.20495  Avg mIoU:  66.52  
[Epoch: 33] [Batch: 0251/0580] Loss: 0.18949  Avg Loss: 0.20641  Avg mIoU:  66.05  
[Epoch: 33] [Batch: 0301/0580] Loss: 0.15602  Avg Loss: 0.20553  Avg mIoU:  66.27  
[Epoch: 33] [Batch: 0351/0580] Loss: 0.22524  Avg Loss: 0.20553  Avg mIoU:  66.31  
[Epoch: 33] [Batch: 0401/0580] Loss: 0.13216  Avg Loss: 0.20496  Avg mIoU:  66.46  
[Epoch: 33] [Batch: 0451/0580] Loss: 0.23184  Avg Loss: 0.20435  Avg mIoU:  66.45  
[Epoch: 33] [Batch: 0501/0580] Loss: 0.18840  Avg Loss: 0.20363  Avg mIoU:  66.63  
[Epoch: 33] [Batch: 0551/0580] Loss: 0.17100  Avg Loss: 0.20408  Avg mIoU:  66.49  

*** Training [@Epoch 33] Avg Loss: 0.20350  Avg mIoU:  66.52  ***

[Epoch: 33] [Batch: 0001/0050] Loss: 0.18927  Avg Loss: 0.18927  Avg mIoU:  60.19  

*** Validation [@Epoch 33] Avg Loss: 0.26000  Avg mIoU:  58.52  ***

[Epoch: 34] [Batch: 0001/0580] Loss: 0.27885  Avg Loss: 0.27885  Avg mIoU:  40.34  
[Epoch: 34] [Batch: 0051/0580] Loss: 0.20555  Avg Loss: 0.19800  Avg mIoU:  67.01  
[Epoch: 34] [Batch: 0101/0580] Loss: 0.20145  Avg Loss: 0.20213  Avg mIoU:  66.15  
[Epoch: 34] [Batch: 0151/0580] Loss: 0.27958  Avg Loss: 0.20234  Avg mIoU:  66.49  
[Epoch: 34] [Batch: 0201/0580] Loss: 0.21524  Avg Loss: 0.20387  Avg mIoU:  66.67  
[Epoch: 34] [Batch: 0251/0580] Loss: 0.16539  Avg Loss: 0.20301  Avg mIoU:  66.34  
[Epoch: 34] [Batch: 0301/0580] Loss: 0.16749  Avg Loss: 0.20205  Avg mIoU:  66.24  
[Epoch: 34] [Batch: 0351/0580] Loss: 0.25915  Avg Loss: 0.20411  Avg mIoU:  66.00  
[Epoch: 34] [Batch: 0401/0580] Loss: 0.15421  Avg Loss: 0.20382  Avg mIoU:  66.20  
[Epoch: 34] [Batch: 0451/0580] Loss: 0.24323  Avg Loss: 0.20449  Avg mIoU:  66.16  
[Epoch: 34] [Batch: 0501/0580] Loss: 0.31175  Avg Loss: 0.20454  Avg mIoU:  66.30  
[Epoch: 34] [Batch: 0551/0580] Loss: 0.13165  Avg Loss: 0.20358  Avg mIoU:  66.32  

*** Training [@Epoch 34] Avg Loss: 0.20405  Avg mIoU:  66.32  ***

[Epoch: 34] [Batch: 0001/0050] Loss: 0.18687  Avg Loss: 0.18687  Avg mIoU:  59.81  

*** Validation [@Epoch 34] Avg Loss: 0.26154  Avg mIoU:  57.03  ***

[Epoch: 35] [Batch: 0001/0580] Loss: 0.13560  Avg Loss: 0.13560  Avg mIoU:  36.07  
[Epoch: 35] [Batch: 0051/0580] Loss: 0.25739  Avg Loss: 0.19736  Avg mIoU:  65.28  
[Epoch: 35] [Batch: 0101/0580] Loss: 0.21182  Avg Loss: 0.20044  Avg mIoU:  67.42  
[Epoch: 35] [Batch: 0151/0580] Loss: 0.23622  Avg Loss: 0.19841  Avg mIoU:  67.23  
[Epoch: 35] [Batch: 0201/0580] Loss: 0.19945  Avg Loss: 0.19787  Avg mIoU:  67.39  
[Epoch: 35] [Batch: 0251/0580] Loss: 0.20865  Avg Loss: 0.20072  Avg mIoU:  66.91  
[Epoch: 35] [Batch: 0301/0580] Loss: 0.21839  Avg Loss: 0.20060  Avg mIoU:  67.04  
[Epoch: 35] [Batch: 0351/0580] Loss: 0.16542  Avg Loss: 0.20143  Avg mIoU:  67.13  
[Epoch: 35] [Batch: 0401/0580] Loss: 0.15939  Avg Loss: 0.20207  Avg mIoU:  66.90  
[Epoch: 35] [Batch: 0451/0580] Loss: 0.21694  Avg Loss: 0.20185  Avg mIoU:  66.81  
[Epoch: 35] [Batch: 0501/0580] Loss: 0.19988  Avg Loss: 0.20106  Avg mIoU:  66.93  
[Epoch: 35] [Batch: 0551/0580] Loss: 0.16671  Avg Loss: 0.20052  Avg mIoU:  66.98  

*** Training [@Epoch 35] Avg Loss: 0.20102  Avg mIoU:  66.82  ***

[Epoch: 35] [Batch: 0001/0050] Loss: 0.17861  Avg Loss: 0.17861  Avg mIoU:  61.68  

*** Validation [@Epoch 35] Avg Loss: 0.25764  Avg mIoU:  58.58  ***

[Epoch: 36] [Batch: 0001/0580] Loss: 0.21830  Avg Loss: 0.21830  Avg mIoU:  39.49  
[Epoch: 36] [Batch: 0051/0580] Loss: 0.28224  Avg Loss: 0.20026  Avg mIoU:  66.21  
[Epoch: 36] [Batch: 0101/0580] Loss: 0.16733  Avg Loss: 0.20183  Avg mIoU:  66.31  
[Epoch: 36] [Batch: 0151/0580] Loss: 0.23660  Avg Loss: 0.20262  Avg mIoU:  66.71  
[Epoch: 36] [Batch: 0201/0580] Loss: 0.14777  Avg Loss: 0.20005  Avg mIoU:  67.05  
[Epoch: 36] [Batch: 0251/0580] Loss: 0.12103  Avg Loss: 0.20094  Avg mIoU:  66.86  
[Epoch: 36] [Batch: 0301/0580] Loss: 0.22318  Avg Loss: 0.20068  Avg mIoU:  66.82  
[Epoch: 36] [Batch: 0351/0580] Loss: 0.13606  Avg Loss: 0.20061  Avg mIoU:  66.85  
[Epoch: 36] [Batch: 0401/0580] Loss: 0.18338  Avg Loss: 0.20141  Avg mIoU:  66.85  
[Epoch: 36] [Batch: 0451/0580] Loss: 0.31766  Avg Loss: 0.20108  Avg mIoU:  66.90  
[Epoch: 36] [Batch: 0501/0580] Loss: 0.24817  Avg Loss: 0.20260  Avg mIoU:  66.70  
[Epoch: 36] [Batch: 0551/0580] Loss: 0.19860  Avg Loss: 0.20205  Avg mIoU:  66.71  

*** Training [@Epoch 36] Avg Loss: 0.20158  Avg mIoU:  66.80  ***

[Epoch: 36] [Batch: 0001/0050] Loss: 0.18254  Avg Loss: 0.18254  Avg mIoU:  59.53  

*** Validation [@Epoch 36] Avg Loss: 0.26424  Avg mIoU:  53.58  ***

[Epoch: 37] [Batch: 0001/0580] Loss: 0.17147  Avg Loss: 0.17147  Avg mIoU:  45.80  
[Epoch: 37] [Batch: 0051/0580] Loss: 0.18482  Avg Loss: 0.19481  Avg mIoU:  66.26  
[Epoch: 37] [Batch: 0101/0580] Loss: 0.19231  Avg Loss: 0.19980  Avg mIoU:  66.01  
[Epoch: 37] [Batch: 0151/0580] Loss: 0.17688  Avg Loss: 0.19732  Avg mIoU:  66.57  
[Epoch: 37] [Batch: 0201/0580] Loss: 0.15329  Avg Loss: 0.19718  Avg mIoU:  66.41  
[Epoch: 37] [Batch: 0251/0580] Loss: 0.18479  Avg Loss: 0.19920  Avg mIoU:  66.76  
[Epoch: 37] [Batch: 0301/0580] Loss: 0.20107  Avg Loss: 0.20050  Avg mIoU:  67.07  
[Epoch: 37] [Batch: 0351/0580] Loss: 0.20648  Avg Loss: 0.19994  Avg mIoU:  67.18  
[Epoch: 37] [Batch: 0401/0580] Loss: 0.21589  Avg Loss: 0.19889  Avg mIoU:  67.43  
[Epoch: 37] [Batch: 0451/0580] Loss: 0.31046  Avg Loss: 0.19949  Avg mIoU:  67.25  
[Epoch: 37] [Batch: 0501/0580] Loss: 0.13833  Avg Loss: 0.19850  Avg mIoU:  67.04  
[Epoch: 37] [Batch: 0551/0580] Loss: 0.20997  Avg Loss: 0.19797  Avg mIoU:  67.01  

*** Training [@Epoch 37] Avg Loss: 0.19791  Avg mIoU:  67.19  ***

[Epoch: 37] [Batch: 0001/0050] Loss: 0.18068  Avg Loss: 0.18068  Avg mIoU:  61.79  

*** Validation [@Epoch 37] Avg Loss: 0.26257  Avg mIoU:  58.57  ***

[Epoch: 38] [Batch: 0001/0580] Loss: 0.16089  Avg Loss: 0.16089  Avg mIoU:  49.82  
[Epoch: 38] [Batch: 0051/0580] Loss: 0.15768  Avg Loss: 0.18762  Avg mIoU:  66.76  
[Epoch: 38] [Batch: 0101/0580] Loss: 0.12946  Avg Loss: 0.19060  Avg mIoU:  66.39  
[Epoch: 38] [Batch: 0151/0580] Loss: 0.18927  Avg Loss: 0.19644  Avg mIoU:  66.21  
[Epoch: 38] [Batch: 0201/0580] Loss: 0.27713  Avg Loss: 0.20100  Avg mIoU:  65.97  
[Epoch: 38] [Batch: 0251/0580] Loss: 0.16680  Avg Loss: 0.19979  Avg mIoU:  66.57  
[Epoch: 38] [Batch: 0301/0580] Loss: 0.22129  Avg Loss: 0.20016  Avg mIoU:  66.70  
[Epoch: 38] [Batch: 0351/0580] Loss: 0.13778  Avg Loss: 0.19842  Avg mIoU:  66.86  
[Epoch: 38] [Batch: 0401/0580] Loss: 0.17301  Avg Loss: 0.19834  Avg mIoU:  66.87  
[Epoch: 38] [Batch: 0451/0580] Loss: 0.20659  Avg Loss: 0.19888  Avg mIoU:  66.87  
[Epoch: 38] [Batch: 0501/0580] Loss: 0.21176  Avg Loss: 0.19855  Avg mIoU:  66.98  
[Epoch: 38] [Batch: 0551/0580] Loss: 0.15873  Avg Loss: 0.19861  Avg mIoU:  67.20  

*** Training [@Epoch 38] Avg Loss: 0.19904  Avg mIoU:  67.17  ***

[Epoch: 38] [Batch: 0001/0050] Loss: 0.20334  Avg Loss: 0.20334  Avg mIoU:  60.42  

*** Validation [@Epoch 38] Avg Loss: 0.28816  Avg mIoU:  59.45  ***

[Epoch: 39] [Batch: 0001/0580] Loss: 0.26133  Avg Loss: 0.26133  Avg mIoU:  39.80  
[Epoch: 39] [Batch: 0051/0580] Loss: 0.20724  Avg Loss: 0.19931  Avg mIoU:  66.67  
[Epoch: 39] [Batch: 0101/0580] Loss: 0.14059  Avg Loss: 0.20150  Avg mIoU:  66.47  
[Epoch: 39] [Batch: 0151/0580] Loss: 0.18420  Avg Loss: 0.20380  Avg mIoU:  65.58  
[Epoch: 39] [Batch: 0201/0580] Loss: 0.24113  Avg Loss: 0.20244  Avg mIoU:  66.14  
[Epoch: 39] [Batch: 0251/0580] Loss: 0.14874  Avg Loss: 0.20028  Avg mIoU:  66.30  
[Epoch: 39] [Batch: 0301/0580] Loss: 0.17655  Avg Loss: 0.20081  Avg mIoU:  66.54  
[Epoch: 39] [Batch: 0351/0580] Loss: 0.17678  Avg Loss: 0.19967  Avg mIoU:  66.87  
[Epoch: 39] [Batch: 0401/0580] Loss: 0.28443  Avg Loss: 0.20010  Avg mIoU:  66.96  
[Epoch: 39] [Batch: 0451/0580] Loss: 0.15004  Avg Loss: 0.19938  Avg mIoU:  67.04  
[Epoch: 39] [Batch: 0501/0580] Loss: 0.23821  Avg Loss: 0.19912  Avg mIoU:  67.31  
[Epoch: 39] [Batch: 0551/0580] Loss: 0.16001  Avg Loss: 0.19849  Avg mIoU:  67.34  

*** Training [@Epoch 39] Avg Loss: 0.19907  Avg mIoU:  67.31  ***

[Epoch: 39] [Batch: 0001/0050] Loss: 0.18320  Avg Loss: 0.18320  Avg mIoU:  52.73  

*** Validation [@Epoch 39] Avg Loss: 0.29067  Avg mIoU:  44.72  ***

[Epoch: 40] [Batch: 0001/0580] Loss: 0.23361  Avg Loss: 0.23361  Avg mIoU:  34.27  
[Epoch: 40] [Batch: 0051/0580] Loss: 0.23238  Avg Loss: 0.19587  Avg mIoU:  66.49  
[Epoch: 40] [Batch: 0101/0580] Loss: 0.21501  Avg Loss: 0.19897  Avg mIoU:  66.69  
[Epoch: 40] [Batch: 0151/0580] Loss: 0.19103  Avg Loss: 0.19600  Avg mIoU:  67.20  
[Epoch: 40] [Batch: 0201/0580] Loss: 0.18834  Avg Loss: 0.19441  Avg mIoU:  67.85  
[Epoch: 40] [Batch: 0251/0580] Loss: 0.18249  Avg Loss: 0.19342  Avg mIoU:  67.85  
[Epoch: 40] [Batch: 0301/0580] Loss: 0.24345  Avg Loss: 0.19493  Avg mIoU:  67.69  
[Epoch: 40] [Batch: 0351/0580] Loss: 0.16558  Avg Loss: 0.19648  Avg mIoU:  67.42  
[Epoch: 40] [Batch: 0401/0580] Loss: 0.27072  Avg Loss: 0.19533  Avg mIoU:  67.72  
[Epoch: 40] [Batch: 0451/0580] Loss: 0.18521  Avg Loss: 0.19620  Avg mIoU:  67.57  
[Epoch: 40] [Batch: 0501/0580] Loss: 0.18912  Avg Loss: 0.19660  Avg mIoU:  67.84  
[Epoch: 40] [Batch: 0551/0580] Loss: 0.18410  Avg Loss: 0.19542  Avg mIoU:  67.77  

*** Training [@Epoch 40] Avg Loss: 0.19565  Avg mIoU:  67.71  ***

[Epoch: 40] [Batch: 0001/0050] Loss: 0.17520  Avg Loss: 0.17520  Avg mIoU:  59.99  

*** Validation [@Epoch 40] Avg Loss: 0.26103  Avg mIoU:  57.89  ***

[Epoch: 41] [Batch: 0001/0580] Loss: 0.12225  Avg Loss: 0.12225  Avg mIoU:  39.45  
[Epoch: 41] [Batch: 0051/0580] Loss: 0.19805  Avg Loss: 0.18910  Avg mIoU:  67.42  
[Epoch: 41] [Batch: 0101/0580] Loss: 0.20122  Avg Loss: 0.18895  Avg mIoU:  66.62  
[Epoch: 41] [Batch: 0151/0580] Loss: 0.18973  Avg Loss: 0.19264  Avg mIoU:  66.79  
[Epoch: 41] [Batch: 0201/0580] Loss: 0.19613  Avg Loss: 0.19455  Avg mIoU:  66.44  
[Epoch: 41] [Batch: 0251/0580] Loss: 0.11191  Avg Loss: 0.19455  Avg mIoU:  66.85  
[Epoch: 41] [Batch: 0301/0580] Loss: 0.24857  Avg Loss: 0.19658  Avg mIoU:  66.82  
[Epoch: 41] [Batch: 0351/0580] Loss: 0.17102  Avg Loss: 0.19564  Avg mIoU:  67.11  
[Epoch: 41] [Batch: 0401/0580] Loss: 0.20751  Avg Loss: 0.19545  Avg mIoU:  67.09  
[Epoch: 41] [Batch: 0451/0580] Loss: 0.17162  Avg Loss: 0.19427  Avg mIoU:  67.35  
[Epoch: 41] [Batch: 0501/0580] Loss: 0.32103  Avg Loss: 0.19451  Avg mIoU:  67.41  
[Epoch: 41] [Batch: 0551/0580] Loss: 0.25003  Avg Loss: 0.19565  Avg mIoU:  67.45  

*** Training [@Epoch 41] Avg Loss: 0.19650  Avg mIoU:  67.48  ***

[Epoch: 41] [Batch: 0001/0050] Loss: 0.17703  Avg Loss: 0.17703  Avg mIoU:  61.14  

*** Validation [@Epoch 41] Avg Loss: 0.26705  Avg mIoU:  57.68  ***

[Epoch: 42] [Batch: 0001/0580] Loss: 0.16368  Avg Loss: 0.16368  Avg mIoU:  28.53  
[Epoch: 42] [Batch: 0051/0580] Loss: 0.18711  Avg Loss: 0.20585  Avg mIoU:  67.68  
[Epoch: 42] [Batch: 0101/0580] Loss: 0.16794  Avg Loss: 0.19907  Avg mIoU:  68.89  
[Epoch: 42] [Batch: 0151/0580] Loss: 0.21844  Avg Loss: 0.19377  Avg mIoU:  69.10  
[Epoch: 42] [Batch: 0201/0580] Loss: 0.15380  Avg Loss: 0.19271  Avg mIoU:  68.56  
[Epoch: 42] [Batch: 0251/0580] Loss: 0.17760  Avg Loss: 0.19268  Avg mIoU:  68.36  
[Epoch: 42] [Batch: 0301/0580] Loss: 0.14587  Avg Loss: 0.19146  Avg mIoU:  68.32  
[Epoch: 42] [Batch: 0351/0580] Loss: 0.19627  Avg Loss: 0.19284  Avg mIoU:  68.33  
[Epoch: 42] [Batch: 0401/0580] Loss: 0.17142  Avg Loss: 0.19359  Avg mIoU:  68.20  
[Epoch: 42] [Batch: 0451/0580] Loss: 0.23380  Avg Loss: 0.19371  Avg mIoU:  68.17  
[Epoch: 42] [Batch: 0501/0580] Loss: 0.22328  Avg Loss: 0.19350  Avg mIoU:  68.18  
[Epoch: 42] [Batch: 0551/0580] Loss: 0.22157  Avg Loss: 0.19388  Avg mIoU:  68.02  

*** Training [@Epoch 42] Avg Loss: 0.19434  Avg mIoU:  68.04  ***

[Epoch: 42] [Batch: 0001/0050] Loss: 0.16876  Avg Loss: 0.16876  Avg mIoU:  60.47  

*** Validation [@Epoch 42] Avg Loss: 0.25405  Avg mIoU:  59.25  ***

[Epoch: 43] [Batch: 0001/0580] Loss: 0.28187  Avg Loss: 0.28187  Avg mIoU:  29.99  
[Epoch: 43] [Batch: 0051/0580] Loss: 0.15473  Avg Loss: 0.19343  Avg mIoU:  66.91  
[Epoch: 43] [Batch: 0101/0580] Loss: 0.15276  Avg Loss: 0.19994  Avg mIoU:  67.03  
[Epoch: 43] [Batch: 0151/0580] Loss: 0.27171  Avg Loss: 0.19799  Avg mIoU:  67.23  
[Epoch: 43] [Batch: 0201/0580] Loss: 0.14622  Avg Loss: 0.19641  Avg mIoU:  67.26  
[Epoch: 43] [Batch: 0251/0580] Loss: 0.20823  Avg Loss: 0.19637  Avg mIoU:  67.37  
[Epoch: 43] [Batch: 0301/0580] Loss: 0.25235  Avg Loss: 0.19575  Avg mIoU:  67.35  
[Epoch: 43] [Batch: 0351/0580] Loss: 0.24298  Avg Loss: 0.19706  Avg mIoU:  67.44  
[Epoch: 43] [Batch: 0401/0580] Loss: 0.23218  Avg Loss: 0.19504  Avg mIoU:  67.46  
[Epoch: 43] [Batch: 0451/0580] Loss: 0.18691  Avg Loss: 0.19488  Avg mIoU:  67.67  
[Epoch: 43] [Batch: 0501/0580] Loss: 0.19299  Avg Loss: 0.19429  Avg mIoU:  67.79  
[Epoch: 43] [Batch: 0551/0580] Loss: 0.21965  Avg Loss: 0.19520  Avg mIoU:  67.82  

*** Training [@Epoch 43] Avg Loss: 0.19502  Avg mIoU:  67.85  ***

[Epoch: 43] [Batch: 0001/0050] Loss: 0.16448  Avg Loss: 0.16448  Avg mIoU:  61.42  

*** Validation [@Epoch 43] Avg Loss: 0.24723  Avg mIoU:  57.43  ***

[Epoch: 44] [Batch: 0001/0580] Loss: 0.19319  Avg Loss: 0.19319  Avg mIoU:  47.04  
[Epoch: 44] [Batch: 0051/0580] Loss: 0.16297  Avg Loss: 0.20378  Avg mIoU:  66.02  
[Epoch: 44] [Batch: 0101/0580] Loss: 0.16798  Avg Loss: 0.19726  Avg mIoU:  66.79  
[Epoch: 44] [Batch: 0151/0580] Loss: 0.21786  Avg Loss: 0.19737  Avg mIoU:  68.01  
[Epoch: 44] [Batch: 0201/0580] Loss: 0.15215  Avg Loss: 0.19932  Avg mIoU:  67.75  
[Epoch: 44] [Batch: 0251/0580] Loss: 0.20312  Avg Loss: 0.19845  Avg mIoU:  68.01  
[Epoch: 44] [Batch: 0301/0580] Loss: 0.15473  Avg Loss: 0.19669  Avg mIoU:  67.93  
[Epoch: 44] [Batch: 0351/0580] Loss: 0.17154  Avg Loss: 0.19467  Avg mIoU:  67.98  
[Epoch: 44] [Batch: 0401/0580] Loss: 0.12173  Avg Loss: 0.19480  Avg mIoU:  67.82  
[Epoch: 44] [Batch: 0451/0580] Loss: 0.21173  Avg Loss: 0.19468  Avg mIoU:  67.84  
[Epoch: 44] [Batch: 0501/0580] Loss: 0.13099  Avg Loss: 0.19397  Avg mIoU:  67.85  
[Epoch: 44] [Batch: 0551/0580] Loss: 0.13563  Avg Loss: 0.19267  Avg mIoU:  68.00  

*** Training [@Epoch 44] Avg Loss: 0.19304  Avg mIoU:  68.00  ***

[Epoch: 44] [Batch: 0001/0050] Loss: 0.18526  Avg Loss: 0.18526  Avg mIoU:  63.01  

*** Validation [@Epoch 44] Avg Loss: 0.26872  Avg mIoU:  58.60  ***

[Epoch: 45] [Batch: 0001/0580] Loss: 0.16616  Avg Loss: 0.16616  Avg mIoU:  41.34  
[Epoch: 45] [Batch: 0051/0580] Loss: 0.19364  Avg Loss: 0.20319  Avg mIoU:  66.82  
[Epoch: 45] [Batch: 0101/0580] Loss: 0.21262  Avg Loss: 0.19625  Avg mIoU:  66.91  
[Epoch: 45] [Batch: 0151/0580] Loss: 0.19758  Avg Loss: 0.19674  Avg mIoU:  66.98  
[Epoch: 45] [Batch: 0201/0580] Loss: 0.21536  Avg Loss: 0.19365  Avg mIoU:  67.66  
[Epoch: 45] [Batch: 0251/0580] Loss: 0.18820  Avg Loss: 0.19353  Avg mIoU:  67.52  
[Epoch: 45] [Batch: 0301/0580] Loss: 0.22332  Avg Loss: 0.19471  Avg mIoU:  67.35  
[Epoch: 45] [Batch: 0351/0580] Loss: 0.20549  Avg Loss: 0.19411  Avg mIoU:  67.35  
[Epoch: 45] [Batch: 0401/0580] Loss: 0.11009  Avg Loss: 0.19414  Avg mIoU:  67.40  
[Epoch: 45] [Batch: 0451/0580] Loss: 0.15524  Avg Loss: 0.19384  Avg mIoU:  67.47  
[Epoch: 45] [Batch: 0501/0580] Loss: 0.16641  Avg Loss: 0.19340  Avg mIoU:  67.69  
[Epoch: 45] [Batch: 0551/0580] Loss: 0.15176  Avg Loss: 0.19333  Avg mIoU:  67.83  

*** Training [@Epoch 45] Avg Loss: 0.19302  Avg mIoU:  67.85  ***

[Epoch: 45] [Batch: 0001/0050] Loss: 0.17162  Avg Loss: 0.17162  Avg mIoU:  64.33  

*** Validation [@Epoch 45] Avg Loss: 0.25393  Avg mIoU:  57.60  ***

[Epoch: 46] [Batch: 0001/0580] Loss: 0.15870  Avg Loss: 0.15870  Avg mIoU:  42.85  
[Epoch: 46] [Batch: 0051/0580] Loss: 0.18974  Avg Loss: 0.19478  Avg mIoU:  65.42  
[Epoch: 46] [Batch: 0101/0580] Loss: 0.16805  Avg Loss: 0.19526  Avg mIoU:  67.27  
[Epoch: 46] [Batch: 0151/0580] Loss: 0.17616  Avg Loss: 0.19174  Avg mIoU:  67.35  
[Epoch: 46] [Batch: 0201/0580] Loss: 0.18203  Avg Loss: 0.19155  Avg mIoU:  68.02  
[Epoch: 46] [Batch: 0251/0580] Loss: 0.15109  Avg Loss: 0.19390  Avg mIoU:  67.94  
[Epoch: 46] [Batch: 0301/0580] Loss: 0.19199  Avg Loss: 0.19552  Avg mIoU:  67.89  
[Epoch: 46] [Batch: 0351/0580] Loss: 0.21993  Avg Loss: 0.19571  Avg mIoU:  67.98  
[Epoch: 46] [Batch: 0401/0580] Loss: 0.17618  Avg Loss: 0.19407  Avg mIoU:  67.96  
[Epoch: 46] [Batch: 0451/0580] Loss: 0.15055  Avg Loss: 0.19298  Avg mIoU:  67.97  
[Epoch: 46] [Batch: 0501/0580] Loss: 0.18003  Avg Loss: 0.19253  Avg mIoU:  68.15  
[Epoch: 46] [Batch: 0551/0580] Loss: 0.18355  Avg Loss: 0.19220  Avg mIoU:  68.05  

*** Training [@Epoch 46] Avg Loss: 0.19200  Avg mIoU:  68.07  ***

[Epoch: 46] [Batch: 0001/0050] Loss: 0.17393  Avg Loss: 0.17393  Avg mIoU:  65.05  

*** Validation [@Epoch 46] Avg Loss: 0.25905  Avg mIoU:  59.11  ***

[Epoch: 47] [Batch: 0001/0580] Loss: 0.16122  Avg Loss: 0.16122  Avg mIoU:  49.74  
[Epoch: 47] [Batch: 0051/0580] Loss: 0.15253  Avg Loss: 0.19248  Avg mIoU:  67.01  
[Epoch: 47] [Batch: 0101/0580] Loss: 0.14245  Avg Loss: 0.18745  Avg mIoU:  68.00  
[Epoch: 47] [Batch: 0151/0580] Loss: 0.25467  Avg Loss: 0.18853  Avg mIoU:  68.20  
[Epoch: 47] [Batch: 0201/0580] Loss: 0.16063  Avg Loss: 0.18981  Avg mIoU:  68.52  
[Epoch: 47] [Batch: 0251/0580] Loss: 0.15311  Avg Loss: 0.18916  Avg mIoU:  68.51  
[Epoch: 47] [Batch: 0301/0580] Loss: 0.22626  Avg Loss: 0.19058  Avg mIoU:  68.48  
[Epoch: 47] [Batch: 0351/0580] Loss: 0.14259  Avg Loss: 0.19251  Avg mIoU:  68.36  
[Epoch: 47] [Batch: 0401/0580] Loss: 0.17965  Avg Loss: 0.19183  Avg mIoU:  68.33  
[Epoch: 47] [Batch: 0451/0580] Loss: 0.14716  Avg Loss: 0.19151  Avg mIoU:  68.26  
[Epoch: 47] [Batch: 0501/0580] Loss: 0.12565  Avg Loss: 0.19178  Avg mIoU:  68.29  
[Epoch: 47] [Batch: 0551/0580] Loss: 0.18704  Avg Loss: 0.19129  Avg mIoU:  68.38  

*** Training [@Epoch 47] Avg Loss: 0.19159  Avg mIoU:  68.36  ***

[Epoch: 47] [Batch: 0001/0050] Loss: 0.17383  Avg Loss: 0.17383  Avg mIoU:  66.31  

*** Validation [@Epoch 47] Avg Loss: 0.25122  Avg mIoU:  60.78  ***

Model saved @47 w/ val. mIoU: 60.78.

[Epoch: 48] [Batch: 0001/0580] Loss: 0.17718  Avg Loss: 0.17718  Avg mIoU:  47.59  
[Epoch: 48] [Batch: 0051/0580] Loss: 0.22831  Avg Loss: 0.19045  Avg mIoU:  68.45  
[Epoch: 48] [Batch: 0101/0580] Loss: 0.22231  Avg Loss: 0.19439  Avg mIoU:  69.04  
[Epoch: 48] [Batch: 0151/0580] Loss: 0.20772  Avg Loss: 0.19829  Avg mIoU:  68.55  
[Epoch: 48] [Batch: 0201/0580] Loss: 0.21077  Avg Loss: 0.19381  Avg mIoU:  68.77  
[Epoch: 48] [Batch: 0251/0580] Loss: 0.17537  Avg Loss: 0.19271  Avg mIoU:  68.56  
[Epoch: 48] [Batch: 0301/0580] Loss: 0.20104  Avg Loss: 0.19430  Avg mIoU:  68.78  
[Epoch: 48] [Batch: 0351/0580] Loss: 0.15991  Avg Loss: 0.19341  Avg mIoU:  68.81  
[Epoch: 48] [Batch: 0401/0580] Loss: 0.25847  Avg Loss: 0.19149  Avg mIoU:  68.96  
[Epoch: 48] [Batch: 0451/0580] Loss: 0.19812  Avg Loss: 0.19245  Avg mIoU:  68.93  
[Epoch: 48] [Batch: 0501/0580] Loss: 0.15809  Avg Loss: 0.19356  Avg mIoU:  68.77  
[Epoch: 48] [Batch: 0551/0580] Loss: 0.18674  Avg Loss: 0.19337  Avg mIoU:  68.67  

*** Training [@Epoch 48] Avg Loss: 0.19287  Avg mIoU:  68.49  ***

[Epoch: 48] [Batch: 0001/0050] Loss: 0.17530  Avg Loss: 0.17530  Avg mIoU:  58.57  

*** Validation [@Epoch 48] Avg Loss: 0.26580  Avg mIoU:  55.17  ***

[Epoch: 49] [Batch: 0001/0580] Loss: 0.17436  Avg Loss: 0.17436  Avg mIoU:  32.23  
[Epoch: 49] [Batch: 0051/0580] Loss: 0.13866  Avg Loss: 0.18452  Avg mIoU:  68.92  
[Epoch: 49] [Batch: 0101/0580] Loss: 0.18846  Avg Loss: 0.18682  Avg mIoU:  68.70  
[Epoch: 49] [Batch: 0151/0580] Loss: 0.17678  Avg Loss: 0.18526  Avg mIoU:  69.29  
[Epoch: 49] [Batch: 0201/0580] Loss: 0.16852  Avg Loss: 0.18291  Avg mIoU:  69.17  
[Epoch: 49] [Batch: 0251/0580] Loss: 0.26378  Avg Loss: 0.18457  Avg mIoU:  68.90  
[Epoch: 49] [Batch: 0301/0580] Loss: 0.15348  Avg Loss: 0.18644  Avg mIoU:  68.78  
[Epoch: 49] [Batch: 0351/0580] Loss: 0.20009  Avg Loss: 0.18755  Avg mIoU:  68.97  
[Epoch: 49] [Batch: 0401/0580] Loss: 0.15535  Avg Loss: 0.18872  Avg mIoU:  68.97  
[Epoch: 49] [Batch: 0451/0580] Loss: 0.18310  Avg Loss: 0.18904  Avg mIoU:  68.77  
[Epoch: 49] [Batch: 0501/0580] Loss: 0.16708  Avg Loss: 0.18894  Avg mIoU:  68.68  
[Epoch: 49] [Batch: 0551/0580] Loss: 0.14325  Avg Loss: 0.19050  Avg mIoU:  68.75  

*** Training [@Epoch 49] Avg Loss: 0.19050  Avg mIoU:  68.62  ***

[Epoch: 49] [Batch: 0001/0050] Loss: 0.17775  Avg Loss: 0.17775  Avg mIoU:  60.19  

*** Validation [@Epoch 49] Avg Loss: 0.26037  Avg mIoU:  56.33  ***

[Epoch: 50] [Batch: 0001/0580] Loss: 0.20114  Avg Loss: 0.20114  Avg mIoU:  30.74  
[Epoch: 50] [Batch: 0051/0580] Loss: 0.18556  Avg Loss: 0.19180  Avg mIoU:  67.50  
[Epoch: 50] [Batch: 0101/0580] Loss: 0.20982  Avg Loss: 0.19420  Avg mIoU:  67.21  
[Epoch: 50] [Batch: 0151/0580] Loss: 0.18112  Avg Loss: 0.19284  Avg mIoU:  67.04  
[Epoch: 50] [Batch: 0201/0580] Loss: 0.21762  Avg Loss: 0.19297  Avg mIoU:  67.27  
[Epoch: 50] [Batch: 0251/0580] Loss: 0.15458  Avg Loss: 0.19166  Avg mIoU:  67.27  
[Epoch: 50] [Batch: 0301/0580] Loss: 0.18744  Avg Loss: 0.19238  Avg mIoU:  67.31  
[Epoch: 50] [Batch: 0351/0580] Loss: 0.12823  Avg Loss: 0.19166  Avg mIoU:  67.66  
[Epoch: 50] [Batch: 0401/0580] Loss: 0.22970  Avg Loss: 0.19113  Avg mIoU:  67.85  
[Epoch: 50] [Batch: 0451/0580] Loss: 0.27932  Avg Loss: 0.19155  Avg mIoU:  67.95  
[Epoch: 50] [Batch: 0501/0580] Loss: 0.13960  Avg Loss: 0.19136  Avg mIoU:  67.95  
[Epoch: 50] [Batch: 0551/0580] Loss: 0.18541  Avg Loss: 0.19177  Avg mIoU:  68.02  

*** Training [@Epoch 50] Avg Loss: 0.19141  Avg mIoU:  68.08  ***

[Epoch: 50] [Batch: 0001/0050] Loss: 0.18034  Avg Loss: 0.18034  Avg mIoU:  57.76  

*** Validation [@Epoch 50] Avg Loss: 0.26284  Avg mIoU:  53.50  ***

[Epoch: 51] [Batch: 0001/0580] Loss: 0.15361  Avg Loss: 0.15361  Avg mIoU:  46.34  
[Epoch: 51] [Batch: 0051/0580] Loss: 0.20478  Avg Loss: 0.18740  Avg mIoU:  70.24  
[Epoch: 51] [Batch: 0101/0580] Loss: 0.22581  Avg Loss: 0.18849  Avg mIoU:  68.86  
[Epoch: 51] [Batch: 0151/0580] Loss: 0.13441  Avg Loss: 0.18974  Avg mIoU:  69.08  
[Epoch: 51] [Batch: 0201/0580] Loss: 0.15643  Avg Loss: 0.19023  Avg mIoU:  68.61  
[Epoch: 51] [Batch: 0251/0580] Loss: 0.26306  Avg Loss: 0.18861  Avg mIoU:  68.76  
[Epoch: 51] [Batch: 0301/0580] Loss: 0.17892  Avg Loss: 0.19056  Avg mIoU:  68.37  
[Epoch: 51] [Batch: 0351/0580] Loss: 0.15760  Avg Loss: 0.19032  Avg mIoU:  68.20  
[Epoch: 51] [Batch: 0401/0580] Loss: 0.16851  Avg Loss: 0.19022  Avg mIoU:  68.37  
[Epoch: 51] [Batch: 0451/0580] Loss: 0.19863  Avg Loss: 0.18965  Avg mIoU:  68.46  
[Epoch: 51] [Batch: 0501/0580] Loss: 0.16286  Avg Loss: 0.18879  Avg mIoU:  68.48  
[Epoch: 51] [Batch: 0551/0580] Loss: 0.13402  Avg Loss: 0.18866  Avg mIoU:  68.58  

*** Training [@Epoch 51] Avg Loss: 0.18885  Avg mIoU:  68.64  ***

[Epoch: 51] [Batch: 0001/0050] Loss: 0.16611  Avg Loss: 0.16611  Avg mIoU:  60.21  

*** Validation [@Epoch 51] Avg Loss: 0.24763  Avg mIoU:  56.48  ***

[Epoch: 52] [Batch: 0001/0580] Loss: 0.19823  Avg Loss: 0.19823  Avg mIoU:  34.56  
[Epoch: 52] [Batch: 0051/0580] Loss: 0.23790  Avg Loss: 0.18194  Avg mIoU:  67.67  
[Epoch: 52] [Batch: 0101/0580] Loss: 0.15394  Avg Loss: 0.17955  Avg mIoU:  68.39  
[Epoch: 52] [Batch: 0151/0580] Loss: 0.17583  Avg Loss: 0.18312  Avg mIoU:  69.06  
[Epoch: 52] [Batch: 0201/0580] Loss: 0.24530  Avg Loss: 0.18550  Avg mIoU:  68.86  
[Epoch: 52] [Batch: 0251/0580] Loss: 0.18028  Avg Loss: 0.18804  Avg mIoU:  68.36  
[Epoch: 52] [Batch: 0301/0580] Loss: 0.27023  Avg Loss: 0.18985  Avg mIoU:  68.42  
[Epoch: 52] [Batch: 0351/0580] Loss: 0.16530  Avg Loss: 0.18956  Avg mIoU:  68.47  
[Epoch: 52] [Batch: 0401/0580] Loss: 0.13728  Avg Loss: 0.18881  Avg mIoU:  68.39  
[Epoch: 52] [Batch: 0451/0580] Loss: 0.19344  Avg Loss: 0.18754  Avg mIoU:  68.53  
[Epoch: 52] [Batch: 0501/0580] Loss: 0.21913  Avg Loss: 0.18762  Avg mIoU:  68.46  
[Epoch: 52] [Batch: 0551/0580] Loss: 0.15630  Avg Loss: 0.18814  Avg mIoU:  68.48  

*** Training [@Epoch 52] Avg Loss: 0.18862  Avg mIoU:  68.51  ***

[Epoch: 52] [Batch: 0001/0050] Loss: 0.17762  Avg Loss: 0.17762  Avg mIoU:  60.84  

*** Validation [@Epoch 52] Avg Loss: 0.25283  Avg mIoU:  60.90  ***

Model saved @52 w/ val. mIoU: 60.90.

[Epoch: 53] [Batch: 0001/0580] Loss: 0.17537  Avg Loss: 0.17537  Avg mIoU:  36.92  
[Epoch: 53] [Batch: 0051/0580] Loss: 0.15985  Avg Loss: 0.19074  Avg mIoU:  68.73  
[Epoch: 53] [Batch: 0101/0580] Loss: 0.15700  Avg Loss: 0.19386  Avg mIoU:  68.88  
[Epoch: 53] [Batch: 0151/0580] Loss: 0.24628  Avg Loss: 0.19266  Avg mIoU:  68.00  
[Epoch: 53] [Batch: 0201/0580] Loss: 0.19258  Avg Loss: 0.19094  Avg mIoU:  68.76  
[Epoch: 53] [Batch: 0251/0580] Loss: 0.17235  Avg Loss: 0.18858  Avg mIoU:  68.88  
[Epoch: 53] [Batch: 0301/0580] Loss: 0.14116  Avg Loss: 0.18890  Avg mIoU:  68.67  
[Epoch: 53] [Batch: 0351/0580] Loss: 0.20651  Avg Loss: 0.19006  Avg mIoU:  68.34  
[Epoch: 53] [Batch: 0401/0580] Loss: 0.20998  Avg Loss: 0.18889  Avg mIoU:  68.50  
[Epoch: 53] [Batch: 0451/0580] Loss: 0.23136  Avg Loss: 0.18950  Avg mIoU:  68.60  
[Epoch: 53] [Batch: 0501/0580] Loss: 0.27595  Avg Loss: 0.18887  Avg mIoU:  68.61  
[Epoch: 53] [Batch: 0551/0580] Loss: 0.23174  Avg Loss: 0.18903  Avg mIoU:  68.71  

*** Training [@Epoch 53] Avg Loss: 0.18868  Avg mIoU:  68.74  ***

[Epoch: 53] [Batch: 0001/0050] Loss: 0.17520  Avg Loss: 0.17520  Avg mIoU:  60.37  

*** Validation [@Epoch 53] Avg Loss: 0.26576  Avg mIoU:  56.01  ***

[Epoch: 54] [Batch: 0001/0580] Loss: 0.21941  Avg Loss: 0.21941  Avg mIoU:  39.26  
[Epoch: 54] [Batch: 0051/0580] Loss: 0.21847  Avg Loss: 0.18121  Avg mIoU:  69.12  
[Epoch: 54] [Batch: 0101/0580] Loss: 0.18362  Avg Loss: 0.18393  Avg mIoU:  70.05  
[Epoch: 54] [Batch: 0151/0580] Loss: 0.18277  Avg Loss: 0.18398  Avg mIoU:  70.16  
[Epoch: 54] [Batch: 0201/0580] Loss: 0.27923  Avg Loss: 0.18440  Avg mIoU:  70.55  
[Epoch: 54] [Batch: 0251/0580] Loss: 0.14879  Avg Loss: 0.18438  Avg mIoU:  70.35  
[Epoch: 54] [Batch: 0301/0580] Loss: 0.13869  Avg Loss: 0.18408  Avg mIoU:  69.85  
[Epoch: 54] [Batch: 0351/0580] Loss: 0.18153  Avg Loss: 0.18544  Avg mIoU:  69.41  
[Epoch: 54] [Batch: 0401/0580] Loss: 0.21306  Avg Loss: 0.18676  Avg mIoU:  69.16  
[Epoch: 54] [Batch: 0451/0580] Loss: 0.15031  Avg Loss: 0.18640  Avg mIoU:  69.18  
[Epoch: 54] [Batch: 0501/0580] Loss: 0.14871  Avg Loss: 0.18603  Avg mIoU:  69.07  
[Epoch: 54] [Batch: 0551/0580] Loss: 0.16979  Avg Loss: 0.18647  Avg mIoU:  68.88  

*** Training [@Epoch 54] Avg Loss: 0.18606  Avg mIoU:  68.86  ***

[Epoch: 54] [Batch: 0001/0050] Loss: 0.16379  Avg Loss: 0.16379  Avg mIoU:  63.30  

*** Validation [@Epoch 54] Avg Loss: 0.24791  Avg mIoU:  60.37  ***

[Epoch: 55] [Batch: 0001/0580] Loss: 0.26356  Avg Loss: 0.26356  Avg mIoU:  45.85  
[Epoch: 55] [Batch: 0051/0580] Loss: 0.17538  Avg Loss: 0.17979  Avg mIoU:  69.78  
[Epoch: 55] [Batch: 0101/0580] Loss: 0.12481  Avg Loss: 0.18692  Avg mIoU:  68.88  
[Epoch: 55] [Batch: 0151/0580] Loss: 0.20630  Avg Loss: 0.18740  Avg mIoU:  68.56  
[Epoch: 55] [Batch: 0201/0580] Loss: 0.12560  Avg Loss: 0.18757  Avg mIoU:  68.57  
[Epoch: 55] [Batch: 0251/0580] Loss: 0.13227  Avg Loss: 0.18688  Avg mIoU:  68.94  
[Epoch: 55] [Batch: 0301/0580] Loss: 0.22705  Avg Loss: 0.18619  Avg mIoU:  68.92  
[Epoch: 55] [Batch: 0351/0580] Loss: 0.24118  Avg Loss: 0.18654  Avg mIoU:  69.10  
[Epoch: 55] [Batch: 0401/0580] Loss: 0.20867  Avg Loss: 0.18810  Avg mIoU:  69.12  
[Epoch: 55] [Batch: 0451/0580] Loss: 0.19786  Avg Loss: 0.18814  Avg mIoU:  69.05  
[Epoch: 55] [Batch: 0501/0580] Loss: 0.16456  Avg Loss: 0.18628  Avg mIoU:  69.21  
[Epoch: 55] [Batch: 0551/0580] Loss: 0.16607  Avg Loss: 0.18593  Avg mIoU:  69.21  

*** Training [@Epoch 55] Avg Loss: 0.18605  Avg mIoU:  69.05  ***

[Epoch: 55] [Batch: 0001/0050] Loss: 0.18195  Avg Loss: 0.18195  Avg mIoU:  63.46  

*** Validation [@Epoch 55] Avg Loss: 0.26703  Avg mIoU:  59.48  ***

[Epoch: 56] [Batch: 0001/0580] Loss: 0.25460  Avg Loss: 0.25460  Avg mIoU:  38.35  
[Epoch: 56] [Batch: 0051/0580] Loss: 0.20879  Avg Loss: 0.18857  Avg mIoU:  69.13  
[Epoch: 56] [Batch: 0101/0580] Loss: 0.15979  Avg Loss: 0.18915  Avg mIoU:  69.48  
[Epoch: 56] [Batch: 0151/0580] Loss: 0.28914  Avg Loss: 0.18900  Avg mIoU:  68.66  
[Epoch: 56] [Batch: 0201/0580] Loss: 0.19241  Avg Loss: 0.18773  Avg mIoU:  68.61  
[Epoch: 56] [Batch: 0251/0580] Loss: 0.20328  Avg Loss: 0.18768  Avg mIoU:  68.38  
[Epoch: 56] [Batch: 0301/0580] Loss: 0.25234  Avg Loss: 0.18954  Avg mIoU:  68.24  
[Epoch: 56] [Batch: 0351/0580] Loss: 0.14350  Avg Loss: 0.18855  Avg mIoU:  68.55  
[Epoch: 56] [Batch: 0401/0580] Loss: 0.19175  Avg Loss: 0.18809  Avg mIoU:  68.68  
[Epoch: 56] [Batch: 0451/0580] Loss: 0.21137  Avg Loss: 0.18787  Avg mIoU:  68.75  
[Epoch: 56] [Batch: 0501/0580] Loss: 0.16166  Avg Loss: 0.18762  Avg mIoU:  68.75  
[Epoch: 56] [Batch: 0551/0580] Loss: 0.14268  Avg Loss: 0.18723  Avg mIoU:  68.82  

*** Training [@Epoch 56] Avg Loss: 0.18770  Avg mIoU:  68.81  ***

[Epoch: 56] [Batch: 0001/0050] Loss: 0.16900  Avg Loss: 0.16900  Avg mIoU:  65.56  

*** Validation [@Epoch 56] Avg Loss: 0.25529  Avg mIoU:  58.20  ***

[Epoch: 57] [Batch: 0001/0580] Loss: 0.17202  Avg Loss: 0.17202  Avg mIoU:  58.27  
[Epoch: 57] [Batch: 0051/0580] Loss: 0.13974  Avg Loss: 0.18400  Avg mIoU:  69.81  
[Epoch: 57] [Batch: 0101/0580] Loss: 0.18471  Avg Loss: 0.18240  Avg mIoU:  69.71  
[Epoch: 57] [Batch: 0151/0580] Loss: 0.22764  Avg Loss: 0.18802  Avg mIoU:  68.86  
[Epoch: 57] [Batch: 0201/0580] Loss: 0.18840  Avg Loss: 0.18802  Avg mIoU:  68.82  
[Epoch: 57] [Batch: 0251/0580] Loss: 0.17289  Avg Loss: 0.18885  Avg mIoU:  68.82  
[Epoch: 57] [Batch: 0301/0580] Loss: 0.17345  Avg Loss: 0.18773  Avg mIoU:  68.97  
[Epoch: 57] [Batch: 0351/0580] Loss: 0.21351  Avg Loss: 0.18588  Avg mIoU:  69.16  
[Epoch: 57] [Batch: 0401/0580] Loss: 0.17087  Avg Loss: 0.18559  Avg mIoU:  69.14  
[Epoch: 57] [Batch: 0451/0580] Loss: 0.09795  Avg Loss: 0.18612  Avg mIoU:  69.10  
[Epoch: 57] [Batch: 0501/0580] Loss: 0.16296  Avg Loss: 0.18661  Avg mIoU:  68.97  
[Epoch: 57] [Batch: 0551/0580] Loss: 0.14450  Avg Loss: 0.18529  Avg mIoU:  69.19  

*** Training [@Epoch 57] Avg Loss: 0.18558  Avg mIoU:  69.05  ***

[Epoch: 57] [Batch: 0001/0050] Loss: 0.17473  Avg Loss: 0.17473  Avg mIoU:  60.34  

*** Validation [@Epoch 57] Avg Loss: 0.24686  Avg mIoU:  60.88  ***

[Epoch: 58] [Batch: 0001/0580] Loss: 0.17285  Avg Loss: 0.17285  Avg mIoU:  37.85  
[Epoch: 58] [Batch: 0051/0580] Loss: 0.18883  Avg Loss: 0.18424  Avg mIoU:  69.49  
[Epoch: 58] [Batch: 0101/0580] Loss: 0.16345  Avg Loss: 0.18366  Avg mIoU:  68.88  
[Epoch: 58] [Batch: 0151/0580] Loss: 0.17961  Avg Loss: 0.17895  Avg mIoU:  69.23  
[Epoch: 58] [Batch: 0201/0580] Loss: 0.15815  Avg Loss: 0.18004  Avg mIoU:  69.53  
[Epoch: 58] [Batch: 0251/0580] Loss: 0.16657  Avg Loss: 0.18035  Avg mIoU:  69.41  
[Epoch: 58] [Batch: 0301/0580] Loss: 0.15688  Avg Loss: 0.18085  Avg mIoU:  69.53  
[Epoch: 58] [Batch: 0351/0580] Loss: 0.21418  Avg Loss: 0.18219  Avg mIoU:  69.43  
[Epoch: 58] [Batch: 0401/0580] Loss: 0.14526  Avg Loss: 0.18208  Avg mIoU:  69.54  
[Epoch: 58] [Batch: 0451/0580] Loss: 0.13276  Avg Loss: 0.18305  Avg mIoU:  69.33  
[Epoch: 58] [Batch: 0501/0580] Loss: 0.23329  Avg Loss: 0.18389  Avg mIoU:  69.37  
[Epoch: 58] [Batch: 0551/0580] Loss: 0.16042  Avg Loss: 0.18479  Avg mIoU:  69.33  

*** Training [@Epoch 58] Avg Loss: 0.18477  Avg mIoU:  69.25  ***

[Epoch: 58] [Batch: 0001/0050] Loss: 0.17559  Avg Loss: 0.17559  Avg mIoU:  57.45  

*** Validation [@Epoch 58] Avg Loss: 0.25875  Avg mIoU:  55.90  ***

[Epoch: 59] [Batch: 0001/0580] Loss: 0.13469  Avg Loss: 0.13469  Avg mIoU:  54.95  
[Epoch: 59] [Batch: 0051/0580] Loss: 0.13325  Avg Loss: 0.18449  Avg mIoU:  67.01  
[Epoch: 59] [Batch: 0101/0580] Loss: 0.17534  Avg Loss: 0.18278  Avg mIoU:  67.78  
[Epoch: 59] [Batch: 0151/0580] Loss: 0.19170  Avg Loss: 0.18511  Avg mIoU:  67.60  
[Epoch: 59] [Batch: 0201/0580] Loss: 0.25140  Avg Loss: 0.18574  Avg mIoU:  67.70  
[Epoch: 59] [Batch: 0251/0580] Loss: 0.10341  Avg Loss: 0.18480  Avg mIoU:  68.11  
[Epoch: 59] [Batch: 0301/0580] Loss: 0.11558  Avg Loss: 0.18557  Avg mIoU:  68.10  
[Epoch: 59] [Batch: 0351/0580] Loss: 0.15880  Avg Loss: 0.18497  Avg mIoU:  68.39  
[Epoch: 59] [Batch: 0401/0580] Loss: 0.17973  Avg Loss: 0.18496  Avg mIoU:  68.48  
[Epoch: 59] [Batch: 0451/0580] Loss: 0.20761  Avg Loss: 0.18571  Avg mIoU:  68.57  
[Epoch: 59] [Batch: 0501/0580] Loss: 0.22006  Avg Loss: 0.18542  Avg mIoU:  68.62  
[Epoch: 59] [Batch: 0551/0580] Loss: 0.25001  Avg Loss: 0.18547  Avg mIoU:  68.78  

*** Training [@Epoch 59] Avg Loss: 0.18572  Avg mIoU:  68.90  ***

[Epoch: 59] [Batch: 0001/0050] Loss: 0.16161  Avg Loss: 0.16161  Avg mIoU:  65.48  

*** Validation [@Epoch 59] Avg Loss: 0.25302  Avg mIoU:  60.01  ***

[Epoch: 60] [Batch: 0001/0580] Loss: 0.15381  Avg Loss: 0.15381  Avg mIoU:  50.13  
[Epoch: 60] [Batch: 0051/0580] Loss: 0.14116  Avg Loss: 0.17836  Avg mIoU:  70.09  
[Epoch: 60] [Batch: 0101/0580] Loss: 0.16350  Avg Loss: 0.17956  Avg mIoU:  70.10  
[Epoch: 60] [Batch: 0151/0580] Loss: 0.21124  Avg Loss: 0.18250  Avg mIoU:  69.55  
[Epoch: 60] [Batch: 0201/0580] Loss: 0.18402  Avg Loss: 0.18356  Avg mIoU:  69.44  
[Epoch: 60] [Batch: 0251/0580] Loss: 0.15743  Avg Loss: 0.18321  Avg mIoU:  69.29  
[Epoch: 60] [Batch: 0301/0580] Loss: 0.19533  Avg Loss: 0.18378  Avg mIoU:  69.49  
[Epoch: 60] [Batch: 0351/0580] Loss: 0.16556  Avg Loss: 0.18351  Avg mIoU:  69.27  
[Epoch: 60] [Batch: 0401/0580] Loss: 0.16920  Avg Loss: 0.18197  Avg mIoU:  69.57  
[Epoch: 60] [Batch: 0451/0580] Loss: 0.13848  Avg Loss: 0.18170  Avg mIoU:  69.63  
[Epoch: 60] [Batch: 0501/0580] Loss: 0.15808  Avg Loss: 0.18352  Avg mIoU:  69.21  
[Epoch: 60] [Batch: 0551/0580] Loss: 0.20791  Avg Loss: 0.18398  Avg mIoU:  69.15  

*** Training [@Epoch 60] Avg Loss: 0.18400  Avg mIoU:  69.15  ***

[Epoch: 60] [Batch: 0001/0050] Loss: 0.16130  Avg Loss: 0.16130  Avg mIoU:  63.65  

*** Validation [@Epoch 60] Avg Loss: 0.25000  Avg mIoU:  59.88  ***

[Epoch: 61] [Batch: 0001/0580] Loss: 0.18250  Avg Loss: 0.18250  Avg mIoU:  40.54  
[Epoch: 61] [Batch: 0051/0580] Loss: 0.24826  Avg Loss: 0.18440  Avg mIoU:  69.52  
[Epoch: 61] [Batch: 0101/0580] Loss: 0.19961  Avg Loss: 0.18358  Avg mIoU:  68.88  
[Epoch: 61] [Batch: 0151/0580] Loss: 0.18304  Avg Loss: 0.18546  Avg mIoU:  68.76  
[Epoch: 61] [Batch: 0201/0580] Loss: 0.15486  Avg Loss: 0.18633  Avg mIoU:  68.85  
[Epoch: 61] [Batch: 0251/0580] Loss: 0.21697  Avg Loss: 0.18354  Avg mIoU:  69.43  
[Epoch: 61] [Batch: 0301/0580] Loss: 0.24235  Avg Loss: 0.18382  Avg mIoU:  69.39  
[Epoch: 61] [Batch: 0351/0580] Loss: 0.18772  Avg Loss: 0.18361  Avg mIoU:  69.29  
[Epoch: 61] [Batch: 0401/0580] Loss: 0.20438  Avg Loss: 0.18359  Avg mIoU:  69.14  
[Epoch: 61] [Batch: 0451/0580] Loss: 0.11945  Avg Loss: 0.18358  Avg mIoU:  69.04  
[Epoch: 61] [Batch: 0501/0580] Loss: 0.14783  Avg Loss: 0.18347  Avg mIoU:  69.02  
[Epoch: 61] [Batch: 0551/0580] Loss: 0.22393  Avg Loss: 0.18330  Avg mIoU:  69.10  

*** Training [@Epoch 61] Avg Loss: 0.18279  Avg mIoU:  69.15  ***

[Epoch: 61] [Batch: 0001/0050] Loss: 0.17210  Avg Loss: 0.17210  Avg mIoU:  63.21  

*** Validation [@Epoch 61] Avg Loss: 0.25147  Avg mIoU:  60.62  ***

[Epoch: 62] [Batch: 0001/0580] Loss: 0.22000  Avg Loss: 0.22000  Avg mIoU:  25.39  
[Epoch: 62] [Batch: 0051/0580] Loss: 0.23690  Avg Loss: 0.17883  Avg mIoU:  68.15  
[Epoch: 62] [Batch: 0101/0580] Loss: 0.13101  Avg Loss: 0.18191  Avg mIoU:  68.63  
[Epoch: 62] [Batch: 0151/0580] Loss: 0.18976  Avg Loss: 0.18034  Avg mIoU:  68.98  
[Epoch: 62] [Batch: 0201/0580] Loss: 0.20544  Avg Loss: 0.18160  Avg mIoU:  69.45  
[Epoch: 62] [Batch: 0251/0580] Loss: 0.20547  Avg Loss: 0.18114  Avg mIoU:  69.80  
[Epoch: 62] [Batch: 0301/0580] Loss: 0.12931  Avg Loss: 0.18085  Avg mIoU:  70.00  
[Epoch: 62] [Batch: 0351/0580] Loss: 0.20782  Avg Loss: 0.18235  Avg mIoU:  70.06  
[Epoch: 62] [Batch: 0401/0580] Loss: 0.14754  Avg Loss: 0.18229  Avg mIoU:  69.77  
[Epoch: 62] [Batch: 0451/0580] Loss: 0.20063  Avg Loss: 0.18236  Avg mIoU:  69.83  
[Epoch: 62] [Batch: 0501/0580] Loss: 0.20409  Avg Loss: 0.18206  Avg mIoU:  69.89  
[Epoch: 62] [Batch: 0551/0580] Loss: 0.12248  Avg Loss: 0.18265  Avg mIoU:  69.69  

*** Training [@Epoch 62] Avg Loss: 0.18306  Avg mIoU:  69.62  ***

[Epoch: 62] [Batch: 0001/0050] Loss: 0.17721  Avg Loss: 0.17721  Avg mIoU:  56.03  

*** Validation [@Epoch 62] Avg Loss: 0.24334  Avg mIoU:  54.99  ***

[Epoch: 63] [Batch: 0001/0580] Loss: 0.17686  Avg Loss: 0.17686  Avg mIoU:  53.91  
[Epoch: 63] [Batch: 0051/0580] Loss: 0.26245  Avg Loss: 0.18311  Avg mIoU:  69.75  
[Epoch: 63] [Batch: 0101/0580] Loss: 0.21625  Avg Loss: 0.18343  Avg mIoU:  69.64  
[Epoch: 63] [Batch: 0151/0580] Loss: 0.19159  Avg Loss: 0.18435  Avg mIoU:  68.98  
[Epoch: 63] [Batch: 0201/0580] Loss: 0.19650  Avg Loss: 0.18507  Avg mIoU:  69.11  
[Epoch: 63] [Batch: 0251/0580] Loss: 0.23348  Avg Loss: 0.18500  Avg mIoU:  69.35  
[Epoch: 63] [Batch: 0301/0580] Loss: 0.16473  Avg Loss: 0.18487  Avg mIoU:  69.46  
[Epoch: 63] [Batch: 0351/0580] Loss: 0.14960  Avg Loss: 0.18447  Avg mIoU:  69.46  
[Epoch: 63] [Batch: 0401/0580] Loss: 0.20886  Avg Loss: 0.18535  Avg mIoU:  69.29  
[Epoch: 63] [Batch: 0451/0580] Loss: 0.20169  Avg Loss: 0.18483  Avg mIoU:  69.33  
[Epoch: 63] [Batch: 0501/0580] Loss: 0.21628  Avg Loss: 0.18367  Avg mIoU:  69.55  
[Epoch: 63] [Batch: 0551/0580] Loss: 0.16284  Avg Loss: 0.18335  Avg mIoU:  69.56  

*** Training [@Epoch 63] Avg Loss: 0.18367  Avg mIoU:  69.48  ***

[Epoch: 63] [Batch: 0001/0050] Loss: 0.17389  Avg Loss: 0.17389  Avg mIoU:  62.10  

*** Validation [@Epoch 63] Avg Loss: 0.25217  Avg mIoU:  59.80  ***

[Epoch: 64] [Batch: 0001/0580] Loss: 0.21649  Avg Loss: 0.21649  Avg mIoU:  41.54  
[Epoch: 64] [Batch: 0051/0580] Loss: 0.22912  Avg Loss: 0.19323  Avg mIoU:  66.46  
[Epoch: 64] [Batch: 0101/0580] Loss: 0.17857  Avg Loss: 0.18768  Avg mIoU:  67.40  
[Epoch: 64] [Batch: 0151/0580] Loss: 0.15126  Avg Loss: 0.18576  Avg mIoU:  68.52  
[Epoch: 64] [Batch: 0201/0580] Loss: 0.17323  Avg Loss: 0.18295  Avg mIoU:  69.05  
[Epoch: 64] [Batch: 0251/0580] Loss: 0.25414  Avg Loss: 0.18405  Avg mIoU:  68.71  
[Epoch: 64] [Batch: 0301/0580] Loss: 0.26017  Avg Loss: 0.18355  Avg mIoU:  68.85  
[Epoch: 64] [Batch: 0351/0580] Loss: 0.17236  Avg Loss: 0.18202  Avg mIoU:  69.30  
[Epoch: 64] [Batch: 0401/0580] Loss: 0.16895  Avg Loss: 0.18127  Avg mIoU:  69.36  
[Epoch: 64] [Batch: 0451/0580] Loss: 0.21851  Avg Loss: 0.18280  Avg mIoU:  69.15  
[Epoch: 64] [Batch: 0501/0580] Loss: 0.17049  Avg Loss: 0.18279  Avg mIoU:  69.27  
[Epoch: 64] [Batch: 0551/0580] Loss: 0.15578  Avg Loss: 0.18316  Avg mIoU:  69.21  

*** Training [@Epoch 64] Avg Loss: 0.18300  Avg mIoU:  69.35  ***

[Epoch: 64] [Batch: 0001/0050] Loss: 0.17583  Avg Loss: 0.17583  Avg mIoU:  60.16  

*** Validation [@Epoch 64] Avg Loss: 0.24340  Avg mIoU:  58.90  ***

[Epoch: 65] [Batch: 0001/0580] Loss: 0.11738  Avg Loss: 0.11738  Avg mIoU:  51.00  
[Epoch: 65] [Batch: 0051/0580] Loss: 0.19030  Avg Loss: 0.17761  Avg mIoU:  69.81  
[Epoch: 65] [Batch: 0101/0580] Loss: 0.17366  Avg Loss: 0.17621  Avg mIoU:  70.43  
[Epoch: 65] [Batch: 0151/0580] Loss: 0.20176  Avg Loss: 0.17846  Avg mIoU:  70.23  
[Epoch: 65] [Batch: 0201/0580] Loss: 0.26645  Avg Loss: 0.17925  Avg mIoU:  70.18  
[Epoch: 65] [Batch: 0251/0580] Loss: 0.17278  Avg Loss: 0.17973  Avg mIoU:  70.58  
[Epoch: 65] [Batch: 0301/0580] Loss: 0.15965  Avg Loss: 0.17989  Avg mIoU:  70.54  
[Epoch: 65] [Batch: 0351/0580] Loss: 0.25899  Avg Loss: 0.18252  Avg mIoU:  69.99  
[Epoch: 65] [Batch: 0401/0580] Loss: 0.19861  Avg Loss: 0.18188  Avg mIoU:  70.02  
[Epoch: 65] [Batch: 0451/0580] Loss: 0.22007  Avg Loss: 0.18150  Avg mIoU:  69.83  
[Epoch: 65] [Batch: 0501/0580] Loss: 0.15596  Avg Loss: 0.18235  Avg mIoU:  69.54  
[Epoch: 65] [Batch: 0551/0580] Loss: 0.25738  Avg Loss: 0.18275  Avg mIoU:  69.43  

*** Training [@Epoch 65] Avg Loss: 0.18236  Avg mIoU:  69.46  ***

[Epoch: 65] [Batch: 0001/0050] Loss: 0.16878  Avg Loss: 0.16878  Avg mIoU:  62.53  

*** Validation [@Epoch 65] Avg Loss: 0.25683  Avg mIoU:  60.71  ***

[Epoch: 66] [Batch: 0001/0580] Loss: 0.17480  Avg Loss: 0.17480  Avg mIoU:  43.54  
[Epoch: 66] [Batch: 0051/0580] Loss: 0.22725  Avg Loss: 0.16771  Avg mIoU:  70.57  
[Epoch: 66] [Batch: 0101/0580] Loss: 0.10222  Avg Loss: 0.17350  Avg mIoU:  70.18  
[Epoch: 66] [Batch: 0151/0580] Loss: 0.17457  Avg Loss: 0.17628  Avg mIoU:  69.67  
[Epoch: 66] [Batch: 0201/0580] Loss: 0.13552  Avg Loss: 0.17814  Avg mIoU:  69.62  
[Epoch: 66] [Batch: 0251/0580] Loss: 0.25184  Avg Loss: 0.17968  Avg mIoU:  68.95  
[Epoch: 66] [Batch: 0301/0580] Loss: 0.13844  Avg Loss: 0.18006  Avg mIoU:  68.75  
[Epoch: 66] [Batch: 0351/0580] Loss: 0.21162  Avg Loss: 0.18110  Avg mIoU:  69.14  
[Epoch: 66] [Batch: 0401/0580] Loss: 0.17481  Avg Loss: 0.17969  Avg mIoU:  69.28  
[Epoch: 66] [Batch: 0451/0580] Loss: 0.14616  Avg Loss: 0.17998  Avg mIoU:  69.43  
[Epoch: 66] [Batch: 0501/0580] Loss: 0.17594  Avg Loss: 0.18042  Avg mIoU:  69.35  
[Epoch: 66] [Batch: 0551/0580] Loss: 0.18888  Avg Loss: 0.18081  Avg mIoU:  69.48  

*** Training [@Epoch 66] Avg Loss: 0.18101  Avg mIoU:  69.51  ***

[Epoch: 66] [Batch: 0001/0050] Loss: 0.14877  Avg Loss: 0.14877  Avg mIoU:  65.75  

*** Validation [@Epoch 66] Avg Loss: 0.25405  Avg mIoU:  58.79  ***

[Epoch: 67] [Batch: 0001/0580] Loss: 0.32931  Avg Loss: 0.32931  Avg mIoU:  45.19  
[Epoch: 67] [Batch: 0051/0580] Loss: 0.18924  Avg Loss: 0.19003  Avg mIoU:  65.79  
[Epoch: 67] [Batch: 0101/0580] Loss: 0.33897  Avg Loss: 0.18868  Avg mIoU:  67.40  
[Epoch: 67] [Batch: 0151/0580] Loss: 0.14548  Avg Loss: 0.18919  Avg mIoU:  67.81  
[Epoch: 67] [Batch: 0201/0580] Loss: 0.14755  Avg Loss: 0.18877  Avg mIoU:  67.60  
[Epoch: 67] [Batch: 0251/0580] Loss: 0.20476  Avg Loss: 0.18737  Avg mIoU:  68.33  
[Epoch: 67] [Batch: 0301/0580] Loss: 0.15744  Avg Loss: 0.18649  Avg mIoU:  68.42  
[Epoch: 67] [Batch: 0351/0580] Loss: 0.17380  Avg Loss: 0.18590  Avg mIoU:  68.97  
[Epoch: 67] [Batch: 0401/0580] Loss: 0.18786  Avg Loss: 0.18493  Avg mIoU:  69.13  
[Epoch: 67] [Batch: 0451/0580] Loss: 0.13047  Avg Loss: 0.18379  Avg mIoU:  69.17  
[Epoch: 67] [Batch: 0501/0580] Loss: 0.15906  Avg Loss: 0.18292  Avg mIoU:  69.39  
[Epoch: 67] [Batch: 0551/0580] Loss: 0.18682  Avg Loss: 0.18156  Avg mIoU:  69.63  

*** Training [@Epoch 67] Avg Loss: 0.18133  Avg mIoU:  69.74  ***

[Epoch: 67] [Batch: 0001/0050] Loss: 0.18537  Avg Loss: 0.18537  Avg mIoU:  61.34  

*** Validation [@Epoch 67] Avg Loss: 0.26793  Avg mIoU:  61.17  ***

Model saved @67 w/ val. mIoU: 61.17.

[Epoch: 68] [Batch: 0001/0580] Loss: 0.16884  Avg Loss: 0.16884  Avg mIoU:  52.22  
[Epoch: 68] [Batch: 0051/0580] Loss: 0.20115  Avg Loss: 0.17716  Avg mIoU:  69.36  
[Epoch: 68] [Batch: 0101/0580] Loss: 0.18256  Avg Loss: 0.17744  Avg mIoU:  70.54  
[Epoch: 68] [Batch: 0151/0580] Loss: 0.18703  Avg Loss: 0.17800  Avg mIoU:  70.12  
[Epoch: 68] [Batch: 0201/0580] Loss: 0.23121  Avg Loss: 0.17802  Avg mIoU:  70.28  
[Epoch: 68] [Batch: 0251/0580] Loss: 0.19980  Avg Loss: 0.17985  Avg mIoU:  69.75  
[Epoch: 68] [Batch: 0301/0580] Loss: 0.21210  Avg Loss: 0.18075  Avg mIoU:  69.58  
[Epoch: 68] [Batch: 0351/0580] Loss: 0.17157  Avg Loss: 0.18161  Avg mIoU:  69.41  
[Epoch: 68] [Batch: 0401/0580] Loss: 0.17496  Avg Loss: 0.18218  Avg mIoU:  69.46  
[Epoch: 68] [Batch: 0451/0580] Loss: 0.18173  Avg Loss: 0.18227  Avg mIoU:  69.57  
[Epoch: 68] [Batch: 0501/0580] Loss: 0.19956  Avg Loss: 0.18143  Avg mIoU:  69.64  
[Epoch: 68] [Batch: 0551/0580] Loss: 0.14518  Avg Loss: 0.18075  Avg mIoU:  69.61  

*** Training [@Epoch 68] Avg Loss: 0.18085  Avg mIoU:  69.59  ***

[Epoch: 68] [Batch: 0001/0050] Loss: 0.16737  Avg Loss: 0.16737  Avg mIoU:  59.16  

*** Validation [@Epoch 68] Avg Loss: 0.24576  Avg mIoU:  59.77  ***

[Epoch: 69] [Batch: 0001/0580] Loss: 0.19715  Avg Loss: 0.19715  Avg mIoU:  43.35  
[Epoch: 69] [Batch: 0051/0580] Loss: 0.22119  Avg Loss: 0.17669  Avg mIoU:  71.85  
[Epoch: 69] [Batch: 0101/0580] Loss: 0.15699  Avg Loss: 0.18345  Avg mIoU:  70.07  
[Epoch: 69] [Batch: 0151/0580] Loss: 0.12995  Avg Loss: 0.18213  Avg mIoU:  69.73  
[Epoch: 69] [Batch: 0201/0580] Loss: 0.14156  Avg Loss: 0.18317  Avg mIoU:  69.23  
[Epoch: 69] [Batch: 0251/0580] Loss: 0.13275  Avg Loss: 0.18254  Avg mIoU:  68.81  
[Epoch: 69] [Batch: 0301/0580] Loss: 0.17854  Avg Loss: 0.18155  Avg mIoU:  68.91  
[Epoch: 69] [Batch: 0351/0580] Loss: 0.16807  Avg Loss: 0.18160  Avg mIoU:  68.99  
[Epoch: 69] [Batch: 0401/0580] Loss: 0.19710  Avg Loss: 0.18019  Avg mIoU:  69.06  
[Epoch: 69] [Batch: 0451/0580] Loss: 0.12326  Avg Loss: 0.17953  Avg mIoU:  69.48  
[Epoch: 69] [Batch: 0501/0580] Loss: 0.20495  Avg Loss: 0.17991  Avg mIoU:  69.58  
[Epoch: 69] [Batch: 0551/0580] Loss: 0.16721  Avg Loss: 0.18059  Avg mIoU:  69.52  

*** Training [@Epoch 69] Avg Loss: 0.18052  Avg mIoU:  69.56  ***

[Epoch: 69] [Batch: 0001/0050] Loss: 0.17655  Avg Loss: 0.17655  Avg mIoU:  62.27  

*** Validation [@Epoch 69] Avg Loss: 0.24312  Avg mIoU:  58.93  ***

[Epoch: 70] [Batch: 0001/0580] Loss: 0.14834  Avg Loss: 0.14834  Avg mIoU:  54.43  
[Epoch: 70] [Batch: 0051/0580] Loss: 0.20775  Avg Loss: 0.17761  Avg mIoU:  70.25  
[Epoch: 70] [Batch: 0101/0580] Loss: 0.18428  Avg Loss: 0.17913  Avg mIoU:  69.93  
[Epoch: 70] [Batch: 0151/0580] Loss: 0.19185  Avg Loss: 0.17761  Avg mIoU:  69.68  
[Epoch: 70] [Batch: 0201/0580] Loss: 0.16051  Avg Loss: 0.17940  Avg mIoU:  70.29  
[Epoch: 70] [Batch: 0251/0580] Loss: 0.17150  Avg Loss: 0.17938  Avg mIoU:  70.30  
[Epoch: 70] [Batch: 0301/0580] Loss: 0.17893  Avg Loss: 0.17922  Avg mIoU:  70.23  
[Epoch: 70] [Batch: 0351/0580] Loss: 0.15428  Avg Loss: 0.17944  Avg mIoU:  70.10  
[Epoch: 70] [Batch: 0401/0580] Loss: 0.17683  Avg Loss: 0.18015  Avg mIoU:  69.99  
[Epoch: 70] [Batch: 0451/0580] Loss: 0.12069  Avg Loss: 0.18097  Avg mIoU:  69.89  
[Epoch: 70] [Batch: 0501/0580] Loss: 0.20355  Avg Loss: 0.18139  Avg mIoU:  69.80  
[Epoch: 70] [Batch: 0551/0580] Loss: 0.19667  Avg Loss: 0.18073  Avg mIoU:  69.91  

*** Training [@Epoch 70] Avg Loss: 0.18003  Avg mIoU:  69.96  ***

[Epoch: 70] [Batch: 0001/0050] Loss: 0.17208  Avg Loss: 0.17208  Avg mIoU:  62.00  

*** Validation [@Epoch 70] Avg Loss: 0.23771  Avg mIoU:  59.30  ***

[Epoch: 71] [Batch: 0001/0580] Loss: 0.20185  Avg Loss: 0.20185  Avg mIoU:  39.76  
[Epoch: 71] [Batch: 0051/0580] Loss: 0.20257  Avg Loss: 0.18497  Avg mIoU:  70.38  
[Epoch: 71] [Batch: 0101/0580] Loss: 0.20038  Avg Loss: 0.18267  Avg mIoU:  69.38  
[Epoch: 71] [Batch: 0151/0580] Loss: 0.18440  Avg Loss: 0.18187  Avg mIoU:  70.19  
[Epoch: 71] [Batch: 0201/0580] Loss: 0.22396  Avg Loss: 0.18172  Avg mIoU:  70.35  
[Epoch: 71] [Batch: 0251/0580] Loss: 0.14630  Avg Loss: 0.18101  Avg mIoU:  70.14  
[Epoch: 71] [Batch: 0301/0580] Loss: 0.17665  Avg Loss: 0.18026  Avg mIoU:  70.07  
[Epoch: 71] [Batch: 0351/0580] Loss: 0.14814  Avg Loss: 0.17971  Avg mIoU:  70.03  
[Epoch: 71] [Batch: 0401/0580] Loss: 0.12629  Avg Loss: 0.17888  Avg mIoU:  69.80  
[Epoch: 71] [Batch: 0451/0580] Loss: 0.18973  Avg Loss: 0.17966  Avg mIoU:  69.83  
[Epoch: 71] [Batch: 0501/0580] Loss: 0.21899  Avg Loss: 0.17989  Avg mIoU:  69.82  
[Epoch: 71] [Batch: 0551/0580] Loss: 0.18481  Avg Loss: 0.17913  Avg mIoU:  69.87  

*** Training [@Epoch 71] Avg Loss: 0.17949  Avg mIoU:  69.93  ***

[Epoch: 71] [Batch: 0001/0050] Loss: 0.17018  Avg Loss: 0.17018  Avg mIoU:  59.06  

*** Validation [@Epoch 71] Avg Loss: 0.25038  Avg mIoU:  57.19  ***

[Epoch: 72] [Batch: 0001/0580] Loss: 0.22471  Avg Loss: 0.22471  Avg mIoU:  36.90  
[Epoch: 72] [Batch: 0051/0580] Loss: 0.17666  Avg Loss: 0.18896  Avg mIoU:  68.72  
[Epoch: 72] [Batch: 0101/0580] Loss: 0.19339  Avg Loss: 0.18331  Avg mIoU:  68.70  
[Epoch: 72] [Batch: 0151/0580] Loss: 0.26041  Avg Loss: 0.18426  Avg mIoU:  68.70  
[Epoch: 72] [Batch: 0201/0580] Loss: 0.13860  Avg Loss: 0.18424  Avg mIoU:  69.18  
[Epoch: 72] [Batch: 0251/0580] Loss: 0.17708  Avg Loss: 0.18460  Avg mIoU:  69.29  
[Epoch: 72] [Batch: 0301/0580] Loss: 0.15622  Avg Loss: 0.18216  Avg mIoU:  69.40  
[Epoch: 72] [Batch: 0351/0580] Loss: 0.11650  Avg Loss: 0.18203  Avg mIoU:  69.37  
[Epoch: 72] [Batch: 0401/0580] Loss: 0.13981  Avg Loss: 0.18053  Avg mIoU:  69.67  
[Epoch: 72] [Batch: 0451/0580] Loss: 0.21474  Avg Loss: 0.18138  Avg mIoU:  69.49  
[Epoch: 72] [Batch: 0501/0580] Loss: 0.12897  Avg Loss: 0.17973  Avg mIoU:  69.53  
[Epoch: 72] [Batch: 0551/0580] Loss: 0.22376  Avg Loss: 0.17893  Avg mIoU:  69.83  

*** Training [@Epoch 72] Avg Loss: 0.17913  Avg mIoU:  69.86  ***

[Epoch: 72] [Batch: 0001/0050] Loss: 0.16390  Avg Loss: 0.16390  Avg mIoU:  61.69  

*** Validation [@Epoch 72] Avg Loss: 0.24509  Avg mIoU:  56.72  ***

[Epoch: 73] [Batch: 0001/0580] Loss: 0.14084  Avg Loss: 0.14084  Avg mIoU:  39.58  
[Epoch: 73] [Batch: 0051/0580] Loss: 0.10994  Avg Loss: 0.17873  Avg mIoU:  70.08  
[Epoch: 73] [Batch: 0101/0580] Loss: 0.19788  Avg Loss: 0.17528  Avg mIoU:  70.76  
[Epoch: 73] [Batch: 0151/0580] Loss: 0.16748  Avg Loss: 0.17628  Avg mIoU:  70.75  
[Epoch: 73] [Batch: 0201/0580] Loss: 0.17463  Avg Loss: 0.17906  Avg mIoU:  70.48  
[Epoch: 73] [Batch: 0251/0580] Loss: 0.29277  Avg Loss: 0.17821  Avg mIoU:  70.69  
[Epoch: 73] [Batch: 0301/0580] Loss: 0.12491  Avg Loss: 0.17802  Avg mIoU:  70.76  
[Epoch: 73] [Batch: 0351/0580] Loss: 0.25643  Avg Loss: 0.17886  Avg mIoU:  70.42  
[Epoch: 73] [Batch: 0401/0580] Loss: 0.18700  Avg Loss: 0.17948  Avg mIoU:  70.37  
[Epoch: 73] [Batch: 0451/0580] Loss: 0.17191  Avg Loss: 0.18011  Avg mIoU:  70.33  
[Epoch: 73] [Batch: 0501/0580] Loss: 0.16956  Avg Loss: 0.17938  Avg mIoU:  70.31  
[Epoch: 73] [Batch: 0551/0580] Loss: 0.13015  Avg Loss: 0.17904  Avg mIoU:  70.17  

*** Training [@Epoch 73] Avg Loss: 0.17922  Avg mIoU:  70.12  ***

[Epoch: 73] [Batch: 0001/0050] Loss: 0.16542  Avg Loss: 0.16542  Avg mIoU:  62.53  

*** Validation [@Epoch 73] Avg Loss: 0.24220  Avg mIoU:  60.23  ***

[Epoch: 74] [Batch: 0001/0580] Loss: 0.12761  Avg Loss: 0.12761  Avg mIoU:  38.25  
[Epoch: 74] [Batch: 0051/0580] Loss: 0.19432  Avg Loss: 0.18172  Avg mIoU:  68.39  
[Epoch: 74] [Batch: 0101/0580] Loss: 0.17759  Avg Loss: 0.17834  Avg mIoU:  69.55  
[Epoch: 74] [Batch: 0151/0580] Loss: 0.16439  Avg Loss: 0.17914  Avg mIoU:  70.36  
[Epoch: 74] [Batch: 0201/0580] Loss: 0.15637  Avg Loss: 0.17887  Avg mIoU:  70.37  
[Epoch: 74] [Batch: 0251/0580] Loss: 0.26529  Avg Loss: 0.17677  Avg mIoU:  70.68  
[Epoch: 74] [Batch: 0301/0580] Loss: 0.20069  Avg Loss: 0.17751  Avg mIoU:  70.50  
[Epoch: 74] [Batch: 0351/0580] Loss: 0.08249  Avg Loss: 0.17744  Avg mIoU:  70.61  
[Epoch: 74] [Batch: 0401/0580] Loss: 0.22204  Avg Loss: 0.17638  Avg mIoU:  70.84  
[Epoch: 74] [Batch: 0451/0580] Loss: 0.17778  Avg Loss: 0.17776  Avg mIoU:  70.70  
[Epoch: 74] [Batch: 0501/0580] Loss: 0.19818  Avg Loss: 0.17831  Avg mIoU:  70.66  
[Epoch: 74] [Batch: 0551/0580] Loss: 0.19361  Avg Loss: 0.17790  Avg mIoU:  70.60  

*** Training [@Epoch 74] Avg Loss: 0.17790  Avg mIoU:  70.46  ***

[Epoch: 74] [Batch: 0001/0050] Loss: 0.18040  Avg Loss: 0.18040  Avg mIoU:  57.61  

*** Validation [@Epoch 74] Avg Loss: 0.24304  Avg mIoU:  57.45  ***

[Epoch: 75] [Batch: 0001/0580] Loss: 0.19028  Avg Loss: 0.19028  Avg mIoU:  44.83  
[Epoch: 75] [Batch: 0051/0580] Loss: 0.19063  Avg Loss: 0.17874  Avg mIoU:  69.11  
[Epoch: 75] [Batch: 0101/0580] Loss: 0.27759  Avg Loss: 0.17674  Avg mIoU:  69.52  
[Epoch: 75] [Batch: 0151/0580] Loss: 0.16639  Avg Loss: 0.17547  Avg mIoU:  69.81  
[Epoch: 75] [Batch: 0201/0580] Loss: 0.29789  Avg Loss: 0.17600  Avg mIoU:  70.12  
[Epoch: 75] [Batch: 0251/0580] Loss: 0.16509  Avg Loss: 0.17704  Avg mIoU:  69.99  
[Epoch: 75] [Batch: 0301/0580] Loss: 0.23366  Avg Loss: 0.17792  Avg mIoU:  69.87  
[Epoch: 75] [Batch: 0351/0580] Loss: 0.22093  Avg Loss: 0.17812  Avg mIoU:  69.87  
[Epoch: 75] [Batch: 0401/0580] Loss: 0.17892  Avg Loss: 0.17782  Avg mIoU:  70.12  
[Epoch: 75] [Batch: 0451/0580] Loss: 0.22079  Avg Loss: 0.17750  Avg mIoU:  70.31  
[Epoch: 75] [Batch: 0501/0580] Loss: 0.09227  Avg Loss: 0.17695  Avg mIoU:  70.59  
[Epoch: 75] [Batch: 0551/0580] Loss: 0.15072  Avg Loss: 0.17715  Avg mIoU:  70.47  

*** Training [@Epoch 75] Avg Loss: 0.17690  Avg mIoU:  70.47  ***

[Epoch: 75] [Batch: 0001/0050] Loss: 0.16352  Avg Loss: 0.16352  Avg mIoU:  60.57  

*** Validation [@Epoch 75] Avg Loss: 0.24078  Avg mIoU:  58.76  ***

[Epoch: 76] [Batch: 0001/0580] Loss: 0.14079  Avg Loss: 0.14079  Avg mIoU:  32.53  
[Epoch: 76] [Batch: 0051/0580] Loss: 0.14481  Avg Loss: 0.17138  Avg mIoU:  70.33  
[Epoch: 76] [Batch: 0101/0580] Loss: 0.23949  Avg Loss: 0.17571  Avg mIoU:  70.39  
[Epoch: 76] [Batch: 0151/0580] Loss: 0.16292  Avg Loss: 0.17549  Avg mIoU:  70.95  
[Epoch: 76] [Batch: 0201/0580] Loss: 0.10408  Avg Loss: 0.17539  Avg mIoU:  70.86  
[Epoch: 76] [Batch: 0251/0580] Loss: 0.26581  Avg Loss: 0.17695  Avg mIoU:  70.20  
[Epoch: 76] [Batch: 0301/0580] Loss: 0.21231  Avg Loss: 0.17592  Avg mIoU:  70.15  
[Epoch: 76] [Batch: 0351/0580] Loss: 0.20474  Avg Loss: 0.17588  Avg mIoU:  70.29  
[Epoch: 76] [Batch: 0401/0580] Loss: 0.25145  Avg Loss: 0.17600  Avg mIoU:  70.37  
[Epoch: 76] [Batch: 0451/0580] Loss: 0.17177  Avg Loss: 0.17572  Avg mIoU:  70.25  
[Epoch: 76] [Batch: 0501/0580] Loss: 0.24673  Avg Loss: 0.17593  Avg mIoU:  70.23  
[Epoch: 76] [Batch: 0551/0580] Loss: 0.17183  Avg Loss: 0.17641  Avg mIoU:  70.24  

*** Training [@Epoch 76] Avg Loss: 0.17612  Avg mIoU:  70.19  ***

[Epoch: 76] [Batch: 0001/0050] Loss: 0.17635  Avg Loss: 0.17635  Avg mIoU:  63.17  

*** Validation [@Epoch 76] Avg Loss: 0.25370  Avg mIoU:  58.74  ***

[Epoch: 77] [Batch: 0001/0580] Loss: 0.20817  Avg Loss: 0.20817  Avg mIoU:  33.72  
[Epoch: 77] [Batch: 0051/0580] Loss: 0.14144  Avg Loss: 0.17810  Avg mIoU:  69.13  
[Epoch: 77] [Batch: 0101/0580] Loss: 0.15748  Avg Loss: 0.17210  Avg mIoU:  69.35  
[Epoch: 77] [Batch: 0151/0580] Loss: 0.25980  Avg Loss: 0.17558  Avg mIoU:  70.18  
[Epoch: 77] [Batch: 0201/0580] Loss: 0.18358  Avg Loss: 0.17828  Avg mIoU:  70.79  
[Epoch: 77] [Batch: 0251/0580] Loss: 0.17200  Avg Loss: 0.17830  Avg mIoU:  70.59  
[Epoch: 77] [Batch: 0301/0580] Loss: 0.17904  Avg Loss: 0.17771  Avg mIoU:  70.70  
[Epoch: 77] [Batch: 0351/0580] Loss: 0.16391  Avg Loss: 0.17875  Avg mIoU:  70.67  
[Epoch: 77] [Batch: 0401/0580] Loss: 0.19308  Avg Loss: 0.17860  Avg mIoU:  70.67  
[Epoch: 77] [Batch: 0451/0580] Loss: 0.15923  Avg Loss: 0.17851  Avg mIoU:  70.47  
[Epoch: 77] [Batch: 0501/0580] Loss: 0.08885  Avg Loss: 0.17764  Avg mIoU:  70.62  
[Epoch: 77] [Batch: 0551/0580] Loss: 0.20104  Avg Loss: 0.17690  Avg mIoU:  70.70  

*** Training [@Epoch 77] Avg Loss: 0.17682  Avg mIoU:  70.77  ***

[Epoch: 77] [Batch: 0001/0050] Loss: 0.15639  Avg Loss: 0.15639  Avg mIoU:  60.13  

*** Validation [@Epoch 77] Avg Loss: 0.25606  Avg mIoU:  55.71  ***

[Epoch: 78] [Batch: 0001/0580] Loss: 0.14010  Avg Loss: 0.14010  Avg mIoU:  51.15  
[Epoch: 78] [Batch: 0051/0580] Loss: 0.13552  Avg Loss: 0.17042  Avg mIoU:  71.53  
[Epoch: 78] [Batch: 0101/0580] Loss: 0.12111  Avg Loss: 0.16749  Avg mIoU:  71.32  
[Epoch: 78] [Batch: 0151/0580] Loss: 0.15935  Avg Loss: 0.16849  Avg mIoU:  70.84  
[Epoch: 78] [Batch: 0201/0580] Loss: 0.12598  Avg Loss: 0.17076  Avg mIoU:  70.76  
[Epoch: 78] [Batch: 0251/0580] Loss: 0.18679  Avg Loss: 0.17483  Avg mIoU:  70.44  
[Epoch: 78] [Batch: 0301/0580] Loss: 0.15135  Avg Loss: 0.17695  Avg mIoU:  70.35  
[Epoch: 78] [Batch: 0351/0580] Loss: 0.20571  Avg Loss: 0.17506  Avg mIoU:  70.50  
[Epoch: 78] [Batch: 0401/0580] Loss: 0.18126  Avg Loss: 0.17513  Avg mIoU:  70.40  
[Epoch: 78] [Batch: 0451/0580] Loss: 0.22098  Avg Loss: 0.17600  Avg mIoU:  70.50  
[Epoch: 78] [Batch: 0501/0580] Loss: 0.23568  Avg Loss: 0.17677  Avg mIoU:  70.60  
[Epoch: 78] [Batch: 0551/0580] Loss: 0.19753  Avg Loss: 0.17712  Avg mIoU:  70.37  

*** Training [@Epoch 78] Avg Loss: 0.17679  Avg mIoU:  70.33  ***

[Epoch: 78] [Batch: 0001/0050] Loss: 0.17635  Avg Loss: 0.17635  Avg mIoU:  58.73  

*** Validation [@Epoch 78] Avg Loss: 0.25543  Avg mIoU:  55.22  ***

[Epoch: 79] [Batch: 0001/0580] Loss: 0.22089  Avg Loss: 0.22089  Avg mIoU:  35.23  
[Epoch: 79] [Batch: 0051/0580] Loss: 0.09731  Avg Loss: 0.17178  Avg mIoU:  70.48  
[Epoch: 79] [Batch: 0101/0580] Loss: 0.14103  Avg Loss: 0.17359  Avg mIoU:  70.59  
[Epoch: 79] [Batch: 0151/0580] Loss: 0.18793  Avg Loss: 0.17512  Avg mIoU:  70.31  
[Epoch: 79] [Batch: 0201/0580] Loss: 0.13481  Avg Loss: 0.17430  Avg mIoU:  69.86  
[Epoch: 79] [Batch: 0251/0580] Loss: 0.19426  Avg Loss: 0.17558  Avg mIoU:  69.91  
[Epoch: 79] [Batch: 0301/0580] Loss: 0.17013  Avg Loss: 0.17681  Avg mIoU:  69.86  
[Epoch: 79] [Batch: 0351/0580] Loss: 0.12562  Avg Loss: 0.17474  Avg mIoU:  70.05  
[Epoch: 79] [Batch: 0401/0580] Loss: 0.15312  Avg Loss: 0.17572  Avg mIoU:  70.21  
[Epoch: 79] [Batch: 0451/0580] Loss: 0.17610  Avg Loss: 0.17568  Avg mIoU:  70.12  
[Epoch: 79] [Batch: 0501/0580] Loss: 0.13604  Avg Loss: 0.17654  Avg mIoU:  70.16  
[Epoch: 79] [Batch: 0551/0580] Loss: 0.18270  Avg Loss: 0.17684  Avg mIoU:  70.33  

*** Training [@Epoch 79] Avg Loss: 0.17613  Avg mIoU:  70.34  ***

[Epoch: 79] [Batch: 0001/0050] Loss: 0.16490  Avg Loss: 0.16490  Avg mIoU:  64.57  

*** Validation [@Epoch 79] Avg Loss: 0.24361  Avg mIoU:  61.73  ***

Model saved @79 w/ val. mIoU: 61.73.

[Epoch: 80] [Batch: 0001/0580] Loss: 0.21629  Avg Loss: 0.21629  Avg mIoU:  39.23  
[Epoch: 80] [Batch: 0051/0580] Loss: 0.12149  Avg Loss: 0.16543  Avg mIoU:  70.61  
[Epoch: 80] [Batch: 0101/0580] Loss: 0.11897  Avg Loss: 0.16855  Avg mIoU:  70.74  
[Epoch: 80] [Batch: 0151/0580] Loss: 0.19963  Avg Loss: 0.17050  Avg mIoU:  71.62  
[Epoch: 80] [Batch: 0201/0580] Loss: 0.23668  Avg Loss: 0.17079  Avg mIoU:  71.65  
[Epoch: 80] [Batch: 0251/0580] Loss: 0.12678  Avg Loss: 0.17306  Avg mIoU:  71.23  
[Epoch: 80] [Batch: 0301/0580] Loss: 0.12386  Avg Loss: 0.17262  Avg mIoU:  71.00  
[Epoch: 80] [Batch: 0351/0580] Loss: 0.16313  Avg Loss: 0.17426  Avg mIoU:  70.97  
[Epoch: 80] [Batch: 0401/0580] Loss: 0.17104  Avg Loss: 0.17358  Avg mIoU:  70.97  
[Epoch: 80] [Batch: 0451/0580] Loss: 0.21615  Avg Loss: 0.17365  Avg mIoU:  70.85  
[Epoch: 80] [Batch: 0501/0580] Loss: 0.22848  Avg Loss: 0.17430  Avg mIoU:  70.78  
[Epoch: 80] [Batch: 0551/0580] Loss: 0.15984  Avg Loss: 0.17472  Avg mIoU:  70.63  

*** Training [@Epoch 80] Avg Loss: 0.17465  Avg mIoU:  70.56  ***

[Epoch: 80] [Batch: 0001/0050] Loss: 0.16317  Avg Loss: 0.16317  Avg mIoU:  63.09  

*** Validation [@Epoch 80] Avg Loss: 0.24645  Avg mIoU:  58.79  ***

[Epoch: 81] [Batch: 0001/0580] Loss: 0.15112  Avg Loss: 0.15112  Avg mIoU:  48.83  
[Epoch: 81] [Batch: 0051/0580] Loss: 0.18242  Avg Loss: 0.18537  Avg mIoU:  71.23  
[Epoch: 81] [Batch: 0101/0580] Loss: 0.18735  Avg Loss: 0.18016  Avg mIoU:  70.91  
[Epoch: 81] [Batch: 0151/0580] Loss: 0.14514  Avg Loss: 0.17548  Avg mIoU:  71.46  
[Epoch: 81] [Batch: 0201/0580] Loss: 0.22867  Avg Loss: 0.17450  Avg mIoU:  71.03  
[Epoch: 81] [Batch: 0251/0580] Loss: 0.12224  Avg Loss: 0.17333  Avg mIoU:  70.80  
[Epoch: 81] [Batch: 0301/0580] Loss: 0.19061  Avg Loss: 0.17373  Avg mIoU:  70.59  
[Epoch: 81] [Batch: 0351/0580] Loss: 0.12741  Avg Loss: 0.17388  Avg mIoU:  70.72  
[Epoch: 81] [Batch: 0401/0580] Loss: 0.12785  Avg Loss: 0.17368  Avg mIoU:  70.64  
[Epoch: 81] [Batch: 0451/0580] Loss: 0.27423  Avg Loss: 0.17491  Avg mIoU:  70.59  
[Epoch: 81] [Batch: 0501/0580] Loss: 0.18995  Avg Loss: 0.17536  Avg mIoU:  70.40  
[Epoch: 81] [Batch: 0551/0580] Loss: 0.16875  Avg Loss: 0.17601  Avg mIoU:  70.29  

*** Training [@Epoch 81] Avg Loss: 0.17621  Avg mIoU:  70.19  ***

[Epoch: 81] [Batch: 0001/0050] Loss: 0.16728  Avg Loss: 0.16728  Avg mIoU:  58.80  

*** Validation [@Epoch 81] Avg Loss: 0.24394  Avg mIoU:  54.09  ***

[Epoch: 82] [Batch: 0001/0580] Loss: 0.19648  Avg Loss: 0.19648  Avg mIoU:  45.22  
[Epoch: 82] [Batch: 0051/0580] Loss: 0.15342  Avg Loss: 0.17097  Avg mIoU:  71.50  
[Epoch: 82] [Batch: 0101/0580] Loss: 0.17431  Avg Loss: 0.17454  Avg mIoU:  71.36  
[Epoch: 82] [Batch: 0151/0580] Loss: 0.16050  Avg Loss: 0.17340  Avg mIoU:  71.10  
[Epoch: 82] [Batch: 0201/0580] Loss: 0.17128  Avg Loss: 0.17409  Avg mIoU:  70.44  
[Epoch: 82] [Batch: 0251/0580] Loss: 0.22138  Avg Loss: 0.17303  Avg mIoU:  70.83  
[Epoch: 82] [Batch: 0301/0580] Loss: 0.15324  Avg Loss: 0.17320  Avg mIoU:  70.65  
[Epoch: 82] [Batch: 0351/0580] Loss: 0.19486  Avg Loss: 0.17410  Avg mIoU:  70.53  
[Epoch: 82] [Batch: 0401/0580] Loss: 0.14926  Avg Loss: 0.17428  Avg mIoU:  70.51  
[Epoch: 82] [Batch: 0451/0580] Loss: 0.11641  Avg Loss: 0.17524  Avg mIoU:  70.44  
[Epoch: 82] [Batch: 0501/0580] Loss: 0.21564  Avg Loss: 0.17555  Avg mIoU:  70.46  
[Epoch: 82] [Batch: 0551/0580] Loss: 0.12046  Avg Loss: 0.17526  Avg mIoU:  70.40  

*** Training [@Epoch 82] Avg Loss: 0.17518  Avg mIoU:  70.43  ***

[Epoch: 82] [Batch: 0001/0050] Loss: 0.16923  Avg Loss: 0.16923  Avg mIoU:  62.41  

*** Validation [@Epoch 82] Avg Loss: 0.23956  Avg mIoU:  61.19  ***

[Epoch: 83] [Batch: 0001/0580] Loss: 0.14429  Avg Loss: 0.14429  Avg mIoU:  39.76  
[Epoch: 83] [Batch: 0051/0580] Loss: 0.15795  Avg Loss: 0.17116  Avg mIoU:  71.34  
[Epoch: 83] [Batch: 0101/0580] Loss: 0.14235  Avg Loss: 0.17399  Avg mIoU:  71.48  
[Epoch: 83] [Batch: 0151/0580] Loss: 0.17270  Avg Loss: 0.17378  Avg mIoU:  71.69  
[Epoch: 83] [Batch: 0201/0580] Loss: 0.14051  Avg Loss: 0.17336  Avg mIoU:  71.36  
[Epoch: 83] [Batch: 0251/0580] Loss: 0.15496  Avg Loss: 0.17110  Avg mIoU:  71.71  
[Epoch: 83] [Batch: 0301/0580] Loss: 0.21745  Avg Loss: 0.17109  Avg mIoU:  71.69  
[Epoch: 83] [Batch: 0351/0580] Loss: 0.16139  Avg Loss: 0.17221  Avg mIoU:  71.41  
[Epoch: 83] [Batch: 0401/0580] Loss: 0.21920  Avg Loss: 0.17265  Avg mIoU:  71.16  
[Epoch: 83] [Batch: 0451/0580] Loss: 0.13477  Avg Loss: 0.17189  Avg mIoU:  71.17  
[Epoch: 83] [Batch: 0501/0580] Loss: 0.19962  Avg Loss: 0.17249  Avg mIoU:  70.83  
[Epoch: 83] [Batch: 0551/0580] Loss: 0.16219  Avg Loss: 0.17407  Avg mIoU:  70.62  

*** Training [@Epoch 83] Avg Loss: 0.17409  Avg mIoU:  70.56  ***

[Epoch: 83] [Batch: 0001/0050] Loss: 0.17390  Avg Loss: 0.17390  Avg mIoU:  62.23  

*** Validation [@Epoch 83] Avg Loss: 0.25207  Avg mIoU:  61.60  ***

[Epoch: 84] [Batch: 0001/0580] Loss: 0.14224  Avg Loss: 0.14224  Avg mIoU:  48.75  
[Epoch: 84] [Batch: 0051/0580] Loss: 0.20404  Avg Loss: 0.17058  Avg mIoU:  71.34  
[Epoch: 84] [Batch: 0101/0580] Loss: 0.22753  Avg Loss: 0.17383  Avg mIoU:  70.78  
[Epoch: 84] [Batch: 0151/0580] Loss: 0.17102  Avg Loss: 0.17619  Avg mIoU:  70.42  
[Epoch: 84] [Batch: 0201/0580] Loss: 0.15408  Avg Loss: 0.17506  Avg mIoU:  70.38  
[Epoch: 84] [Batch: 0251/0580] Loss: 0.12066  Avg Loss: 0.17523  Avg mIoU:  70.06  
[Epoch: 84] [Batch: 0301/0580] Loss: 0.15628  Avg Loss: 0.17518  Avg mIoU:  70.08  
[Epoch: 84] [Batch: 0351/0580] Loss: 0.15012  Avg Loss: 0.17500  Avg mIoU:  70.11  
[Epoch: 84] [Batch: 0401/0580] Loss: 0.21607  Avg Loss: 0.17545  Avg mIoU:  70.13  
[Epoch: 84] [Batch: 0451/0580] Loss: 0.17733  Avg Loss: 0.17549  Avg mIoU:  70.11  
[Epoch: 84] [Batch: 0501/0580] Loss: 0.14724  Avg Loss: 0.17520  Avg mIoU:  70.33  
[Epoch: 84] [Batch: 0551/0580] Loss: 0.19667  Avg Loss: 0.17465  Avg mIoU:  70.45  

*** Training [@Epoch 84] Avg Loss: 0.17440  Avg mIoU:  70.57  ***

[Epoch: 84] [Batch: 0001/0050] Loss: 0.17272  Avg Loss: 0.17272  Avg mIoU:  61.79  

*** Validation [@Epoch 84] Avg Loss: 0.25777  Avg mIoU:  59.77  ***

[Epoch: 85] [Batch: 0001/0580] Loss: 0.18010  Avg Loss: 0.18010  Avg mIoU:  38.71  
[Epoch: 85] [Batch: 0051/0580] Loss: 0.19075  Avg Loss: 0.17714  Avg mIoU:  70.65  
[Epoch: 85] [Batch: 0101/0580] Loss: 0.18168  Avg Loss: 0.17771  Avg mIoU:  69.95  
[Epoch: 85] [Batch: 0151/0580] Loss: 0.22705  Avg Loss: 0.17699  Avg mIoU:  70.00  
[Epoch: 85] [Batch: 0201/0580] Loss: 0.15934  Avg Loss: 0.17560  Avg mIoU:  70.77  
[Epoch: 85] [Batch: 0251/0580] Loss: 0.18517  Avg Loss: 0.17538  Avg mIoU:  70.75  
[Epoch: 85] [Batch: 0301/0580] Loss: 0.18581  Avg Loss: 0.17504  Avg mIoU:  70.49  
[Epoch: 85] [Batch: 0351/0580] Loss: 0.19689  Avg Loss: 0.17374  Avg mIoU:  70.74  
[Epoch: 85] [Batch: 0401/0580] Loss: 0.17406  Avg Loss: 0.17343  Avg mIoU:  70.75  
[Epoch: 85] [Batch: 0451/0580] Loss: 0.19272  Avg Loss: 0.17474  Avg mIoU:  70.61  
[Epoch: 85] [Batch: 0501/0580] Loss: 0.20614  Avg Loss: 0.17553  Avg mIoU:  70.51  
[Epoch: 85] [Batch: 0551/0580] Loss: 0.21332  Avg Loss: 0.17542  Avg mIoU:  70.35  

*** Training [@Epoch 85] Avg Loss: 0.17586  Avg mIoU:  70.42  ***

[Epoch: 85] [Batch: 0001/0050] Loss: 0.17894  Avg Loss: 0.17894  Avg mIoU:  60.64  

*** Validation [@Epoch 85] Avg Loss: 0.24103  Avg mIoU:  58.26  ***

[Epoch: 86] [Batch: 0001/0580] Loss: 0.14659  Avg Loss: 0.14659  Avg mIoU:  31.21  
[Epoch: 86] [Batch: 0051/0580] Loss: 0.14752  Avg Loss: 0.16577  Avg mIoU:  69.61  
[Epoch: 86] [Batch: 0101/0580] Loss: 0.16613  Avg Loss: 0.16336  Avg mIoU:  71.48  
[Epoch: 86] [Batch: 0151/0580] Loss: 0.18829  Avg Loss: 0.16764  Avg mIoU:  71.08  
[Epoch: 86] [Batch: 0201/0580] Loss: 0.12432  Avg Loss: 0.17004  Avg mIoU:  71.35  
[Epoch: 86] [Batch: 0251/0580] Loss: 0.13570  Avg Loss: 0.17212  Avg mIoU:  70.91  
[Epoch: 86] [Batch: 0301/0580] Loss: 0.15019  Avg Loss: 0.17295  Avg mIoU:  70.60  
[Epoch: 86] [Batch: 0351/0580] Loss: 0.16045  Avg Loss: 0.17310  Avg mIoU:  70.69  
[Epoch: 86] [Batch: 0401/0580] Loss: 0.17323  Avg Loss: 0.17380  Avg mIoU:  71.01  
[Epoch: 86] [Batch: 0451/0580] Loss: 0.15357  Avg Loss: 0.17373  Avg mIoU:  70.99  
[Epoch: 86] [Batch: 0501/0580] Loss: 0.20249  Avg Loss: 0.17440  Avg mIoU:  71.00  
[Epoch: 86] [Batch: 0551/0580] Loss: 0.18997  Avg Loss: 0.17409  Avg mIoU:  70.86  

*** Training [@Epoch 86] Avg Loss: 0.17356  Avg mIoU:  70.89  ***

[Epoch: 86] [Batch: 0001/0050] Loss: 0.16521  Avg Loss: 0.16521  Avg mIoU:  63.83  

*** Validation [@Epoch 86] Avg Loss: 0.24894  Avg mIoU:  60.50  ***

[Epoch: 87] [Batch: 0001/0580] Loss: 0.15940  Avg Loss: 0.15940  Avg mIoU:  56.48  
[Epoch: 87] [Batch: 0051/0580] Loss: 0.11562  Avg Loss: 0.17011  Avg mIoU:  70.08  
[Epoch: 87] [Batch: 0101/0580] Loss: 0.14061  Avg Loss: 0.17182  Avg mIoU:  70.57  
[Epoch: 87] [Batch: 0151/0580] Loss: 0.12717  Avg Loss: 0.16999  Avg mIoU:  70.51  
[Epoch: 87] [Batch: 0201/0580] Loss: 0.14347  Avg Loss: 0.17301  Avg mIoU:  70.23  
[Epoch: 87] [Batch: 0251/0580] Loss: 0.15892  Avg Loss: 0.17246  Avg mIoU:  70.29  
[Epoch: 87] [Batch: 0301/0580] Loss: 0.17495  Avg Loss: 0.17105  Avg mIoU:  70.62  
[Epoch: 87] [Batch: 0351/0580] Loss: 0.15455  Avg Loss: 0.17166  Avg mIoU:  70.73  
[Epoch: 87] [Batch: 0401/0580] Loss: 0.15348  Avg Loss: 0.17138  Avg mIoU:  70.58  
[Epoch: 87] [Batch: 0451/0580] Loss: 0.18877  Avg Loss: 0.17167  Avg mIoU:  70.78  
[Epoch: 87] [Batch: 0501/0580] Loss: 0.14114  Avg Loss: 0.17093  Avg mIoU:  70.85  
[Epoch: 87] [Batch: 0551/0580] Loss: 0.15219  Avg Loss: 0.17146  Avg mIoU:  71.01  

*** Training [@Epoch 87] Avg Loss: 0.17134  Avg mIoU:  71.07  ***

[Epoch: 87] [Batch: 0001/0050] Loss: 0.16573  Avg Loss: 0.16573  Avg mIoU:  57.82  

*** Validation [@Epoch 87] Avg Loss: 0.24457  Avg mIoU:  54.35  ***

[Epoch: 88] [Batch: 0001/0580] Loss: 0.22244  Avg Loss: 0.22244  Avg mIoU:  41.64  
[Epoch: 88] [Batch: 0051/0580] Loss: 0.17308  Avg Loss: 0.17233  Avg mIoU:  70.51  
[Epoch: 88] [Batch: 0101/0580] Loss: 0.20781  Avg Loss: 0.16753  Avg mIoU:  70.94  
[Epoch: 88] [Batch: 0151/0580] Loss: 0.16779  Avg Loss: 0.16796  Avg mIoU:  70.81  
[Epoch: 88] [Batch: 0201/0580] Loss: 0.14827  Avg Loss: 0.16814  Avg mIoU:  71.08  
[Epoch: 88] [Batch: 0251/0580] Loss: 0.22028  Avg Loss: 0.17134  Avg mIoU:  70.69  
[Epoch: 88] [Batch: 0301/0580] Loss: 0.13456  Avg Loss: 0.17040  Avg mIoU:  70.81  
[Epoch: 88] [Batch: 0351/0580] Loss: 0.11362  Avg Loss: 0.16900  Avg mIoU:  70.86  
[Epoch: 88] [Batch: 0401/0580] Loss: 0.18927  Avg Loss: 0.17000  Avg mIoU:  70.69  
[Epoch: 88] [Batch: 0451/0580] Loss: 0.15193  Avg Loss: 0.17030  Avg mIoU:  70.84  
[Epoch: 88] [Batch: 0501/0580] Loss: 0.15185  Avg Loss: 0.17150  Avg mIoU:  70.79  
[Epoch: 88] [Batch: 0551/0580] Loss: 0.16137  Avg Loss: 0.17211  Avg mIoU:  70.80  

*** Training [@Epoch 88] Avg Loss: 0.17215  Avg mIoU:  70.76  ***

[Epoch: 88] [Batch: 0001/0050] Loss: 0.16347  Avg Loss: 0.16347  Avg mIoU:  61.69  

*** Validation [@Epoch 88] Avg Loss: 0.24277  Avg mIoU:  60.13  ***

[Epoch: 89] [Batch: 0001/0580] Loss: 0.16934  Avg Loss: 0.16934  Avg mIoU:  45.54  
[Epoch: 89] [Batch: 0051/0580] Loss: 0.23482  Avg Loss: 0.17039  Avg mIoU:  71.27  
[Epoch: 89] [Batch: 0101/0580] Loss: 0.17218  Avg Loss: 0.16926  Avg mIoU:  71.79  
[Epoch: 89] [Batch: 0151/0580] Loss: 0.16155  Avg Loss: 0.16871  Avg mIoU:  71.86  
[Epoch: 89] [Batch: 0201/0580] Loss: 0.16500  Avg Loss: 0.17016  Avg mIoU:  71.15  
[Epoch: 89] [Batch: 0251/0580] Loss: 0.10838  Avg Loss: 0.17035  Avg mIoU:  71.13  
[Epoch: 89] [Batch: 0301/0580] Loss: 0.18498  Avg Loss: 0.17009  Avg mIoU:  71.17  
[Epoch: 89] [Batch: 0351/0580] Loss: 0.16948  Avg Loss: 0.17071  Avg mIoU:  71.14  
[Epoch: 89] [Batch: 0401/0580] Loss: 0.24591  Avg Loss: 0.17165  Avg mIoU:  71.07  
[Epoch: 89] [Batch: 0451/0580] Loss: 0.13702  Avg Loss: 0.17106  Avg mIoU:  71.11  
[Epoch: 89] [Batch: 0501/0580] Loss: 0.15271  Avg Loss: 0.17023  Avg mIoU:  71.18  
[Epoch: 89] [Batch: 0551/0580] Loss: 0.16157  Avg Loss: 0.17043  Avg mIoU:  71.28  

*** Training [@Epoch 89] Avg Loss: 0.17042  Avg mIoU:  71.36  ***

[Epoch: 89] [Batch: 0001/0050] Loss: 0.16612  Avg Loss: 0.16612  Avg mIoU:  61.50  

*** Validation [@Epoch 89] Avg Loss: 0.26111  Avg mIoU:  57.54  ***

[Epoch: 90] [Batch: 0001/0580] Loss: 0.18150  Avg Loss: 0.18150  Avg mIoU:  35.27  
[Epoch: 90] [Batch: 0051/0580] Loss: 0.15773  Avg Loss: 0.16613  Avg mIoU:  73.31  
[Epoch: 90] [Batch: 0101/0580] Loss: 0.12482  Avg Loss: 0.17055  Avg mIoU:  72.71  
[Epoch: 90] [Batch: 0151/0580] Loss: 0.17023  Avg Loss: 0.17095  Avg mIoU:  71.57  
[Epoch: 90] [Batch: 0201/0580] Loss: 0.16783  Avg Loss: 0.17226  Avg mIoU:  70.97  
[Epoch: 90] [Batch: 0251/0580] Loss: 0.16547  Avg Loss: 0.17238  Avg mIoU:  71.02  
[Epoch: 90] [Batch: 0301/0580] Loss: 0.09557  Avg Loss: 0.17352  Avg mIoU:  71.03  
[Epoch: 90] [Batch: 0351/0580] Loss: 0.16443  Avg Loss: 0.17326  Avg mIoU:  71.35  
[Epoch: 90] [Batch: 0401/0580] Loss: 0.17482  Avg Loss: 0.17325  Avg mIoU:  71.14  
[Epoch: 90] [Batch: 0451/0580] Loss: 0.20796  Avg Loss: 0.17267  Avg mIoU:  71.24  
[Epoch: 90] [Batch: 0501/0580] Loss: 0.14795  Avg Loss: 0.17298  Avg mIoU:  71.24  
[Epoch: 90] [Batch: 0551/0580] Loss: 0.17593  Avg Loss: 0.17243  Avg mIoU:  71.30  

*** Training [@Epoch 90] Avg Loss: 0.17265  Avg mIoU:  71.17  ***

[Epoch: 90] [Batch: 0001/0050] Loss: 0.16521  Avg Loss: 0.16521  Avg mIoU:  62.70  

*** Validation [@Epoch 90] Avg Loss: 0.25359  Avg mIoU:  60.10  ***

[Epoch: 91] [Batch: 0001/0580] Loss: 0.11716  Avg Loss: 0.11716  Avg mIoU:  61.24  
[Epoch: 91] [Batch: 0051/0580] Loss: 0.16372  Avg Loss: 0.17630  Avg mIoU:  69.80  
[Epoch: 91] [Batch: 0101/0580] Loss: 0.23021  Avg Loss: 0.17509  Avg mIoU:  70.88  
[Epoch: 91] [Batch: 0151/0580] Loss: 0.23655  Avg Loss: 0.17229  Avg mIoU:  71.34  
[Epoch: 91] [Batch: 0201/0580] Loss: 0.16127  Avg Loss: 0.17211  Avg mIoU:  71.51  
[Epoch: 91] [Batch: 0251/0580] Loss: 0.12492  Avg Loss: 0.17109  Avg mIoU:  71.41  
[Epoch: 91] [Batch: 0301/0580] Loss: 0.21941  Avg Loss: 0.17110  Avg mIoU:  71.69  
[Epoch: 91] [Batch: 0351/0580] Loss: 0.12367  Avg Loss: 0.16962  Avg mIoU:  71.98  
[Epoch: 91] [Batch: 0401/0580] Loss: 0.21207  Avg Loss: 0.16910  Avg mIoU:  72.07  
[Epoch: 91] [Batch: 0451/0580] Loss: 0.18722  Avg Loss: 0.17080  Avg mIoU:  71.82  
[Epoch: 91] [Batch: 0501/0580] Loss: 0.21192  Avg Loss: 0.17106  Avg mIoU:  71.74  
[Epoch: 91] [Batch: 0551/0580] Loss: 0.21466  Avg Loss: 0.17085  Avg mIoU:  71.68  

*** Training [@Epoch 91] Avg Loss: 0.17078  Avg mIoU:  71.39  ***

[Epoch: 91] [Batch: 0001/0050] Loss: 0.17185  Avg Loss: 0.17185  Avg mIoU:  59.65  

*** Validation [@Epoch 91] Avg Loss: 0.25667  Avg mIoU:  58.08  ***

[Epoch: 92] [Batch: 0001/0580] Loss: 0.13720  Avg Loss: 0.13720  Avg mIoU:  39.01  
[Epoch: 92] [Batch: 0051/0580] Loss: 0.13010  Avg Loss: 0.16177  Avg mIoU:  71.07  
[Epoch: 92] [Batch: 0101/0580] Loss: 0.18767  Avg Loss: 0.16518  Avg mIoU:  71.88  
[Epoch: 92] [Batch: 0151/0580] Loss: 0.19618  Avg Loss: 0.16445  Avg mIoU:  71.70  
[Epoch: 92] [Batch: 0201/0580] Loss: 0.21598  Avg Loss: 0.16740  Avg mIoU:  71.63  
[Epoch: 92] [Batch: 0251/0580] Loss: 0.14432  Avg Loss: 0.16788  Avg mIoU:  71.42  
[Epoch: 92] [Batch: 0301/0580] Loss: 0.14318  Avg Loss: 0.16860  Avg mIoU:  71.69  
[Epoch: 92] [Batch: 0351/0580] Loss: 0.19629  Avg Loss: 0.16923  Avg mIoU:  71.75  
[Epoch: 92] [Batch: 0401/0580] Loss: 0.19439  Avg Loss: 0.16917  Avg mIoU:  71.58  
[Epoch: 92] [Batch: 0451/0580] Loss: 0.15562  Avg Loss: 0.17018  Avg mIoU:  71.33  
[Epoch: 92] [Batch: 0501/0580] Loss: 0.20302  Avg Loss: 0.17193  Avg mIoU:  71.17  
[Epoch: 92] [Batch: 0551/0580] Loss: 0.16231  Avg Loss: 0.17142  Avg mIoU:  71.20  

*** Training [@Epoch 92] Avg Loss: 0.17142  Avg mIoU:  71.15  ***

[Epoch: 92] [Batch: 0001/0050] Loss: 0.17464  Avg Loss: 0.17464  Avg mIoU:  61.09  

*** Validation [@Epoch 92] Avg Loss: 0.25781  Avg mIoU:  58.24  ***

[Epoch: 93] [Batch: 0001/0580] Loss: 0.22331  Avg Loss: 0.22331  Avg mIoU:  44.89  
[Epoch: 93] [Batch: 0051/0580] Loss: 0.18129  Avg Loss: 0.17783  Avg mIoU:  70.35  
[Epoch: 93] [Batch: 0101/0580] Loss: 0.15592  Avg Loss: 0.16753  Avg mIoU:  71.57  
[Epoch: 93] [Batch: 0151/0580] Loss: 0.16867  Avg Loss: 0.17072  Avg mIoU:  71.78  
[Epoch: 93] [Batch: 0201/0580] Loss: 0.13784  Avg Loss: 0.17016  Avg mIoU:  72.03  
[Epoch: 93] [Batch: 0251/0580] Loss: 0.18368  Avg Loss: 0.17191  Avg mIoU:  71.83  
[Epoch: 93] [Batch: 0301/0580] Loss: 0.13375  Avg Loss: 0.17121  Avg mIoU:  71.92  
[Epoch: 93] [Batch: 0351/0580] Loss: 0.16680  Avg Loss: 0.17012  Avg mIoU:  71.84  
[Epoch: 93] [Batch: 0401/0580] Loss: 0.18951  Avg Loss: 0.17006  Avg mIoU:  71.67  
[Epoch: 93] [Batch: 0451/0580] Loss: 0.16626  Avg Loss: 0.16954  Avg mIoU:  71.55  
[Epoch: 93] [Batch: 0501/0580] Loss: 0.18174  Avg Loss: 0.16993  Avg mIoU:  71.32  
[Epoch: 93] [Batch: 0551/0580] Loss: 0.15972  Avg Loss: 0.17057  Avg mIoU:  71.20  

*** Training [@Epoch 93] Avg Loss: 0.17093  Avg mIoU:  71.15  ***

[Epoch: 93] [Batch: 0001/0050] Loss: 0.15760  Avg Loss: 0.15760  Avg mIoU:  62.20  

*** Validation [@Epoch 93] Avg Loss: 0.25107  Avg mIoU:  57.14  ***

[Epoch: 94] [Batch: 0001/0580] Loss: 0.23909  Avg Loss: 0.23909  Avg mIoU:  32.03  
[Epoch: 94] [Batch: 0051/0580] Loss: 0.18202  Avg Loss: 0.17721  Avg mIoU:  71.12  
[Epoch: 94] [Batch: 0101/0580] Loss: 0.23593  Avg Loss: 0.17298  Avg mIoU:  71.93  
[Epoch: 94] [Batch: 0151/0580] Loss: 0.19581  Avg Loss: 0.17102  Avg mIoU:  71.59  
[Epoch: 94] [Batch: 0201/0580] Loss: 0.10551  Avg Loss: 0.16665  Avg mIoU:  71.93  
[Epoch: 94] [Batch: 0251/0580] Loss: 0.11130  Avg Loss: 0.16677  Avg mIoU:  71.87  
[Epoch: 94] [Batch: 0301/0580] Loss: 0.11332  Avg Loss: 0.16668  Avg mIoU:  71.76  
[Epoch: 94] [Batch: 0351/0580] Loss: 0.19148  Avg Loss: 0.17044  Avg mIoU:  71.47  
[Epoch: 94] [Batch: 0401/0580] Loss: 0.12174  Avg Loss: 0.17079  Avg mIoU:  71.45  
[Epoch: 94] [Batch: 0451/0580] Loss: 0.22270  Avg Loss: 0.17075  Avg mIoU:  71.54  
[Epoch: 94] [Batch: 0501/0580] Loss: 0.19709  Avg Loss: 0.17012  Avg mIoU:  71.40  
[Epoch: 94] [Batch: 0551/0580] Loss: 0.15524  Avg Loss: 0.17096  Avg mIoU:  71.28  

*** Training [@Epoch 94] Avg Loss: 0.17073  Avg mIoU:  71.32  ***

[Epoch: 94] [Batch: 0001/0050] Loss: 0.17153  Avg Loss: 0.17153  Avg mIoU:  60.14  

*** Validation [@Epoch 94] Avg Loss: 0.24410  Avg mIoU:  59.90  ***

[Epoch: 95] [Batch: 0001/0580] Loss: 0.15455  Avg Loss: 0.15455  Avg mIoU:  45.40  
[Epoch: 95] [Batch: 0051/0580] Loss: 0.21707  Avg Loss: 0.16806  Avg mIoU:  71.09  
[Epoch: 95] [Batch: 0101/0580] Loss: 0.12938  Avg Loss: 0.16951  Avg mIoU:  70.42  
[Epoch: 95] [Batch: 0151/0580] Loss: 0.20822  Avg Loss: 0.17150  Avg mIoU:  70.56  
[Epoch: 95] [Batch: 0201/0580] Loss: 0.21336  Avg Loss: 0.17363  Avg mIoU:  70.56  
[Epoch: 95] [Batch: 0251/0580] Loss: 0.13021  Avg Loss: 0.17343  Avg mIoU:  70.50  
[Epoch: 95] [Batch: 0301/0580] Loss: 0.12739  Avg Loss: 0.17261  Avg mIoU:  70.66  
[Epoch: 95] [Batch: 0351/0580] Loss: 0.11789  Avg Loss: 0.17181  Avg mIoU:  70.79  
[Epoch: 95] [Batch: 0401/0580] Loss: 0.12504  Avg Loss: 0.17145  Avg mIoU:  70.89  
[Epoch: 95] [Batch: 0451/0580] Loss: 0.17335  Avg Loss: 0.17108  Avg mIoU:  70.83  
[Epoch: 95] [Batch: 0501/0580] Loss: 0.18412  Avg Loss: 0.17087  Avg mIoU:  71.13  
[Epoch: 95] [Batch: 0551/0580] Loss: 0.22284  Avg Loss: 0.17008  Avg mIoU:  71.23  

*** Training [@Epoch 95] Avg Loss: 0.17032  Avg mIoU:  71.24  ***

[Epoch: 95] [Batch: 0001/0050] Loss: 0.19471  Avg Loss: 0.19471  Avg mIoU:  60.49  

*** Validation [@Epoch 95] Avg Loss: 0.25792  Avg mIoU:  60.92  ***

[Epoch: 96] [Batch: 0001/0580] Loss: 0.15164  Avg Loss: 0.15164  Avg mIoU:  45.89  
[Epoch: 96] [Batch: 0051/0580] Loss: 0.15188  Avg Loss: 0.17031  Avg mIoU:  71.01  
[Epoch: 96] [Batch: 0101/0580] Loss: 0.14318  Avg Loss: 0.16854  Avg mIoU:  72.26  
[Epoch: 96] [Batch: 0151/0580] Loss: 0.14725  Avg Loss: 0.17029  Avg mIoU:  72.27  
[Epoch: 96] [Batch: 0201/0580] Loss: 0.14899  Avg Loss: 0.16893  Avg mIoU:  72.16  
[Epoch: 96] [Batch: 0251/0580] Loss: 0.19842  Avg Loss: 0.16884  Avg mIoU:  71.84  
[Epoch: 96] [Batch: 0301/0580] Loss: 0.15423  Avg Loss: 0.16968  Avg mIoU:  71.65  
[Epoch: 96] [Batch: 0351/0580] Loss: 0.12689  Avg Loss: 0.16989  Avg mIoU:  71.70  
[Epoch: 96] [Batch: 0401/0580] Loss: 0.11373  Avg Loss: 0.16981  Avg mIoU:  71.89  
[Epoch: 96] [Batch: 0451/0580] Loss: 0.26024  Avg Loss: 0.17064  Avg mIoU:  71.47  
[Epoch: 96] [Batch: 0501/0580] Loss: 0.24516  Avg Loss: 0.17098  Avg mIoU:  71.45  
[Epoch: 96] [Batch: 0551/0580] Loss: 0.19880  Avg Loss: 0.17073  Avg mIoU:  71.36  

*** Training [@Epoch 96] Avg Loss: 0.17100  Avg mIoU:  71.38  ***

[Epoch: 96] [Batch: 0001/0050] Loss: 0.16413  Avg Loss: 0.16413  Avg mIoU:  56.91  

*** Validation [@Epoch 96] Avg Loss: 0.24950  Avg mIoU:  57.34  ***

[Epoch: 97] [Batch: 0001/0580] Loss: 0.22315  Avg Loss: 0.22315  Avg mIoU:  38.04  
[Epoch: 97] [Batch: 0051/0580] Loss: 0.36539  Avg Loss: 0.17624  Avg mIoU:  71.67  
[Epoch: 97] [Batch: 0101/0580] Loss: 0.18069  Avg Loss: 0.17709  Avg mIoU:  71.00  
[Epoch: 97] [Batch: 0151/0580] Loss: 0.16067  Avg Loss: 0.17448  Avg mIoU:  70.98  
[Epoch: 97] [Batch: 0201/0580] Loss: 0.18082  Avg Loss: 0.17262  Avg mIoU:  70.88  
[Epoch: 97] [Batch: 0251/0580] Loss: 0.16305  Avg Loss: 0.17360  Avg mIoU:  70.58  
[Epoch: 97] [Batch: 0301/0580] Loss: 0.13250  Avg Loss: 0.17362  Avg mIoU:  70.59  
[Epoch: 97] [Batch: 0351/0580] Loss: 0.11656  Avg Loss: 0.17265  Avg mIoU:  70.81  
[Epoch: 97] [Batch: 0401/0580] Loss: 0.19806  Avg Loss: 0.17182  Avg mIoU:  70.88  
[Epoch: 97] [Batch: 0451/0580] Loss: 0.10174  Avg Loss: 0.17019  Avg mIoU:  71.26  
[Epoch: 97] [Batch: 0501/0580] Loss: 0.18534  Avg Loss: 0.16974  Avg mIoU:  71.35  
[Epoch: 97] [Batch: 0551/0580] Loss: 0.20443  Avg Loss: 0.16947  Avg mIoU:  71.43  

*** Training [@Epoch 97] Avg Loss: 0.16894  Avg mIoU:  71.54  ***

[Epoch: 97] [Batch: 0001/0050] Loss: 0.16737  Avg Loss: 0.16737  Avg mIoU:  54.73  

*** Validation [@Epoch 97] Avg Loss: 0.24486  Avg mIoU:  55.18  ***

[Epoch: 98] [Batch: 0001/0580] Loss: 0.16837  Avg Loss: 0.16837  Avg mIoU:  23.53  
[Epoch: 98] [Batch: 0051/0580] Loss: 0.17364  Avg Loss: 0.18063  Avg mIoU:  69.16  
[Epoch: 98] [Batch: 0101/0580] Loss: 0.13762  Avg Loss: 0.17504  Avg mIoU:  69.36  
[Epoch: 98] [Batch: 0151/0580] Loss: 0.16441  Avg Loss: 0.17603  Avg mIoU:  69.90  
[Epoch: 98] [Batch: 0201/0580] Loss: 0.21535  Avg Loss: 0.17381  Avg mIoU:  70.08  
[Epoch: 98] [Batch: 0251/0580] Loss: 0.24976  Avg Loss: 0.17406  Avg mIoU:  70.47  
[Epoch: 98] [Batch: 0301/0580] Loss: 0.21474  Avg Loss: 0.17400  Avg mIoU:  70.85  
[Epoch: 98] [Batch: 0351/0580] Loss: 0.21060  Avg Loss: 0.17204  Avg mIoU:  70.82  
[Epoch: 98] [Batch: 0401/0580] Loss: 0.16780  Avg Loss: 0.17144  Avg mIoU:  70.96  
[Epoch: 98] [Batch: 0451/0580] Loss: 0.12081  Avg Loss: 0.17032  Avg mIoU:  71.12  
[Epoch: 98] [Batch: 0501/0580] Loss: 0.11655  Avg Loss: 0.17022  Avg mIoU:  71.27  
[Epoch: 98] [Batch: 0551/0580] Loss: 0.13876  Avg Loss: 0.17003  Avg mIoU:  71.23  

*** Training [@Epoch 98] Avg Loss: 0.16961  Avg mIoU:  71.31  ***

[Epoch: 98] [Batch: 0001/0050] Loss: 0.14845  Avg Loss: 0.14845  Avg mIoU:  64.67  

*** Validation [@Epoch 98] Avg Loss: 0.24596  Avg mIoU:  59.51  ***

[Epoch: 99] [Batch: 0001/0580] Loss: 0.16724  Avg Loss: 0.16724  Avg mIoU:  49.97  
[Epoch: 99] [Batch: 0051/0580] Loss: 0.20422  Avg Loss: 0.17179  Avg mIoU:  71.94  
[Epoch: 99] [Batch: 0101/0580] Loss: 0.14575  Avg Loss: 0.17078  Avg mIoU:  71.82  
[Epoch: 99] [Batch: 0151/0580] Loss: 0.12153  Avg Loss: 0.17113  Avg mIoU:  71.88  
[Epoch: 99] [Batch: 0201/0580] Loss: 0.11070  Avg Loss: 0.16963  Avg mIoU:  72.16  
[Epoch: 99] [Batch: 0251/0580] Loss: 0.19778  Avg Loss: 0.17015  Avg mIoU:  71.77  
[Epoch: 99] [Batch: 0301/0580] Loss: 0.19661  Avg Loss: 0.16963  Avg mIoU:  71.64  
[Epoch: 99] [Batch: 0351/0580] Loss: 0.13345  Avg Loss: 0.16985  Avg mIoU:  71.24  
[Epoch: 99] [Batch: 0401/0580] Loss: 0.19505  Avg Loss: 0.16865  Avg mIoU:  71.32  
[Epoch: 99] [Batch: 0451/0580] Loss: 0.17899  Avg Loss: 0.16808  Avg mIoU:  71.57  
[Epoch: 99] [Batch: 0501/0580] Loss: 0.18845  Avg Loss: 0.16799  Avg mIoU:  71.56  
[Epoch: 99] [Batch: 0551/0580] Loss: 0.13632  Avg Loss: 0.16825  Avg mIoU:  71.68  

*** Training [@Epoch 99] Avg Loss: 0.16807  Avg mIoU:  71.67  ***

[Epoch: 99] [Batch: 0001/0050] Loss: 0.16185  Avg Loss: 0.16185  Avg mIoU:  60.81  

*** Validation [@Epoch 99] Avg Loss: 0.24826  Avg mIoU:  57.79  ***

[Epoch: 100] [Batch: 0001/0580] Loss: 0.13968  Avg Loss: 0.13968  Avg mIoU:  35.63  
[Epoch: 100] [Batch: 0051/0580] Loss: 0.17458  Avg Loss: 0.15979  Avg mIoU:  71.66  
[Epoch: 100] [Batch: 0101/0580] Loss: 0.29089  Avg Loss: 0.16600  Avg mIoU:  71.21  
[Epoch: 100] [Batch: 0151/0580] Loss: 0.09297  Avg Loss: 0.16475  Avg mIoU:  71.08  
[Epoch: 100] [Batch: 0201/0580] Loss: 0.29500  Avg Loss: 0.16474  Avg mIoU:  71.25  
[Epoch: 100] [Batch: 0251/0580] Loss: 0.22361  Avg Loss: 0.16679  Avg mIoU:  71.42  
[Epoch: 100] [Batch: 0301/0580] Loss: 0.13344  Avg Loss: 0.16888  Avg mIoU:  71.06  
[Epoch: 100] [Batch: 0351/0580] Loss: 0.15180  Avg Loss: 0.16947  Avg mIoU:  71.10  
[Epoch: 100] [Batch: 0401/0580] Loss: 0.14849  Avg Loss: 0.16859  Avg mIoU:  71.16  
[Epoch: 100] [Batch: 0451/0580] Loss: 0.19068  Avg Loss: 0.16941  Avg mIoU:  71.08  
[Epoch: 100] [Batch: 0501/0580] Loss: 0.17678  Avg Loss: 0.16996  Avg mIoU:  70.96  
[Epoch: 100] [Batch: 0551/0580] Loss: 0.12701  Avg Loss: 0.16874  Avg mIoU:  71.14  

*** Training [@Epoch 100] Avg Loss: 0.16824  Avg mIoU:  71.30  ***

[Epoch: 100] [Batch: 0001/0050] Loss: 0.16995  Avg Loss: 0.16995  Avg mIoU:  63.55  

*** Validation [@Epoch 100] Avg Loss: 0.25107  Avg mIoU:  60.99  ***

[Epoch: 101] [Batch: 0001/0580] Loss: 0.15516  Avg Loss: 0.15516  Avg mIoU:  43.33  
[Epoch: 101] [Batch: 0051/0580] Loss: 0.15219  Avg Loss: 0.16471  Avg mIoU:  72.29  
[Epoch: 101] [Batch: 0101/0580] Loss: 0.12609  Avg Loss: 0.16947  Avg mIoU:  72.76  
[Epoch: 101] [Batch: 0151/0580] Loss: 0.22154  Avg Loss: 0.16906  Avg mIoU:  72.63  
[Epoch: 101] [Batch: 0201/0580] Loss: 0.08992  Avg Loss: 0.16875  Avg mIoU:  72.07  
[Epoch: 101] [Batch: 0251/0580] Loss: 0.17533  Avg Loss: 0.16765  Avg mIoU:  71.92  
[Epoch: 101] [Batch: 0301/0580] Loss: 0.17363  Avg Loss: 0.16710  Avg mIoU:  71.96  
[Epoch: 101] [Batch: 0351/0580] Loss: 0.16568  Avg Loss: 0.16776  Avg mIoU:  71.93  
[Epoch: 101] [Batch: 0401/0580] Loss: 0.21167  Avg Loss: 0.16815  Avg mIoU:  71.78  
[Epoch: 101] [Batch: 0451/0580] Loss: 0.18858  Avg Loss: 0.16749  Avg mIoU:  71.85  
[Epoch: 101] [Batch: 0501/0580] Loss: 0.15483  Avg Loss: 0.16799  Avg mIoU:  71.71  
[Epoch: 101] [Batch: 0551/0580] Loss: 0.17393  Avg Loss: 0.16814  Avg mIoU:  71.59  

*** Training [@Epoch 101] Avg Loss: 0.16751  Avg mIoU:  71.73  ***

[Epoch: 101] [Batch: 0001/0050] Loss: 0.17132  Avg Loss: 0.17132  Avg mIoU:  61.39  

*** Validation [@Epoch 101] Avg Loss: 0.24968  Avg mIoU:  60.55  ***

[Epoch: 102] [Batch: 0001/0580] Loss: 0.13535  Avg Loss: 0.13535  Avg mIoU:  44.71  
[Epoch: 102] [Batch: 0051/0580] Loss: 0.22685  Avg Loss: 0.16978  Avg mIoU:  70.69  
[Epoch: 102] [Batch: 0101/0580] Loss: 0.12517  Avg Loss: 0.16938  Avg mIoU:  70.10  
[Epoch: 102] [Batch: 0151/0580] Loss: 0.18571  Avg Loss: 0.16958  Avg mIoU:  70.84  
[Epoch: 102] [Batch: 0201/0580] Loss: 0.16036  Avg Loss: 0.17060  Avg mIoU:  71.38  
[Epoch: 102] [Batch: 0251/0580] Loss: 0.15065  Avg Loss: 0.17240  Avg mIoU:  71.19  
[Epoch: 102] [Batch: 0301/0580] Loss: 0.18019  Avg Loss: 0.17132  Avg mIoU:  71.00  
[Epoch: 102] [Batch: 0351/0580] Loss: 0.22998  Avg Loss: 0.17076  Avg mIoU:  70.84  
[Epoch: 102] [Batch: 0401/0580] Loss: 0.20408  Avg Loss: 0.16962  Avg mIoU:  70.97  
[Epoch: 102] [Batch: 0451/0580] Loss: 0.12705  Avg Loss: 0.16793  Avg mIoU:  71.03  
[Epoch: 102] [Batch: 0501/0580] Loss: 0.17992  Avg Loss: 0.16877  Avg mIoU:  70.97  
[Epoch: 102] [Batch: 0551/0580] Loss: 0.12602  Avg Loss: 0.16962  Avg mIoU:  71.02  

*** Training [@Epoch 102] Avg Loss: 0.17023  Avg mIoU:  70.96  ***

[Epoch: 102] [Batch: 0001/0050] Loss: 0.15867  Avg Loss: 0.15867  Avg mIoU:  60.54  

*** Validation [@Epoch 102] Avg Loss: 0.24696  Avg mIoU:  58.15  ***

[Epoch: 103] [Batch: 0001/0580] Loss: 0.16369  Avg Loss: 0.16369  Avg mIoU:  52.24  
[Epoch: 103] [Batch: 0051/0580] Loss: 0.14023  Avg Loss: 0.18204  Avg mIoU:  71.37  
[Epoch: 103] [Batch: 0101/0580] Loss: 0.16162  Avg Loss: 0.17646  Avg mIoU:  71.33  
[Epoch: 103] [Batch: 0151/0580] Loss: 0.20260  Avg Loss: 0.17249  Avg mIoU:  71.54  
[Epoch: 103] [Batch: 0201/0580] Loss: 0.16636  Avg Loss: 0.17091  Avg mIoU:  71.49  
[Epoch: 103] [Batch: 0251/0580] Loss: 0.14380  Avg Loss: 0.16839  Avg mIoU:  71.75  
[Epoch: 103] [Batch: 0301/0580] Loss: 0.23279  Avg Loss: 0.16836  Avg mIoU:  71.38  
[Epoch: 103] [Batch: 0351/0580] Loss: 0.23999  Avg Loss: 0.16895  Avg mIoU:  70.95  
[Epoch: 103] [Batch: 0401/0580] Loss: 0.17719  Avg Loss: 0.16868  Avg mIoU:  71.14  
[Epoch: 103] [Batch: 0451/0580] Loss: 0.16863  Avg Loss: 0.16846  Avg mIoU:  71.23  
[Epoch: 103] [Batch: 0501/0580] Loss: 0.18658  Avg Loss: 0.16836  Avg mIoU:  71.30  
[Epoch: 103] [Batch: 0551/0580] Loss: 0.14706  Avg Loss: 0.16786  Avg mIoU:  71.36  

*** Training [@Epoch 103] Avg Loss: 0.16821  Avg mIoU:  71.44  ***

[Epoch: 103] [Batch: 0001/0050] Loss: 0.16108  Avg Loss: 0.16108  Avg mIoU:  61.88  

*** Validation [@Epoch 103] Avg Loss: 0.24608  Avg mIoU:  55.21  ***

[Epoch: 104] [Batch: 0001/0580] Loss: 0.16428  Avg Loss: 0.16428  Avg mIoU:  39.75  
[Epoch: 104] [Batch: 0051/0580] Loss: 0.18400  Avg Loss: 0.16805  Avg mIoU:  72.09  
[Epoch: 104] [Batch: 0101/0580] Loss: 0.19120  Avg Loss: 0.17075  Avg mIoU:  71.73  
[Epoch: 104] [Batch: 0151/0580] Loss: 0.12275  Avg Loss: 0.17046  Avg mIoU:  71.31  
[Epoch: 104] [Batch: 0201/0580] Loss: 0.14378  Avg Loss: 0.17220  Avg mIoU:  71.33  
[Epoch: 104] [Batch: 0251/0580] Loss: 0.17379  Avg Loss: 0.16980  Avg mIoU:  71.21  
[Epoch: 104] [Batch: 0301/0580] Loss: 0.19909  Avg Loss: 0.16892  Avg mIoU:  71.30  
[Epoch: 104] [Batch: 0351/0580] Loss: 0.12137  Avg Loss: 0.16821  Avg mIoU:  71.51  
[Epoch: 104] [Batch: 0401/0580] Loss: 0.15834  Avg Loss: 0.16936  Avg mIoU:  71.67  
[Epoch: 104] [Batch: 0451/0580] Loss: 0.15727  Avg Loss: 0.16920  Avg mIoU:  71.72  
[Epoch: 104] [Batch: 0501/0580] Loss: 0.19475  Avg Loss: 0.16833  Avg mIoU:  71.75  
[Epoch: 104] [Batch: 0551/0580] Loss: 0.17226  Avg Loss: 0.16767  Avg mIoU:  71.77  

*** Training [@Epoch 104] Avg Loss: 0.16754  Avg mIoU:  71.80  ***

[Epoch: 104] [Batch: 0001/0050] Loss: 0.17231  Avg Loss: 0.17231  Avg mIoU:  61.95  

*** Validation [@Epoch 104] Avg Loss: 0.25839  Avg mIoU:  59.66  ***

[Epoch: 105] [Batch: 0001/0580] Loss: 0.14549  Avg Loss: 0.14549  Avg mIoU:  45.39  
[Epoch: 105] [Batch: 0051/0580] Loss: 0.09398  Avg Loss: 0.17036  Avg mIoU:  71.22  
[Epoch: 105] [Batch: 0101/0580] Loss: 0.21443  Avg Loss: 0.17136  Avg mIoU:  71.13  
[Epoch: 105] [Batch: 0151/0580] Loss: 0.15978  Avg Loss: 0.16810  Avg mIoU:  71.47  
[Epoch: 105] [Batch: 0201/0580] Loss: 0.12976  Avg Loss: 0.16942  Avg mIoU:  71.23  
[Epoch: 105] [Batch: 0251/0580] Loss: 0.13878  Avg Loss: 0.16907  Avg mIoU:  71.27  
[Epoch: 105] [Batch: 0301/0580] Loss: 0.16387  Avg Loss: 0.16719  Avg mIoU:  71.52  
[Epoch: 105] [Batch: 0351/0580] Loss: 0.17358  Avg Loss: 0.16740  Avg mIoU:  71.30  
[Epoch: 105] [Batch: 0401/0580] Loss: 0.14152  Avg Loss: 0.16809  Avg mIoU:  71.33  
[Epoch: 105] [Batch: 0451/0580] Loss: 0.18719  Avg Loss: 0.16837  Avg mIoU:  71.40  
[Epoch: 105] [Batch: 0501/0580] Loss: 0.21996  Avg Loss: 0.16848  Avg mIoU:  71.60  
[Epoch: 105] [Batch: 0551/0580] Loss: 0.13684  Avg Loss: 0.16834  Avg mIoU:  71.64  

*** Training [@Epoch 105] Avg Loss: 0.16808  Avg mIoU:  71.72  ***

[Epoch: 105] [Batch: 0001/0050] Loss: 0.15555  Avg Loss: 0.15555  Avg mIoU:  63.07  

*** Validation [@Epoch 105] Avg Loss: 0.23987  Avg mIoU:  60.03  ***

[Epoch: 106] [Batch: 0001/0580] Loss: 0.14137  Avg Loss: 0.14137  Avg mIoU:  50.28  
[Epoch: 106] [Batch: 0051/0580] Loss: 0.16942  Avg Loss: 0.16393  Avg mIoU:  71.71  
[Epoch: 106] [Batch: 0101/0580] Loss: 0.13268  Avg Loss: 0.16636  Avg mIoU:  72.71  
[Epoch: 106] [Batch: 0151/0580] Loss: 0.14551  Avg Loss: 0.16364  Avg mIoU:  72.52  
[Epoch: 106] [Batch: 0201/0580] Loss: 0.15386  Avg Loss: 0.16304  Avg mIoU:  72.43  
[Epoch: 106] [Batch: 0251/0580] Loss: 0.16091  Avg Loss: 0.16264  Avg mIoU:  72.24  
[Epoch: 106] [Batch: 0301/0580] Loss: 0.12327  Avg Loss: 0.16217  Avg mIoU:  72.02  
[Epoch: 106] [Batch: 0351/0580] Loss: 0.20221  Avg Loss: 0.16281  Avg mIoU:  72.23  
[Epoch: 106] [Batch: 0401/0580] Loss: 0.17290  Avg Loss: 0.16429  Avg mIoU:  71.91  
[Epoch: 106] [Batch: 0451/0580] Loss: 0.16362  Avg Loss: 0.16364  Avg mIoU:  71.90  
[Epoch: 106] [Batch: 0501/0580] Loss: 0.16136  Avg Loss: 0.16442  Avg mIoU:  71.73  
[Epoch: 106] [Batch: 0551/0580] Loss: 0.26267  Avg Loss: 0.16521  Avg mIoU:  71.79  

*** Training [@Epoch 106] Avg Loss: 0.16527  Avg mIoU:  71.69  ***

[Epoch: 106] [Batch: 0001/0050] Loss: 0.15796  Avg Loss: 0.15796  Avg mIoU:  60.46  

*** Validation [@Epoch 106] Avg Loss: 0.24784  Avg mIoU:  56.49  ***

[Epoch: 107] [Batch: 0001/0580] Loss: 0.17428  Avg Loss: 0.17428  Avg mIoU:  54.26  
[Epoch: 107] [Batch: 0051/0580] Loss: 0.13504  Avg Loss: 0.16188  Avg mIoU:  73.82  
[Epoch: 107] [Batch: 0101/0580] Loss: 0.19240  Avg Loss: 0.16612  Avg mIoU:  72.52  
[Epoch: 107] [Batch: 0151/0580] Loss: 0.13470  Avg Loss: 0.16761  Avg mIoU:  71.95  
[Epoch: 107] [Batch: 0201/0580] Loss: 0.18359  Avg Loss: 0.16517  Avg mIoU:  72.33  
[Epoch: 107] [Batch: 0251/0580] Loss: 0.16754  Avg Loss: 0.16335  Avg mIoU:  72.54  
[Epoch: 107] [Batch: 0301/0580] Loss: 0.14680  Avg Loss: 0.16460  Avg mIoU:  72.45  
[Epoch: 107] [Batch: 0351/0580] Loss: 0.15687  Avg Loss: 0.16517  Avg mIoU:  72.29  
[Epoch: 107] [Batch: 0401/0580] Loss: 0.25280  Avg Loss: 0.16536  Avg mIoU:  72.07  
[Epoch: 107] [Batch: 0451/0580] Loss: 0.14279  Avg Loss: 0.16562  Avg mIoU:  71.96  
[Epoch: 107] [Batch: 0501/0580] Loss: 0.17647  Avg Loss: 0.16505  Avg mIoU:  72.04  
[Epoch: 107] [Batch: 0551/0580] Loss: 0.16223  Avg Loss: 0.16526  Avg mIoU:  71.99  

*** Training [@Epoch 107] Avg Loss: 0.16591  Avg mIoU:  71.99  ***

[Epoch: 107] [Batch: 0001/0050] Loss: 0.15456  Avg Loss: 0.15456  Avg mIoU:  63.28  

*** Validation [@Epoch 107] Avg Loss: 0.25234  Avg mIoU:  59.76  ***

[Epoch: 108] [Batch: 0001/0580] Loss: 0.15499  Avg Loss: 0.15499  Avg mIoU:  43.69  
[Epoch: 108] [Batch: 0051/0580] Loss: 0.14248  Avg Loss: 0.16224  Avg mIoU:  74.06  
[Epoch: 108] [Batch: 0101/0580] Loss: 0.15626  Avg Loss: 0.16027  Avg mIoU:  73.18  
[Epoch: 108] [Batch: 0151/0580] Loss: 0.17657  Avg Loss: 0.16089  Avg mIoU:  73.46  
[Epoch: 108] [Batch: 0201/0580] Loss: 0.14097  Avg Loss: 0.16403  Avg mIoU:  73.16  
[Epoch: 108] [Batch: 0251/0580] Loss: 0.18822  Avg Loss: 0.16413  Avg mIoU:  72.84  
[Epoch: 108] [Batch: 0301/0580] Loss: 0.15704  Avg Loss: 0.16327  Avg mIoU:  72.92  
[Epoch: 108] [Batch: 0351/0580] Loss: 0.17678  Avg Loss: 0.16360  Avg mIoU:  73.01  
[Epoch: 108] [Batch: 0401/0580] Loss: 0.09550  Avg Loss: 0.16417  Avg mIoU:  72.60  
[Epoch: 108] [Batch: 0451/0580] Loss: 0.12744  Avg Loss: 0.16439  Avg mIoU:  72.45  
[Epoch: 108] [Batch: 0501/0580] Loss: 0.16328  Avg Loss: 0.16453  Avg mIoU:  72.26  
[Epoch: 108] [Batch: 0551/0580] Loss: 0.16501  Avg Loss: 0.16447  Avg mIoU:  72.33  

*** Training [@Epoch 108] Avg Loss: 0.16483  Avg mIoU:  72.23  ***

[Epoch: 108] [Batch: 0001/0050] Loss: 0.16409  Avg Loss: 0.16409  Avg mIoU:  62.33  

*** Validation [@Epoch 108] Avg Loss: 0.23957  Avg mIoU:  59.94  ***

[Epoch: 109] [Batch: 0001/0580] Loss: 0.16212  Avg Loss: 0.16212  Avg mIoU:  44.27  
[Epoch: 109] [Batch: 0051/0580] Loss: 0.15089  Avg Loss: 0.15981  Avg mIoU:  72.00  
[Epoch: 109] [Batch: 0101/0580] Loss: 0.13678  Avg Loss: 0.15981  Avg mIoU:  71.82  
[Epoch: 109] [Batch: 0151/0580] Loss: 0.14836  Avg Loss: 0.16241  Avg mIoU:  72.19  
[Epoch: 109] [Batch: 0201/0580] Loss: 0.15389  Avg Loss: 0.16784  Avg mIoU:  71.85  
[Epoch: 109] [Batch: 0251/0580] Loss: 0.12323  Avg Loss: 0.16535  Avg mIoU:  72.50  
[Epoch: 109] [Batch: 0301/0580] Loss: 0.15928  Avg Loss: 0.16603  Avg mIoU:  72.01  
[Epoch: 109] [Batch: 0351/0580] Loss: 0.18900  Avg Loss: 0.16536  Avg mIoU:  72.05  
[Epoch: 109] [Batch: 0401/0580] Loss: 0.14543  Avg Loss: 0.16596  Avg mIoU:  72.10  
[Epoch: 109] [Batch: 0451/0580] Loss: 0.16755  Avg Loss: 0.16628  Avg mIoU:  72.16  
[Epoch: 109] [Batch: 0501/0580] Loss: 0.11237  Avg Loss: 0.16585  Avg mIoU:  72.27  
[Epoch: 109] [Batch: 0551/0580] Loss: 0.11523  Avg Loss: 0.16588  Avg mIoU:  72.11  

*** Training [@Epoch 109] Avg Loss: 0.16586  Avg mIoU:  72.02  ***

[Epoch: 109] [Batch: 0001/0050] Loss: 0.16650  Avg Loss: 0.16650  Avg mIoU:  62.02  

*** Validation [@Epoch 109] Avg Loss: 0.24960  Avg mIoU:  58.92  ***

[Epoch: 110] [Batch: 0001/0580] Loss: 0.19904  Avg Loss: 0.19904  Avg mIoU:  56.02  
[Epoch: 110] [Batch: 0051/0580] Loss: 0.11800  Avg Loss: 0.15827  Avg mIoU:  72.77  
[Epoch: 110] [Batch: 0101/0580] Loss: 0.25214  Avg Loss: 0.16122  Avg mIoU:  72.57  
[Epoch: 110] [Batch: 0151/0580] Loss: 0.13036  Avg Loss: 0.16286  Avg mIoU:  72.24  
[Epoch: 110] [Batch: 0201/0580] Loss: 0.15958  Avg Loss: 0.16329  Avg mIoU:  72.35  
[Epoch: 110] [Batch: 0251/0580] Loss: 0.23479  Avg Loss: 0.16332  Avg mIoU:  72.70  
[Epoch: 110] [Batch: 0301/0580] Loss: 0.08416  Avg Loss: 0.16430  Avg mIoU:  72.41  
[Epoch: 110] [Batch: 0351/0580] Loss: 0.20553  Avg Loss: 0.16501  Avg mIoU:  72.39  
[Epoch: 110] [Batch: 0401/0580] Loss: 0.10927  Avg Loss: 0.16496  Avg mIoU:  72.39  
[Epoch: 110] [Batch: 0451/0580] Loss: 0.17035  Avg Loss: 0.16395  Avg mIoU:  72.39  
[Epoch: 110] [Batch: 0501/0580] Loss: 0.13897  Avg Loss: 0.16466  Avg mIoU:  72.09  
[Epoch: 110] [Batch: 0551/0580] Loss: 0.23753  Avg Loss: 0.16529  Avg mIoU:  72.07  

*** Training [@Epoch 110] Avg Loss: 0.16540  Avg mIoU:  72.11  ***

[Epoch: 110] [Batch: 0001/0050] Loss: 0.15788  Avg Loss: 0.15788  Avg mIoU:  62.91  

*** Validation [@Epoch 110] Avg Loss: 0.23766  Avg mIoU:  59.44  ***

[Epoch: 111] [Batch: 0001/0580] Loss: 0.12634  Avg Loss: 0.12634  Avg mIoU:  59.14  
[Epoch: 111] [Batch: 0051/0580] Loss: 0.09265  Avg Loss: 0.15548  Avg mIoU:  72.44  
[Epoch: 111] [Batch: 0101/0580] Loss: 0.18803  Avg Loss: 0.16048  Avg mIoU:  72.54  
[Epoch: 111] [Batch: 0151/0580] Loss: 0.22488  Avg Loss: 0.16450  Avg mIoU:  72.33  
[Epoch: 111] [Batch: 0201/0580] Loss: 0.14985  Avg Loss: 0.16555  Avg mIoU:  72.15  
[Epoch: 111] [Batch: 0251/0580] Loss: 0.17540  Avg Loss: 0.16633  Avg mIoU:  72.42  
[Epoch: 111] [Batch: 0301/0580] Loss: 0.15728  Avg Loss: 0.16604  Avg mIoU:  72.17  
[Epoch: 111] [Batch: 0351/0580] Loss: 0.37184  Avg Loss: 0.16654  Avg mIoU:  72.13  
[Epoch: 111] [Batch: 0401/0580] Loss: 0.11653  Avg Loss: 0.16690  Avg mIoU:  72.08  
[Epoch: 111] [Batch: 0451/0580] Loss: 0.19504  Avg Loss: 0.16686  Avg mIoU:  71.99  
[Epoch: 111] [Batch: 0501/0580] Loss: 0.12889  Avg Loss: 0.16613  Avg mIoU:  72.10  
[Epoch: 111] [Batch: 0551/0580] Loss: 0.16357  Avg Loss: 0.16565  Avg mIoU:  72.28  

*** Training [@Epoch 111] Avg Loss: 0.16567  Avg mIoU:  72.30  ***

[Epoch: 111] [Batch: 0001/0050] Loss: 0.16171  Avg Loss: 0.16171  Avg mIoU:  57.42  

*** Validation [@Epoch 111] Avg Loss: 0.26020  Avg mIoU:  53.46  ***

[Epoch: 112] [Batch: 0001/0580] Loss: 0.25032  Avg Loss: 0.25032  Avg mIoU:  44.76  
[Epoch: 112] [Batch: 0051/0580] Loss: 0.12216  Avg Loss: 0.16311  Avg mIoU:  71.36  
[Epoch: 112] [Batch: 0101/0580] Loss: 0.13363  Avg Loss: 0.16221  Avg mIoU:  71.38  
[Epoch: 112] [Batch: 0151/0580] Loss: 0.16974  Avg Loss: 0.16541  Avg mIoU:  71.28  
[Epoch: 112] [Batch: 0201/0580] Loss: 0.19470  Avg Loss: 0.16362  Avg mIoU:  71.41  
[Epoch: 112] [Batch: 0251/0580] Loss: 0.14915  Avg Loss: 0.16375  Avg mIoU:  71.76  
[Epoch: 112] [Batch: 0301/0580] Loss: 0.17617  Avg Loss: 0.16286  Avg mIoU:  71.93  
[Epoch: 112] [Batch: 0351/0580] Loss: 0.20672  Avg Loss: 0.16415  Avg mIoU:  72.01  
[Epoch: 112] [Batch: 0401/0580] Loss: 0.11050  Avg Loss: 0.16370  Avg mIoU:  71.98  
[Epoch: 112] [Batch: 0451/0580] Loss: 0.22852  Avg Loss: 0.16451  Avg mIoU:  71.80  
[Epoch: 112] [Batch: 0501/0580] Loss: 0.10044  Avg Loss: 0.16471  Avg mIoU:  71.95  
[Epoch: 112] [Batch: 0551/0580] Loss: 0.15249  Avg Loss: 0.16491  Avg mIoU:  72.12  

*** Training [@Epoch 112] Avg Loss: 0.16538  Avg mIoU:  71.96  ***

[Epoch: 112] [Batch: 0001/0050] Loss: 0.15273  Avg Loss: 0.15273  Avg mIoU:  64.64  

*** Validation [@Epoch 112] Avg Loss: 0.23992  Avg mIoU:  59.46  ***

[Epoch: 113] [Batch: 0001/0580] Loss: 0.19055  Avg Loss: 0.19055  Avg mIoU:  48.65  
[Epoch: 113] [Batch: 0051/0580] Loss: 0.14970  Avg Loss: 0.16501  Avg mIoU:  71.16  
[Epoch: 113] [Batch: 0101/0580] Loss: 0.15273  Avg Loss: 0.16526  Avg mIoU:  72.42  
[Epoch: 113] [Batch: 0151/0580] Loss: 0.20329  Avg Loss: 0.16639  Avg mIoU:  72.02  
[Epoch: 113] [Batch: 0201/0580] Loss: 0.18756  Avg Loss: 0.16878  Avg mIoU:  71.70  
[Epoch: 113] [Batch: 0251/0580] Loss: 0.13072  Avg Loss: 0.16912  Avg mIoU:  71.50  
[Epoch: 113] [Batch: 0301/0580] Loss: 0.23945  Avg Loss: 0.16906  Avg mIoU:  71.60  
[Epoch: 113] [Batch: 0351/0580] Loss: 0.14323  Avg Loss: 0.16684  Avg mIoU:  72.07  
[Epoch: 113] [Batch: 0401/0580] Loss: 0.19144  Avg Loss: 0.16694  Avg mIoU:  71.98  
[Epoch: 113] [Batch: 0451/0580] Loss: 0.20807  Avg Loss: 0.16581  Avg mIoU:  72.12  
[Epoch: 113] [Batch: 0501/0580] Loss: 0.15712  Avg Loss: 0.16559  Avg mIoU:  72.16  
[Epoch: 113] [Batch: 0551/0580] Loss: 0.13374  Avg Loss: 0.16510  Avg mIoU:  72.16  

*** Training [@Epoch 113] Avg Loss: 0.16491  Avg mIoU:  72.19  ***

[Epoch: 113] [Batch: 0001/0050] Loss: 0.15297  Avg Loss: 0.15297  Avg mIoU:  65.60  

*** Validation [@Epoch 113] Avg Loss: 0.23834  Avg mIoU:  60.64  ***

[Epoch: 114] [Batch: 0001/0580] Loss: 0.17347  Avg Loss: 0.17347  Avg mIoU:  46.65  
[Epoch: 114] [Batch: 0051/0580] Loss: 0.20307  Avg Loss: 0.15082  Avg mIoU:  73.03  
[Epoch: 114] [Batch: 0101/0580] Loss: 0.16262  Avg Loss: 0.15603  Avg mIoU:  72.57  
[Epoch: 114] [Batch: 0151/0580] Loss: 0.13497  Avg Loss: 0.15847  Avg mIoU:  72.67  
[Epoch: 114] [Batch: 0201/0580] Loss: 0.17860  Avg Loss: 0.15877  Avg mIoU:  72.35  
[Epoch: 114] [Batch: 0251/0580] Loss: 0.19263  Avg Loss: 0.15969  Avg mIoU:  72.07  
[Epoch: 114] [Batch: 0301/0580] Loss: 0.16329  Avg Loss: 0.16124  Avg mIoU:  72.09  
[Epoch: 114] [Batch: 0351/0580] Loss: 0.18464  Avg Loss: 0.16167  Avg mIoU:  72.10  
[Epoch: 114] [Batch: 0401/0580] Loss: 0.19789  Avg Loss: 0.16238  Avg mIoU:  72.14  
[Epoch: 114] [Batch: 0451/0580] Loss: 0.17129  Avg Loss: 0.16214  Avg mIoU:  72.14  
[Epoch: 114] [Batch: 0501/0580] Loss: 0.16311  Avg Loss: 0.16284  Avg mIoU:  72.16  
[Epoch: 114] [Batch: 0551/0580] Loss: 0.14214  Avg Loss: 0.16233  Avg mIoU:  72.15  

*** Training [@Epoch 114] Avg Loss: 0.16351  Avg mIoU:  72.10  ***

[Epoch: 114] [Batch: 0001/0050] Loss: 0.15979  Avg Loss: 0.15979  Avg mIoU:  62.99  

*** Validation [@Epoch 114] Avg Loss: 0.23941  Avg mIoU:  56.92  ***

[Epoch: 115] [Batch: 0001/0580] Loss: 0.18481  Avg Loss: 0.18481  Avg mIoU:  19.88  
[Epoch: 115] [Batch: 0051/0580] Loss: 0.12823  Avg Loss: 0.15758  Avg mIoU:  73.37  
[Epoch: 115] [Batch: 0101/0580] Loss: 0.18042  Avg Loss: 0.15907  Avg mIoU:  73.76  
[Epoch: 115] [Batch: 0151/0580] Loss: 0.17852  Avg Loss: 0.15909  Avg mIoU:  72.43  
[Epoch: 115] [Batch: 0201/0580] Loss: 0.18210  Avg Loss: 0.16395  Avg mIoU:  72.21  
[Epoch: 115] [Batch: 0251/0580] Loss: 0.10568  Avg Loss: 0.16317  Avg mIoU:  72.47  
[Epoch: 115] [Batch: 0301/0580] Loss: 0.23326  Avg Loss: 0.16348  Avg mIoU:  72.43  
[Epoch: 115] [Batch: 0351/0580] Loss: 0.26017  Avg Loss: 0.16228  Avg mIoU:  72.42  
[Epoch: 115] [Batch: 0401/0580] Loss: 0.17509  Avg Loss: 0.16301  Avg mIoU:  72.31  
[Epoch: 115] [Batch: 0451/0580] Loss: 0.14726  Avg Loss: 0.16304  Avg mIoU:  72.35  
[Epoch: 115] [Batch: 0501/0580] Loss: 0.14252  Avg Loss: 0.16328  Avg mIoU:  72.20  
[Epoch: 115] [Batch: 0551/0580] Loss: 0.15251  Avg Loss: 0.16394  Avg mIoU:  72.47  

*** Training [@Epoch 115] Avg Loss: 0.16429  Avg mIoU:  72.47  ***

[Epoch: 115] [Batch: 0001/0050] Loss: 0.15027  Avg Loss: 0.15027  Avg mIoU:  61.13  

*** Validation [@Epoch 115] Avg Loss: 0.24346  Avg mIoU:  56.59  ***

[Epoch: 116] [Batch: 0001/0580] Loss: 0.22121  Avg Loss: 0.22121  Avg mIoU:  35.12  
[Epoch: 116] [Batch: 0051/0580] Loss: 0.15323  Avg Loss: 0.15494  Avg mIoU:  72.05  
[Epoch: 116] [Batch: 0101/0580] Loss: 0.12963  Avg Loss: 0.15724  Avg mIoU:  72.23  
[Epoch: 116] [Batch: 0151/0580] Loss: 0.14434  Avg Loss: 0.16182  Avg mIoU:  71.99  
[Epoch: 116] [Batch: 0201/0580] Loss: 0.17257  Avg Loss: 0.16260  Avg mIoU:  71.99  
[Epoch: 116] [Batch: 0251/0580] Loss: 0.18499  Avg Loss: 0.16339  Avg mIoU:  71.72  
[Epoch: 116] [Batch: 0301/0580] Loss: 0.20042  Avg Loss: 0.16369  Avg mIoU:  71.83  
[Epoch: 116] [Batch: 0351/0580] Loss: 0.17453  Avg Loss: 0.16327  Avg mIoU:  72.20  
[Epoch: 116] [Batch: 0401/0580] Loss: 0.18025  Avg Loss: 0.16327  Avg mIoU:  72.19  
[Epoch: 116] [Batch: 0451/0580] Loss: 0.15940  Avg Loss: 0.16257  Avg mIoU:  72.36  
[Epoch: 116] [Batch: 0501/0580] Loss: 0.15782  Avg Loss: 0.16239  Avg mIoU:  72.52  
[Epoch: 116] [Batch: 0551/0580] Loss: 0.19267  Avg Loss: 0.16324  Avg mIoU:  72.31  

*** Training [@Epoch 116] Avg Loss: 0.16333  Avg mIoU:  72.42  ***

[Epoch: 116] [Batch: 0001/0050] Loss: 0.15243  Avg Loss: 0.15243  Avg mIoU:  63.83  

*** Validation [@Epoch 116] Avg Loss: 0.25324  Avg mIoU:  60.06  ***

[Epoch: 117] [Batch: 0001/0580] Loss: 0.12877  Avg Loss: 0.12877  Avg mIoU:  55.98  
[Epoch: 117] [Batch: 0051/0580] Loss: 0.14243  Avg Loss: 0.15926  Avg mIoU:  74.28  
[Epoch: 117] [Batch: 0101/0580] Loss: 0.18118  Avg Loss: 0.15664  Avg mIoU:  74.31  
[Epoch: 117] [Batch: 0151/0580] Loss: 0.26192  Avg Loss: 0.15940  Avg mIoU:  73.11  
[Epoch: 117] [Batch: 0201/0580] Loss: 0.14790  Avg Loss: 0.16228  Avg mIoU:  73.03  
[Epoch: 117] [Batch: 0251/0580] Loss: 0.17793  Avg Loss: 0.16297  Avg mIoU:  72.68  
[Epoch: 117] [Batch: 0301/0580] Loss: 0.14336  Avg Loss: 0.16261  Avg mIoU:  72.99  
[Epoch: 117] [Batch: 0351/0580] Loss: 0.14701  Avg Loss: 0.16331  Avg mIoU:  72.76  
[Epoch: 117] [Batch: 0401/0580] Loss: 0.16054  Avg Loss: 0.16321  Avg mIoU:  72.62  
[Epoch: 117] [Batch: 0451/0580] Loss: 0.09179  Avg Loss: 0.16263  Avg mIoU:  72.75  
[Epoch: 117] [Batch: 0501/0580] Loss: 0.12620  Avg Loss: 0.16191  Avg mIoU:  72.62  
[Epoch: 117] [Batch: 0551/0580] Loss: 0.13498  Avg Loss: 0.16186  Avg mIoU:  72.59  

*** Training [@Epoch 117] Avg Loss: 0.16168  Avg mIoU:  72.62  ***

[Epoch: 117] [Batch: 0001/0050] Loss: 0.14877  Avg Loss: 0.14877  Avg mIoU:  63.64  

*** Validation [@Epoch 117] Avg Loss: 0.24285  Avg mIoU:  58.37  ***

[Epoch: 118] [Batch: 0001/0580] Loss: 0.11830  Avg Loss: 0.11830  Avg mIoU:  58.29  
[Epoch: 118] [Batch: 0051/0580] Loss: 0.12880  Avg Loss: 0.16566  Avg mIoU:  72.30  
[Epoch: 118] [Batch: 0101/0580] Loss: 0.22202  Avg Loss: 0.16368  Avg mIoU:  72.10  
[Epoch: 118] [Batch: 0151/0580] Loss: 0.14860  Avg Loss: 0.16136  Avg mIoU:  72.57  
[Epoch: 118] [Batch: 0201/0580] Loss: 0.13760  Avg Loss: 0.16197  Avg mIoU:  72.25  
[Epoch: 118] [Batch: 0251/0580] Loss: 0.19414  Avg Loss: 0.16297  Avg mIoU:  72.23  
[Epoch: 118] [Batch: 0301/0580] Loss: 0.16271  Avg Loss: 0.16194  Avg mIoU:  72.17  
[Epoch: 118] [Batch: 0351/0580] Loss: 0.17204  Avg Loss: 0.16238  Avg mIoU:  71.99  
[Epoch: 118] [Batch: 0401/0580] Loss: 0.17967  Avg Loss: 0.16187  Avg mIoU:  72.21  
[Epoch: 118] [Batch: 0451/0580] Loss: 0.20090  Avg Loss: 0.16262  Avg mIoU:  72.20  
[Epoch: 118] [Batch: 0501/0580] Loss: 0.18599  Avg Loss: 0.16235  Avg mIoU:  72.31  
[Epoch: 118] [Batch: 0551/0580] Loss: 0.16329  Avg Loss: 0.16193  Avg mIoU:  72.39  

*** Training [@Epoch 118] Avg Loss: 0.16158  Avg mIoU:  72.38  ***

[Epoch: 118] [Batch: 0001/0050] Loss: 0.15087  Avg Loss: 0.15087  Avg mIoU:  63.92  

*** Validation [@Epoch 118] Avg Loss: 0.25115  Avg mIoU:  60.71  ***

[Epoch: 119] [Batch: 0001/0580] Loss: 0.14239  Avg Loss: 0.14239  Avg mIoU:  26.42  
[Epoch: 119] [Batch: 0051/0580] Loss: 0.13692  Avg Loss: 0.15544  Avg mIoU:  74.34  
[Epoch: 119] [Batch: 0101/0580] Loss: 0.16381  Avg Loss: 0.15992  Avg mIoU:  73.05  
[Epoch: 119] [Batch: 0151/0580] Loss: 0.11020  Avg Loss: 0.15978  Avg mIoU:  72.79  
[Epoch: 119] [Batch: 0201/0580] Loss: 0.16822  Avg Loss: 0.16191  Avg mIoU:  72.63  
[Epoch: 119] [Batch: 0251/0580] Loss: 0.17095  Avg Loss: 0.16116  Avg mIoU:  72.54  
[Epoch: 119] [Batch: 0301/0580] Loss: 0.17195  Avg Loss: 0.16097  Avg mIoU:  72.80  
[Epoch: 119] [Batch: 0351/0580] Loss: 0.22569  Avg Loss: 0.16266  Avg mIoU:  72.76  
[Epoch: 119] [Batch: 0401/0580] Loss: 0.12880  Avg Loss: 0.16223  Avg mIoU:  72.98  
[Epoch: 119] [Batch: 0451/0580] Loss: 0.16890  Avg Loss: 0.16199  Avg mIoU:  72.92  
[Epoch: 119] [Batch: 0501/0580] Loss: 0.13702  Avg Loss: 0.16216  Avg mIoU:  72.94  
[Epoch: 119] [Batch: 0551/0580] Loss: 0.15493  Avg Loss: 0.16204  Avg mIoU:  72.89  

*** Training [@Epoch 119] Avg Loss: 0.16162  Avg mIoU:  72.94  ***

[Epoch: 119] [Batch: 0001/0050] Loss: 0.15544  Avg Loss: 0.15544  Avg mIoU:  61.46  

*** Validation [@Epoch 119] Avg Loss: 0.26134  Avg mIoU:  59.98  ***

[Epoch: 120] [Batch: 0001/0580] Loss: 0.17290  Avg Loss: 0.17290  Avg mIoU:  29.18  
[Epoch: 120] [Batch: 0051/0580] Loss: 0.13983  Avg Loss: 0.16004  Avg mIoU:  72.43  
[Epoch: 120] [Batch: 0101/0580] Loss: 0.16279  Avg Loss: 0.16405  Avg mIoU:  72.42  
[Epoch: 120] [Batch: 0151/0580] Loss: 0.17992  Avg Loss: 0.16190  Avg mIoU:  72.37  
[Epoch: 120] [Batch: 0201/0580] Loss: 0.14471  Avg Loss: 0.15962  Avg mIoU:  72.78  
[Epoch: 120] [Batch: 0251/0580] Loss: 0.19363  Avg Loss: 0.15957  Avg mIoU:  72.72  
[Epoch: 120] [Batch: 0301/0580] Loss: 0.18421  Avg Loss: 0.16003  Avg mIoU:  72.83  
[Epoch: 120] [Batch: 0351/0580] Loss: 0.21665  Avg Loss: 0.16050  Avg mIoU:  72.82  
[Epoch: 120] [Batch: 0401/0580] Loss: 0.13142  Avg Loss: 0.16087  Avg mIoU:  72.66  
[Epoch: 120] [Batch: 0451/0580] Loss: 0.18431  Avg Loss: 0.16114  Avg mIoU:  72.75  
[Epoch: 120] [Batch: 0501/0580] Loss: 0.18097  Avg Loss: 0.16130  Avg mIoU:  72.64  
[Epoch: 120] [Batch: 0551/0580] Loss: 0.16710  Avg Loss: 0.16110  Avg mIoU:  72.71  

*** Training [@Epoch 120] Avg Loss: 0.16170  Avg mIoU:  72.65  ***

[Epoch: 120] [Batch: 0001/0050] Loss: 0.16114  Avg Loss: 0.16114  Avg mIoU:  59.57  

*** Validation [@Epoch 120] Avg Loss: 0.24426  Avg mIoU:  59.69  ***

[Epoch: 121] [Batch: 0001/0580] Loss: 0.13500  Avg Loss: 0.13500  Avg mIoU:  38.94  
[Epoch: 121] [Batch: 0051/0580] Loss: 0.12022  Avg Loss: 0.15855  Avg mIoU:  72.45  
[Epoch: 121] [Batch: 0101/0580] Loss: 0.12796  Avg Loss: 0.15835  Avg mIoU:  72.99  
[Epoch: 121] [Batch: 0151/0580] Loss: 0.20548  Avg Loss: 0.15932  Avg mIoU:  72.94  
[Epoch: 121] [Batch: 0201/0580] Loss: 0.18898  Avg Loss: 0.16001  Avg mIoU:  72.98  
[Epoch: 121] [Batch: 0251/0580] Loss: 0.18372  Avg Loss: 0.16179  Avg mIoU:  72.83  
[Epoch: 121] [Batch: 0301/0580] Loss: 0.15060  Avg Loss: 0.16154  Avg mIoU:  72.85  
[Epoch: 121] [Batch: 0351/0580] Loss: 0.22031  Avg Loss: 0.16128  Avg mIoU:  72.82  
[Epoch: 121] [Batch: 0401/0580] Loss: 0.22646  Avg Loss: 0.16194  Avg mIoU:  72.65  
[Epoch: 121] [Batch: 0451/0580] Loss: 0.17463  Avg Loss: 0.16098  Avg mIoU:  72.69  
[Epoch: 121] [Batch: 0501/0580] Loss: 0.12517  Avg Loss: 0.16094  Avg mIoU:  72.68  
[Epoch: 121] [Batch: 0551/0580] Loss: 0.10581  Avg Loss: 0.16091  Avg mIoU:  72.76  

*** Training [@Epoch 121] Avg Loss: 0.16160  Avg mIoU:  72.78  ***

[Epoch: 121] [Batch: 0001/0050] Loss: 0.17619  Avg Loss: 0.17619  Avg mIoU:  49.25  

*** Validation [@Epoch 121] Avg Loss: 0.25571  Avg mIoU:  51.00  ***

[Epoch: 122] [Batch: 0001/0580] Loss: 0.16539  Avg Loss: 0.16539  Avg mIoU:  34.26  
[Epoch: 122] [Batch: 0051/0580] Loss: 0.16673  Avg Loss: 0.16484  Avg mIoU:  71.13  
[Epoch: 122] [Batch: 0101/0580] Loss: 0.16438  Avg Loss: 0.16389  Avg mIoU:  71.05  
[Epoch: 122] [Batch: 0151/0580] Loss: 0.13927  Avg Loss: 0.16164  Avg mIoU:  72.01  
[Epoch: 122] [Batch: 0201/0580] Loss: 0.12888  Avg Loss: 0.16198  Avg mIoU:  72.14  
[Epoch: 122] [Batch: 0251/0580] Loss: 0.14019  Avg Loss: 0.16123  Avg mIoU:  72.42  
[Epoch: 122] [Batch: 0301/0580] Loss: 0.17160  Avg Loss: 0.16073  Avg mIoU:  72.66  
[Epoch: 122] [Batch: 0351/0580] Loss: 0.13481  Avg Loss: 0.16185  Avg mIoU:  72.65  
[Epoch: 122] [Batch: 0401/0580] Loss: 0.18579  Avg Loss: 0.16143  Avg mIoU:  72.74  
[Epoch: 122] [Batch: 0451/0580] Loss: 0.11726  Avg Loss: 0.16112  Avg mIoU:  72.76  
[Epoch: 122] [Batch: 0501/0580] Loss: 0.20426  Avg Loss: 0.16067  Avg mIoU:  72.89  
[Epoch: 122] [Batch: 0551/0580] Loss: 0.16324  Avg Loss: 0.16053  Avg mIoU:  72.96  

*** Training [@Epoch 122] Avg Loss: 0.16058  Avg mIoU:  72.98  ***

[Epoch: 122] [Batch: 0001/0050] Loss: 0.16500  Avg Loss: 0.16500  Avg mIoU:  59.86  

*** Validation [@Epoch 122] Avg Loss: 0.25230  Avg mIoU:  58.21  ***

[Epoch: 123] [Batch: 0001/0580] Loss: 0.15470  Avg Loss: 0.15470  Avg mIoU:  46.52  
[Epoch: 123] [Batch: 0051/0580] Loss: 0.11049  Avg Loss: 0.15647  Avg mIoU:  72.59  
[Epoch: 123] [Batch: 0101/0580] Loss: 0.16963  Avg Loss: 0.15393  Avg mIoU:  73.21  
[Epoch: 123] [Batch: 0151/0580] Loss: 0.15617  Avg Loss: 0.15639  Avg mIoU:  72.90  
[Epoch: 123] [Batch: 0201/0580] Loss: 0.25920  Avg Loss: 0.15820  Avg mIoU:  72.57  
[Epoch: 123] [Batch: 0251/0580] Loss: 0.17383  Avg Loss: 0.15829  Avg mIoU:  72.72  
[Epoch: 123] [Batch: 0301/0580] Loss: 0.23058  Avg Loss: 0.16044  Avg mIoU:  72.45  
[Epoch: 123] [Batch: 0351/0580] Loss: 0.19832  Avg Loss: 0.16109  Avg mIoU:  72.67  
[Epoch: 123] [Batch: 0401/0580] Loss: 0.13667  Avg Loss: 0.16086  Avg mIoU:  72.63  
[Epoch: 123] [Batch: 0451/0580] Loss: 0.16301  Avg Loss: 0.16001  Avg mIoU:  72.69  
[Epoch: 123] [Batch: 0501/0580] Loss: 0.13124  Avg Loss: 0.15990  Avg mIoU:  72.78  
[Epoch: 123] [Batch: 0551/0580] Loss: 0.14922  Avg Loss: 0.16088  Avg mIoU:  72.74  

*** Training [@Epoch 123] Avg Loss: 0.16064  Avg mIoU:  72.81  ***

[Epoch: 123] [Batch: 0001/0050] Loss: 0.15321  Avg Loss: 0.15321  Avg mIoU:  63.80  

*** Validation [@Epoch 123] Avg Loss: 0.24492  Avg mIoU:  58.73  ***

[Epoch: 124] [Batch: 0001/0580] Loss: 0.16868  Avg Loss: 0.16868  Avg mIoU:  42.50  
[Epoch: 124] [Batch: 0051/0580] Loss: 0.16030  Avg Loss: 0.16649  Avg mIoU:  72.58  
[Epoch: 124] [Batch: 0101/0580] Loss: 0.21021  Avg Loss: 0.16350  Avg mIoU:  72.01  
[Epoch: 124] [Batch: 0151/0580] Loss: 0.15545  Avg Loss: 0.15922  Avg mIoU:  73.00  
[Epoch: 124] [Batch: 0201/0580] Loss: 0.23210  Avg Loss: 0.16097  Avg mIoU:  73.16  
[Epoch: 124] [Batch: 0251/0580] Loss: 0.16218  Avg Loss: 0.16146  Avg mIoU:  72.95  
[Epoch: 124] [Batch: 0301/0580] Loss: 0.13665  Avg Loss: 0.16147  Avg mIoU:  72.74  
[Epoch: 124] [Batch: 0351/0580] Loss: 0.16739  Avg Loss: 0.16148  Avg mIoU:  72.55  
[Epoch: 124] [Batch: 0401/0580] Loss: 0.20282  Avg Loss: 0.16247  Avg mIoU:  72.34  
[Epoch: 124] [Batch: 0451/0580] Loss: 0.12333  Avg Loss: 0.16297  Avg mIoU:  72.38  
[Epoch: 124] [Batch: 0501/0580] Loss: 0.16144  Avg Loss: 0.16199  Avg mIoU:  72.34  
[Epoch: 124] [Batch: 0551/0580] Loss: 0.12427  Avg Loss: 0.16146  Avg mIoU:  72.45  

*** Training [@Epoch 124] Avg Loss: 0.16145  Avg mIoU:  72.53  ***

[Epoch: 124] [Batch: 0001/0050] Loss: 0.17618  Avg Loss: 0.17618  Avg mIoU:  62.30  

*** Validation [@Epoch 124] Avg Loss: 0.24496  Avg mIoU:  61.40  ***

[Epoch: 125] [Batch: 0001/0580] Loss: 0.15296  Avg Loss: 0.15296  Avg mIoU:  50.66  
[Epoch: 125] [Batch: 0051/0580] Loss: 0.14164  Avg Loss: 0.14999  Avg mIoU:  74.75  
[Epoch: 125] [Batch: 0101/0580] Loss: 0.13412  Avg Loss: 0.15448  Avg mIoU:  73.99  
[Epoch: 125] [Batch: 0151/0580] Loss: 0.18281  Avg Loss: 0.15662  Avg mIoU:  73.79  
[Epoch: 125] [Batch: 0201/0580] Loss: 0.13918  Avg Loss: 0.15645  Avg mIoU:  73.97  
[Epoch: 125] [Batch: 0251/0580] Loss: 0.20114  Avg Loss: 0.15577  Avg mIoU:  73.60  
[Epoch: 125] [Batch: 0301/0580] Loss: 0.17428  Avg Loss: 0.15713  Avg mIoU:  73.38  
[Epoch: 125] [Batch: 0351/0580] Loss: 0.18442  Avg Loss: 0.15775  Avg mIoU:  73.40  
[Epoch: 125] [Batch: 0401/0580] Loss: 0.12259  Avg Loss: 0.15856  Avg mIoU:  73.14  
[Epoch: 125] [Batch: 0451/0580] Loss: 0.21992  Avg Loss: 0.15906  Avg mIoU:  73.03  
[Epoch: 125] [Batch: 0501/0580] Loss: 0.11431  Avg Loss: 0.16010  Avg mIoU:  72.96  
[Epoch: 125] [Batch: 0551/0580] Loss: 0.14659  Avg Loss: 0.15976  Avg mIoU:  72.97  

*** Training [@Epoch 125] Avg Loss: 0.15935  Avg mIoU:  73.01  ***

[Epoch: 125] [Batch: 0001/0050] Loss: 0.15409  Avg Loss: 0.15409  Avg mIoU:  63.12  

*** Validation [@Epoch 125] Avg Loss: 0.25728  Avg mIoU:  60.97  ***

[Epoch: 126] [Batch: 0001/0580] Loss: 0.11096  Avg Loss: 0.11096  Avg mIoU:  49.72  
[Epoch: 126] [Batch: 0051/0580] Loss: 0.10943  Avg Loss: 0.15916  Avg mIoU:  72.04  
[Epoch: 126] [Batch: 0101/0580] Loss: 0.17257  Avg Loss: 0.15663  Avg mIoU:  73.49  
[Epoch: 126] [Batch: 0151/0580] Loss: 0.17701  Avg Loss: 0.16019  Avg mIoU:  73.24  
[Epoch: 126] [Batch: 0201/0580] Loss: 0.26323  Avg Loss: 0.15963  Avg mIoU:  73.34  
[Epoch: 126] [Batch: 0251/0580] Loss: 0.16390  Avg Loss: 0.16126  Avg mIoU:  73.08  
[Epoch: 126] [Batch: 0301/0580] Loss: 0.14279  Avg Loss: 0.16016  Avg mIoU:  73.18  
[Epoch: 126] [Batch: 0351/0580] Loss: 0.12742  Avg Loss: 0.15951  Avg mIoU:  73.17  
[Epoch: 126] [Batch: 0401/0580] Loss: 0.20359  Avg Loss: 0.15960  Avg mIoU:  73.22  
[Epoch: 126] [Batch: 0451/0580] Loss: 0.12701  Avg Loss: 0.15911  Avg mIoU:  73.50  
[Epoch: 126] [Batch: 0501/0580] Loss: 0.22540  Avg Loss: 0.15937  Avg mIoU:  73.36  
[Epoch: 126] [Batch: 0551/0580] Loss: 0.24117  Avg Loss: 0.15958  Avg mIoU:  73.29  

*** Training [@Epoch 126] Avg Loss: 0.15997  Avg mIoU:  73.16  ***

[Epoch: 126] [Batch: 0001/0050] Loss: 0.16538  Avg Loss: 0.16538  Avg mIoU:  60.00  

*** Validation [@Epoch 126] Avg Loss: 0.23889  Avg mIoU:  60.90  ***

[Epoch: 127] [Batch: 0001/0580] Loss: 0.23228  Avg Loss: 0.23228  Avg mIoU:  26.69  
[Epoch: 127] [Batch: 0051/0580] Loss: 0.20199  Avg Loss: 0.15747  Avg mIoU:  73.41  
[Epoch: 127] [Batch: 0101/0580] Loss: 0.13472  Avg Loss: 0.15961  Avg mIoU:  73.00  
[Epoch: 127] [Batch: 0151/0580] Loss: 0.25569  Avg Loss: 0.16315  Avg mIoU:  72.52  
[Epoch: 127] [Batch: 0201/0580] Loss: 0.18701  Avg Loss: 0.16399  Avg mIoU:  72.60  
[Epoch: 127] [Batch: 0251/0580] Loss: 0.13380  Avg Loss: 0.16314  Avg mIoU:  72.72  
[Epoch: 127] [Batch: 0301/0580] Loss: 0.16402  Avg Loss: 0.16234  Avg mIoU:  72.82  
[Epoch: 127] [Batch: 0351/0580] Loss: 0.15137  Avg Loss: 0.16160  Avg mIoU:  72.55  
[Epoch: 127] [Batch: 0401/0580] Loss: 0.17950  Avg Loss: 0.16109  Avg mIoU:  72.67  
[Epoch: 127] [Batch: 0451/0580] Loss: 0.25299  Avg Loss: 0.16057  Avg mIoU:  72.63  
[Epoch: 127] [Batch: 0501/0580] Loss: 0.15743  Avg Loss: 0.16084  Avg mIoU:  72.55  
[Epoch: 127] [Batch: 0551/0580] Loss: 0.19084  Avg Loss: 0.16105  Avg mIoU:  72.64  

*** Training [@Epoch 127] Avg Loss: 0.16112  Avg mIoU:  72.56  ***

[Epoch: 127] [Batch: 0001/0050] Loss: 0.16874  Avg Loss: 0.16874  Avg mIoU:  62.56  

*** Validation [@Epoch 127] Avg Loss: 0.23934  Avg mIoU:  61.45  ***

[Epoch: 128] [Batch: 0001/0580] Loss: 0.19949  Avg Loss: 0.19949  Avg mIoU:  46.52  
[Epoch: 128] [Batch: 0051/0580] Loss: 0.14689  Avg Loss: 0.15932  Avg mIoU:  71.84  
[Epoch: 128] [Batch: 0101/0580] Loss: 0.13119  Avg Loss: 0.16133  Avg mIoU:  72.68  
[Epoch: 128] [Batch: 0151/0580] Loss: 0.15503  Avg Loss: 0.15939  Avg mIoU:  73.47  
[Epoch: 128] [Batch: 0201/0580] Loss: 0.17419  Avg Loss: 0.15974  Avg mIoU:  73.37  
[Epoch: 128] [Batch: 0251/0580] Loss: 0.12691  Avg Loss: 0.16032  Avg mIoU:  73.31  
[Epoch: 128] [Batch: 0301/0580] Loss: 0.17355  Avg Loss: 0.16022  Avg mIoU:  73.23  
[Epoch: 128] [Batch: 0351/0580] Loss: 0.19318  Avg Loss: 0.15901  Avg mIoU:  73.13  
[Epoch: 128] [Batch: 0401/0580] Loss: 0.19362  Avg Loss: 0.16055  Avg mIoU:  72.85  
[Epoch: 128] [Batch: 0451/0580] Loss: 0.14347  Avg Loss: 0.16002  Avg mIoU:  72.75  
[Epoch: 128] [Batch: 0501/0580] Loss: 0.15050  Avg Loss: 0.16123  Avg mIoU:  72.61  
[Epoch: 128] [Batch: 0551/0580] Loss: 0.19979  Avg Loss: 0.16133  Avg mIoU:  72.62  

*** Training [@Epoch 128] Avg Loss: 0.16169  Avg mIoU:  72.57  ***

[Epoch: 128] [Batch: 0001/0050] Loss: 0.17029  Avg Loss: 0.17029  Avg mIoU:  61.76  

*** Validation [@Epoch 128] Avg Loss: 0.23814  Avg mIoU:  61.53  ***

[Epoch: 129] [Batch: 0001/0580] Loss: 0.12628  Avg Loss: 0.12628  Avg mIoU:  33.57  
[Epoch: 129] [Batch: 0051/0580] Loss: 0.14547  Avg Loss: 0.15918  Avg mIoU:  72.80  
[Epoch: 129] [Batch: 0101/0580] Loss: 0.18907  Avg Loss: 0.16075  Avg mIoU:  72.78  
[Epoch: 129] [Batch: 0151/0580] Loss: 0.15381  Avg Loss: 0.15931  Avg mIoU:  72.93  
[Epoch: 129] [Batch: 0201/0580] Loss: 0.18410  Avg Loss: 0.15917  Avg mIoU:  73.29  
[Epoch: 129] [Batch: 0251/0580] Loss: 0.15251  Avg Loss: 0.15797  Avg mIoU:  72.92  
[Epoch: 129] [Batch: 0301/0580] Loss: 0.15548  Avg Loss: 0.15916  Avg mIoU:  72.87  
[Epoch: 129] [Batch: 0351/0580] Loss: 0.17745  Avg Loss: 0.15889  Avg mIoU:  73.07  
[Epoch: 129] [Batch: 0401/0580] Loss: 0.14259  Avg Loss: 0.15925  Avg mIoU:  73.25  
[Epoch: 129] [Batch: 0451/0580] Loss: 0.15366  Avg Loss: 0.15978  Avg mIoU:  73.34  
[Epoch: 129] [Batch: 0501/0580] Loss: 0.15325  Avg Loss: 0.16020  Avg mIoU:  73.23  
[Epoch: 129] [Batch: 0551/0580] Loss: 0.15805  Avg Loss: 0.15953  Avg mIoU:  73.04  

*** Training [@Epoch 129] Avg Loss: 0.15968  Avg mIoU:  72.99  ***

[Epoch: 129] [Batch: 0001/0050] Loss: 0.15030  Avg Loss: 0.15030  Avg mIoU:  60.85  

*** Validation [@Epoch 129] Avg Loss: 0.25351  Avg mIoU:  57.07  ***

[Epoch: 130] [Batch: 0001/0580] Loss: 0.18416  Avg Loss: 0.18416  Avg mIoU:  43.41  
[Epoch: 130] [Batch: 0051/0580] Loss: 0.15747  Avg Loss: 0.16022  Avg mIoU:  73.58  
[Epoch: 130] [Batch: 0101/0580] Loss: 0.19577  Avg Loss: 0.16347  Avg mIoU:  72.22  
[Epoch: 130] [Batch: 0151/0580] Loss: 0.13522  Avg Loss: 0.16196  Avg mIoU:  72.58  
[Epoch: 130] [Batch: 0201/0580] Loss: 0.14663  Avg Loss: 0.16109  Avg mIoU:  72.31  
[Epoch: 130] [Batch: 0251/0580] Loss: 0.13137  Avg Loss: 0.16195  Avg mIoU:  72.27  
[Epoch: 130] [Batch: 0301/0580] Loss: 0.17144  Avg Loss: 0.16358  Avg mIoU:  72.31  
[Epoch: 130] [Batch: 0351/0580] Loss: 0.11572  Avg Loss: 0.16226  Avg mIoU:  72.50  
[Epoch: 130] [Batch: 0401/0580] Loss: 0.12147  Avg Loss: 0.16145  Avg mIoU:  72.59  
[Epoch: 130] [Batch: 0451/0580] Loss: 0.12614  Avg Loss: 0.15987  Avg mIoU:  72.72  
[Epoch: 130] [Batch: 0501/0580] Loss: 0.14029  Avg Loss: 0.16041  Avg mIoU:  72.77  
[Epoch: 130] [Batch: 0551/0580] Loss: 0.17577  Avg Loss: 0.16032  Avg mIoU:  72.85  

*** Training [@Epoch 130] Avg Loss: 0.16014  Avg mIoU:  73.01  ***

[Epoch: 130] [Batch: 0001/0050] Loss: 0.15565  Avg Loss: 0.15565  Avg mIoU:  64.83  

*** Validation [@Epoch 130] Avg Loss: 0.25273  Avg mIoU:  62.99  ***

Model saved @130 w/ val. mIoU: 62.99.

[Epoch: 131] [Batch: 0001/0580] Loss: 0.17953  Avg Loss: 0.17953  Avg mIoU:  48.06  
[Epoch: 131] [Batch: 0051/0580] Loss: 0.20238  Avg Loss: 0.16413  Avg mIoU:  71.57  
[Epoch: 131] [Batch: 0101/0580] Loss: 0.13231  Avg Loss: 0.16405  Avg mIoU:  71.62  
[Epoch: 131] [Batch: 0151/0580] Loss: 0.14239  Avg Loss: 0.16063  Avg mIoU:  72.57  
[Epoch: 131] [Batch: 0201/0580] Loss: 0.19913  Avg Loss: 0.16387  Avg mIoU:  72.67  
[Epoch: 131] [Batch: 0251/0580] Loss: 0.21531  Avg Loss: 0.16239  Avg mIoU:  72.55  
[Epoch: 131] [Batch: 0301/0580] Loss: 0.27268  Avg Loss: 0.16053  Avg mIoU:  72.98  
[Epoch: 131] [Batch: 0351/0580] Loss: 0.14745  Avg Loss: 0.16041  Avg mIoU:  72.97  
[Epoch: 131] [Batch: 0401/0580] Loss: 0.14095  Avg Loss: 0.16058  Avg mIoU:  72.98  
[Epoch: 131] [Batch: 0451/0580] Loss: 0.15340  Avg Loss: 0.16025  Avg mIoU:  73.21  
[Epoch: 131] [Batch: 0501/0580] Loss: 0.16805  Avg Loss: 0.16096  Avg mIoU:  73.22  
[Epoch: 131] [Batch: 0551/0580] Loss: 0.16746  Avg Loss: 0.16036  Avg mIoU:  73.10  

*** Training [@Epoch 131] Avg Loss: 0.15984  Avg mIoU:  73.06  ***

[Epoch: 131] [Batch: 0001/0050] Loss: 0.15825  Avg Loss: 0.15825  Avg mIoU:  61.84  

*** Validation [@Epoch 131] Avg Loss: 0.23611  Avg mIoU:  61.77  ***

[Epoch: 132] [Batch: 0001/0580] Loss: 0.11279  Avg Loss: 0.11279  Avg mIoU:  41.14  
[Epoch: 132] [Batch: 0051/0580] Loss: 0.09487  Avg Loss: 0.16471  Avg mIoU:  73.68  
[Epoch: 132] [Batch: 0101/0580] Loss: 0.13232  Avg Loss: 0.16177  Avg mIoU:  73.89  
[Epoch: 132] [Batch: 0151/0580] Loss: 0.12572  Avg Loss: 0.15968  Avg mIoU:  73.81  
[Epoch: 132] [Batch: 0201/0580] Loss: 0.14567  Avg Loss: 0.16121  Avg mIoU:  73.56  
[Epoch: 132] [Batch: 0251/0580] Loss: 0.14566  Avg Loss: 0.16002  Avg mIoU:  73.37  
[Epoch: 132] [Batch: 0301/0580] Loss: 0.17643  Avg Loss: 0.16028  Avg mIoU:  72.97  
[Epoch: 132] [Batch: 0351/0580] Loss: 0.14477  Avg Loss: 0.15949  Avg mIoU:  72.92  
[Epoch: 132] [Batch: 0401/0580] Loss: 0.26671  Avg Loss: 0.15936  Avg mIoU:  72.75  
[Epoch: 132] [Batch: 0451/0580] Loss: 0.15475  Avg Loss: 0.15897  Avg mIoU:  72.79  
[Epoch: 132] [Batch: 0501/0580] Loss: 0.12594  Avg Loss: 0.15857  Avg mIoU:  72.76  
[Epoch: 132] [Batch: 0551/0580] Loss: 0.16053  Avg Loss: 0.15885  Avg mIoU:  72.78  

*** Training [@Epoch 132] Avg Loss: 0.15959  Avg mIoU:  72.76  ***

[Epoch: 132] [Batch: 0001/0050] Loss: 0.16176  Avg Loss: 0.16176  Avg mIoU:  56.43  

*** Validation [@Epoch 132] Avg Loss: 0.23748  Avg mIoU:  58.28  ***

[Epoch: 133] [Batch: 0001/0580] Loss: 0.25469  Avg Loss: 0.25469  Avg mIoU:  29.00  
[Epoch: 133] [Batch: 0051/0580] Loss: 0.17996  Avg Loss: 0.15636  Avg mIoU:  72.73  
[Epoch: 133] [Batch: 0101/0580] Loss: 0.20413  Avg Loss: 0.15082  Avg mIoU:  73.50  
[Epoch: 133] [Batch: 0151/0580] Loss: 0.17837  Avg Loss: 0.15587  Avg mIoU:  73.54  
[Epoch: 133] [Batch: 0201/0580] Loss: 0.18585  Avg Loss: 0.15760  Avg mIoU:  72.73  
[Epoch: 133] [Batch: 0251/0580] Loss: 0.18071  Avg Loss: 0.15628  Avg mIoU:  72.74  
[Epoch: 133] [Batch: 0301/0580] Loss: 0.18745  Avg Loss: 0.15659  Avg mIoU:  72.73  
[Epoch: 133] [Batch: 0351/0580] Loss: 0.17898  Avg Loss: 0.15932  Avg mIoU:  72.75  
[Epoch: 133] [Batch: 0401/0580] Loss: 0.10946  Avg Loss: 0.15833  Avg mIoU:  72.89  
[Epoch: 133] [Batch: 0451/0580] Loss: 0.15558  Avg Loss: 0.15855  Avg mIoU:  73.01  
[Epoch: 133] [Batch: 0501/0580] Loss: 0.19114  Avg Loss: 0.15846  Avg mIoU:  73.00  
[Epoch: 133] [Batch: 0551/0580] Loss: 0.13829  Avg Loss: 0.15866  Avg mIoU:  73.06  

*** Training [@Epoch 133] Avg Loss: 0.15873  Avg mIoU:  73.14  ***

[Epoch: 133] [Batch: 0001/0050] Loss: 0.16578  Avg Loss: 0.16578  Avg mIoU:  55.27  

*** Validation [@Epoch 133] Avg Loss: 0.25273  Avg mIoU:  55.20  ***

[Epoch: 134] [Batch: 0001/0580] Loss: 0.15330  Avg Loss: 0.15330  Avg mIoU:  34.12  
[Epoch: 134] [Batch: 0051/0580] Loss: 0.16781  Avg Loss: 0.16362  Avg mIoU:  72.22  
[Epoch: 134] [Batch: 0101/0580] Loss: 0.19786  Avg Loss: 0.15828  Avg mIoU:  72.46  
[Epoch: 134] [Batch: 0151/0580] Loss: 0.16311  Avg Loss: 0.15848  Avg mIoU:  72.41  
[Epoch: 134] [Batch: 0201/0580] Loss: 0.13693  Avg Loss: 0.15883  Avg mIoU:  72.86  
[Epoch: 134] [Batch: 0251/0580] Loss: 0.17510  Avg Loss: 0.15936  Avg mIoU:  72.86  
[Epoch: 134] [Batch: 0301/0580] Loss: 0.20342  Avg Loss: 0.15838  Avg mIoU:  73.07  
[Epoch: 134] [Batch: 0351/0580] Loss: 0.14274  Avg Loss: 0.15919  Avg mIoU:  73.10  
[Epoch: 134] [Batch: 0401/0580] Loss: 0.12165  Avg Loss: 0.15824  Avg mIoU:  73.20  
[Epoch: 134] [Batch: 0451/0580] Loss: 0.17419  Avg Loss: 0.15821  Avg mIoU:  73.30  
[Epoch: 134] [Batch: 0501/0580] Loss: 0.20367  Avg Loss: 0.15889  Avg mIoU:  73.07  
[Epoch: 134] [Batch: 0551/0580] Loss: 0.15792  Avg Loss: 0.15965  Avg mIoU:  72.93  

*** Training [@Epoch 134] Avg Loss: 0.15951  Avg mIoU:  72.85  ***

[Epoch: 134] [Batch: 0001/0050] Loss: 0.18754  Avg Loss: 0.18754  Avg mIoU:  60.34  

*** Validation [@Epoch 134] Avg Loss: 0.24926  Avg mIoU:  61.82  ***

[Epoch: 135] [Batch: 0001/0580] Loss: 0.14823  Avg Loss: 0.14823  Avg mIoU:  47.88  
[Epoch: 135] [Batch: 0051/0580] Loss: 0.15571  Avg Loss: 0.15678  Avg mIoU:  72.47  
[Epoch: 135] [Batch: 0101/0580] Loss: 0.16601  Avg Loss: 0.15624  Avg mIoU:  72.68  
[Epoch: 135] [Batch: 0151/0580] Loss: 0.21725  Avg Loss: 0.15617  Avg mIoU:  73.14  
[Epoch: 135] [Batch: 0201/0580] Loss: 0.09864  Avg Loss: 0.15517  Avg mIoU:  73.16  
[Epoch: 135] [Batch: 0251/0580] Loss: 0.17904  Avg Loss: 0.15613  Avg mIoU:  73.04  
[Epoch: 135] [Batch: 0301/0580] Loss: 0.13015  Avg Loss: 0.15626  Avg mIoU:  73.11  
[Epoch: 135] [Batch: 0351/0580] Loss: 0.10556  Avg Loss: 0.15719  Avg mIoU:  73.25  
[Epoch: 135] [Batch: 0401/0580] Loss: 0.17989  Avg Loss: 0.15629  Avg mIoU:  73.44  
[Epoch: 135] [Batch: 0451/0580] Loss: 0.16159  Avg Loss: 0.15644  Avg mIoU:  73.26  
[Epoch: 135] [Batch: 0501/0580] Loss: 0.21870  Avg Loss: 0.15655  Avg mIoU:  73.45  
[Epoch: 135] [Batch: 0551/0580] Loss: 0.11994  Avg Loss: 0.15676  Avg mIoU:  73.51  

*** Training [@Epoch 135] Avg Loss: 0.15662  Avg mIoU:  73.44  ***

[Epoch: 135] [Batch: 0001/0050] Loss: 0.17186  Avg Loss: 0.17186  Avg mIoU:  60.81  

*** Validation [@Epoch 135] Avg Loss: 0.25902  Avg mIoU:  62.41  ***

[Epoch: 136] [Batch: 0001/0580] Loss: 0.17912  Avg Loss: 0.17912  Avg mIoU:  66.34  
[Epoch: 136] [Batch: 0051/0580] Loss: 0.21993  Avg Loss: 0.15874  Avg mIoU:  73.66  
[Epoch: 136] [Batch: 0101/0580] Loss: 0.17360  Avg Loss: 0.16133  Avg mIoU:  73.52  
[Epoch: 136] [Batch: 0151/0580] Loss: 0.15433  Avg Loss: 0.16007  Avg mIoU:  73.40  
[Epoch: 136] [Batch: 0201/0580] Loss: 0.14567  Avg Loss: 0.15972  Avg mIoU:  73.47  
[Epoch: 136] [Batch: 0251/0580] Loss: 0.14083  Avg Loss: 0.15797  Avg mIoU:  73.64  
[Epoch: 136] [Batch: 0301/0580] Loss: 0.16233  Avg Loss: 0.15910  Avg mIoU:  73.68  
[Epoch: 136] [Batch: 0351/0580] Loss: 0.21131  Avg Loss: 0.15971  Avg mIoU:  73.81  
[Epoch: 136] [Batch: 0401/0580] Loss: 0.12418  Avg Loss: 0.15881  Avg mIoU:  73.72  
[Epoch: 136] [Batch: 0451/0580] Loss: 0.18834  Avg Loss: 0.15846  Avg mIoU:  73.61  
[Epoch: 136] [Batch: 0501/0580] Loss: 0.13294  Avg Loss: 0.15800  Avg mIoU:  73.52  
[Epoch: 136] [Batch: 0551/0580] Loss: 0.16697  Avg Loss: 0.15762  Avg mIoU:  73.64  

*** Training [@Epoch 136] Avg Loss: 0.15756  Avg mIoU:  73.59  ***

[Epoch: 136] [Batch: 0001/0050] Loss: 0.16083  Avg Loss: 0.16083  Avg mIoU:  62.13  

*** Validation [@Epoch 136] Avg Loss: 0.24998  Avg mIoU:  60.63  ***

[Epoch: 137] [Batch: 0001/0580] Loss: 0.12788  Avg Loss: 0.12788  Avg mIoU:  47.75  
[Epoch: 137] [Batch: 0051/0580] Loss: 0.14526  Avg Loss: 0.15338  Avg mIoU:  74.88  
[Epoch: 137] [Batch: 0101/0580] Loss: 0.15616  Avg Loss: 0.15475  Avg mIoU:  73.85  
[Epoch: 137] [Batch: 0151/0580] Loss: 0.21476  Avg Loss: 0.15205  Avg mIoU:  73.97  
[Epoch: 137] [Batch: 0201/0580] Loss: 0.15893  Avg Loss: 0.15246  Avg mIoU:  74.53  
[Epoch: 137] [Batch: 0251/0580] Loss: 0.11904  Avg Loss: 0.15607  Avg mIoU:  74.11  
[Epoch: 137] [Batch: 0301/0580] Loss: 0.13992  Avg Loss: 0.15790  Avg mIoU:  73.72  
[Epoch: 137] [Batch: 0351/0580] Loss: 0.15723  Avg Loss: 0.15730  Avg mIoU:  73.93  
[Epoch: 137] [Batch: 0401/0580] Loss: 0.17534  Avg Loss: 0.15837  Avg mIoU:  73.68  
[Epoch: 137] [Batch: 0451/0580] Loss: 0.11871  Avg Loss: 0.15856  Avg mIoU:  73.32  
[Epoch: 137] [Batch: 0501/0580] Loss: 0.15054  Avg Loss: 0.15857  Avg mIoU:  73.47  
[Epoch: 137] [Batch: 0551/0580] Loss: 0.10669  Avg Loss: 0.15820  Avg mIoU:  73.37  

*** Training [@Epoch 137] Avg Loss: 0.15846  Avg mIoU:  73.27  ***

[Epoch: 137] [Batch: 0001/0050] Loss: 0.15125  Avg Loss: 0.15125  Avg mIoU:  62.38  

*** Validation [@Epoch 137] Avg Loss: 0.23860  Avg mIoU:  61.76  ***

[Epoch: 138] [Batch: 0001/0580] Loss: 0.20183  Avg Loss: 0.20183  Avg mIoU:  46.22  
[Epoch: 138] [Batch: 0051/0580] Loss: 0.15996  Avg Loss: 0.15690  Avg mIoU:  72.76  
[Epoch: 138] [Batch: 0101/0580] Loss: 0.13021  Avg Loss: 0.15515  Avg mIoU:  73.75  
[Epoch: 138] [Batch: 0151/0580] Loss: 0.10208  Avg Loss: 0.15501  Avg mIoU:  74.00  
[Epoch: 138] [Batch: 0201/0580] Loss: 0.19299  Avg Loss: 0.15521  Avg mIoU:  73.74  
[Epoch: 138] [Batch: 0251/0580] Loss: 0.15453  Avg Loss: 0.15538  Avg mIoU:  73.86  
[Epoch: 138] [Batch: 0301/0580] Loss: 0.17213  Avg Loss: 0.15647  Avg mIoU:  73.40  
[Epoch: 138] [Batch: 0351/0580] Loss: 0.12110  Avg Loss: 0.15654  Avg mIoU:  73.37  
[Epoch: 138] [Batch: 0401/0580] Loss: 0.10230  Avg Loss: 0.15618  Avg mIoU:  73.36  
[Epoch: 138] [Batch: 0451/0580] Loss: 0.16722  Avg Loss: 0.15782  Avg mIoU:  73.17  
[Epoch: 138] [Batch: 0501/0580] Loss: 0.09024  Avg Loss: 0.15740  Avg mIoU:  73.18  
[Epoch: 138] [Batch: 0551/0580] Loss: 0.13631  Avg Loss: 0.15692  Avg mIoU:  73.28  

*** Training [@Epoch 138] Avg Loss: 0.15708  Avg mIoU:  73.41  ***

[Epoch: 138] [Batch: 0001/0050] Loss: 0.16741  Avg Loss: 0.16741  Avg mIoU:  62.71  

*** Validation [@Epoch 138] Avg Loss: 0.24299  Avg mIoU:  62.18  ***

[Epoch: 139] [Batch: 0001/0580] Loss: 0.14260  Avg Loss: 0.14260  Avg mIoU:  39.76  
[Epoch: 139] [Batch: 0051/0580] Loss: 0.16393  Avg Loss: 0.16047  Avg mIoU:  73.12  
[Epoch: 139] [Batch: 0101/0580] Loss: 0.14294  Avg Loss: 0.15506  Avg mIoU:  74.12  
[Epoch: 139] [Batch: 0151/0580] Loss: 0.13307  Avg Loss: 0.15556  Avg mIoU:  73.50  
[Epoch: 139] [Batch: 0201/0580] Loss: 0.12606  Avg Loss: 0.15591  Avg mIoU:  73.32  
[Epoch: 139] [Batch: 0251/0580] Loss: 0.14704  Avg Loss: 0.15672  Avg mIoU:  73.09  
[Epoch: 139] [Batch: 0301/0580] Loss: 0.20196  Avg Loss: 0.15730  Avg mIoU:  72.90  
[Epoch: 139] [Batch: 0351/0580] Loss: 0.11874  Avg Loss: 0.15623  Avg mIoU:  73.02  
[Epoch: 139] [Batch: 0401/0580] Loss: 0.11997  Avg Loss: 0.15702  Avg mIoU:  73.22  
[Epoch: 139] [Batch: 0451/0580] Loss: 0.09817  Avg Loss: 0.15615  Avg mIoU:  73.38  
[Epoch: 139] [Batch: 0501/0580] Loss: 0.13742  Avg Loss: 0.15613  Avg mIoU:  73.52  
[Epoch: 139] [Batch: 0551/0580] Loss: 0.19301  Avg Loss: 0.15605  Avg mIoU:  73.41  

*** Training [@Epoch 139] Avg Loss: 0.15646  Avg mIoU:  73.51  ***

[Epoch: 139] [Batch: 0001/0050] Loss: 0.16785  Avg Loss: 0.16785  Avg mIoU:  59.20  

*** Validation [@Epoch 139] Avg Loss: 0.24248  Avg mIoU:  59.46  ***

[Epoch: 140] [Batch: 0001/0580] Loss: 0.15490  Avg Loss: 0.15490  Avg mIoU:  52.15  
[Epoch: 140] [Batch: 0051/0580] Loss: 0.11327  Avg Loss: 0.15294  Avg mIoU:  72.26  
[Epoch: 140] [Batch: 0101/0580] Loss: 0.13157  Avg Loss: 0.15185  Avg mIoU:  72.76  
[Epoch: 140] [Batch: 0151/0580] Loss: 0.19566  Avg Loss: 0.15326  Avg mIoU:  73.04  
[Epoch: 140] [Batch: 0201/0580] Loss: 0.14249  Avg Loss: 0.15389  Avg mIoU:  72.83  
[Epoch: 140] [Batch: 0251/0580] Loss: 0.10804  Avg Loss: 0.15200  Avg mIoU:  73.07  
[Epoch: 140] [Batch: 0301/0580] Loss: 0.14142  Avg Loss: 0.15387  Avg mIoU:  72.78  
[Epoch: 140] [Batch: 0351/0580] Loss: 0.16756  Avg Loss: 0.15426  Avg mIoU:  72.88  
[Epoch: 140] [Batch: 0401/0580] Loss: 0.22612  Avg Loss: 0.15598  Avg mIoU:  73.02  
[Epoch: 140] [Batch: 0451/0580] Loss: 0.21697  Avg Loss: 0.15561  Avg mIoU:  73.31  
[Epoch: 140] [Batch: 0501/0580] Loss: 0.20365  Avg Loss: 0.15650  Avg mIoU:  73.33  
[Epoch: 140] [Batch: 0551/0580] Loss: 0.12417  Avg Loss: 0.15631  Avg mIoU:  73.46  

*** Training [@Epoch 140] Avg Loss: 0.15619  Avg mIoU:  73.58  ***

[Epoch: 140] [Batch: 0001/0050] Loss: 0.15588  Avg Loss: 0.15588  Avg mIoU:  62.23  

*** Validation [@Epoch 140] Avg Loss: 0.24015  Avg mIoU:  60.40  ***

[Epoch: 141] [Batch: 0001/0580] Loss: 0.13838  Avg Loss: 0.13838  Avg mIoU:  33.70  
[Epoch: 141] [Batch: 0051/0580] Loss: 0.18326  Avg Loss: 0.15193  Avg mIoU:  74.38  
[Epoch: 141] [Batch: 0101/0580] Loss: 0.10093  Avg Loss: 0.15279  Avg mIoU:  74.73  
[Epoch: 141] [Batch: 0151/0580] Loss: 0.17686  Avg Loss: 0.15526  Avg mIoU:  74.30  
[Epoch: 141] [Batch: 0201/0580] Loss: 0.12620  Avg Loss: 0.15877  Avg mIoU:  73.52  
[Epoch: 141] [Batch: 0251/0580] Loss: 0.12844  Avg Loss: 0.16035  Avg mIoU:  73.25  
[Epoch: 141] [Batch: 0301/0580] Loss: 0.12694  Avg Loss: 0.15977  Avg mIoU:  73.18  
[Epoch: 141] [Batch: 0351/0580] Loss: 0.14265  Avg Loss: 0.15806  Avg mIoU:  73.38  
[Epoch: 141] [Batch: 0401/0580] Loss: 0.15260  Avg Loss: 0.15828  Avg mIoU:  73.22  
[Epoch: 141] [Batch: 0451/0580] Loss: 0.20774  Avg Loss: 0.15839  Avg mIoU:  73.10  
[Epoch: 141] [Batch: 0501/0580] Loss: 0.17803  Avg Loss: 0.15828  Avg mIoU:  73.09  
[Epoch: 141] [Batch: 0551/0580] Loss: 0.11891  Avg Loss: 0.15779  Avg mIoU:  73.10  

*** Training [@Epoch 141] Avg Loss: 0.15781  Avg mIoU:  73.20  ***

[Epoch: 141] [Batch: 0001/0050] Loss: 0.15686  Avg Loss: 0.15686  Avg mIoU:  61.61  

*** Validation [@Epoch 141] Avg Loss: 0.23878  Avg mIoU:  59.07  ***

[Epoch: 142] [Batch: 0001/0580] Loss: 0.17418  Avg Loss: 0.17418  Avg mIoU:  51.63  
[Epoch: 142] [Batch: 0051/0580] Loss: 0.17466  Avg Loss: 0.15689  Avg mIoU:  74.34  
[Epoch: 142] [Batch: 0101/0580] Loss: 0.21876  Avg Loss: 0.15311  Avg mIoU:  73.87  
[Epoch: 142] [Batch: 0151/0580] Loss: 0.15710  Avg Loss: 0.15376  Avg mIoU:  73.77  
[Epoch: 142] [Batch: 0201/0580] Loss: 0.17583  Avg Loss: 0.15309  Avg mIoU:  73.94  
[Epoch: 142] [Batch: 0251/0580] Loss: 0.15457  Avg Loss: 0.15579  Avg mIoU:  73.68  
[Epoch: 142] [Batch: 0301/0580] Loss: 0.17608  Avg Loss: 0.15582  Avg mIoU:  73.76  
[Epoch: 142] [Batch: 0351/0580] Loss: 0.25704  Avg Loss: 0.15558  Avg mIoU:  73.68  
[Epoch: 142] [Batch: 0401/0580] Loss: 0.16998  Avg Loss: 0.15523  Avg mIoU:  73.88  
[Epoch: 142] [Batch: 0451/0580] Loss: 0.20457  Avg Loss: 0.15577  Avg mIoU:  73.85  
[Epoch: 142] [Batch: 0501/0580] Loss: 0.17047  Avg Loss: 0.15605  Avg mIoU:  73.68  
[Epoch: 142] [Batch: 0551/0580] Loss: 0.13147  Avg Loss: 0.15579  Avg mIoU:  73.61  

*** Training [@Epoch 142] Avg Loss: 0.15619  Avg mIoU:  73.52  ***

[Epoch: 142] [Batch: 0001/0050] Loss: 0.15706  Avg Loss: 0.15706  Avg mIoU:  55.38  

*** Validation [@Epoch 142] Avg Loss: 0.24281  Avg mIoU:  53.36  ***

[Epoch: 143] [Batch: 0001/0580] Loss: 0.12716  Avg Loss: 0.12716  Avg mIoU:  49.90  
[Epoch: 143] [Batch: 0051/0580] Loss: 0.13302  Avg Loss: 0.15047  Avg mIoU:  76.89  
[Epoch: 143] [Batch: 0101/0580] Loss: 0.09613  Avg Loss: 0.14954  Avg mIoU:  75.77  
[Epoch: 143] [Batch: 0151/0580] Loss: 0.13135  Avg Loss: 0.15205  Avg mIoU:  74.94  
[Epoch: 143] [Batch: 0201/0580] Loss: 0.21649  Avg Loss: 0.15277  Avg mIoU:  74.60  
[Epoch: 143] [Batch: 0251/0580] Loss: 0.18148  Avg Loss: 0.15453  Avg mIoU:  74.42  
[Epoch: 143] [Batch: 0301/0580] Loss: 0.20465  Avg Loss: 0.15384  Avg mIoU:  74.34  
[Epoch: 143] [Batch: 0351/0580] Loss: 0.12272  Avg Loss: 0.15321  Avg mIoU:  74.57  
[Epoch: 143] [Batch: 0401/0580] Loss: 0.15036  Avg Loss: 0.15335  Avg mIoU:  74.49  
[Epoch: 143] [Batch: 0451/0580] Loss: 0.18977  Avg Loss: 0.15441  Avg mIoU:  74.26  
[Epoch: 143] [Batch: 0501/0580] Loss: 0.18927  Avg Loss: 0.15398  Avg mIoU:  74.26  
[Epoch: 143] [Batch: 0551/0580] Loss: 0.18979  Avg Loss: 0.15439  Avg mIoU:  74.10  

*** Training [@Epoch 143] Avg Loss: 0.15469  Avg mIoU:  74.12  ***

[Epoch: 143] [Batch: 0001/0050] Loss: 0.16631  Avg Loss: 0.16631  Avg mIoU:  58.90  

*** Validation [@Epoch 143] Avg Loss: 0.24438  Avg mIoU:  60.48  ***

[Epoch: 144] [Batch: 0001/0580] Loss: 0.16192  Avg Loss: 0.16192  Avg mIoU:  35.22  
[Epoch: 144] [Batch: 0051/0580] Loss: 0.09002  Avg Loss: 0.15222  Avg mIoU:  74.52  
[Epoch: 144] [Batch: 0101/0580] Loss: 0.09307  Avg Loss: 0.15505  Avg mIoU:  74.10  
[Epoch: 144] [Batch: 0151/0580] Loss: 0.17896  Avg Loss: 0.15247  Avg mIoU:  73.96  
[Epoch: 144] [Batch: 0201/0580] Loss: 0.13941  Avg Loss: 0.15259  Avg mIoU:  74.48  
[Epoch: 144] [Batch: 0251/0580] Loss: 0.18407  Avg Loss: 0.15087  Avg mIoU:  74.64  
[Epoch: 144] [Batch: 0301/0580] Loss: 0.20341  Avg Loss: 0.15178  Avg mIoU:  74.65  
[Epoch: 144] [Batch: 0351/0580] Loss: 0.16258  Avg Loss: 0.15322  Avg mIoU:  74.15  
[Epoch: 144] [Batch: 0401/0580] Loss: 0.13344  Avg Loss: 0.15243  Avg mIoU:  74.36  
[Epoch: 144] [Batch: 0451/0580] Loss: 0.20754  Avg Loss: 0.15256  Avg mIoU:  74.21  
[Epoch: 144] [Batch: 0501/0580] Loss: 0.20861  Avg Loss: 0.15244  Avg mIoU:  74.25  
[Epoch: 144] [Batch: 0551/0580] Loss: 0.14837  Avg Loss: 0.15196  Avg mIoU:  74.27  

*** Training [@Epoch 144] Avg Loss: 0.15194  Avg mIoU:  74.27  ***

[Epoch: 144] [Batch: 0001/0050] Loss: 0.16972  Avg Loss: 0.16972  Avg mIoU:  59.65  

*** Validation [@Epoch 144] Avg Loss: 0.26813  Avg mIoU:  57.56  ***

[Epoch: 145] [Batch: 0001/0580] Loss: 0.12698  Avg Loss: 0.12698  Avg mIoU:  62.72  
[Epoch: 145] [Batch: 0051/0580] Loss: 0.17373  Avg Loss: 0.14969  Avg mIoU:  74.68  
[Epoch: 145] [Batch: 0101/0580] Loss: 0.14909  Avg Loss: 0.15662  Avg mIoU:  73.96  
[Epoch: 145] [Batch: 0151/0580] Loss: 0.17319  Avg Loss: 0.15402  Avg mIoU:  74.01  
[Epoch: 145] [Batch: 0201/0580] Loss: 0.11904  Avg Loss: 0.15468  Avg mIoU:  73.85  
[Epoch: 145] [Batch: 0251/0580] Loss: 0.15296  Avg Loss: 0.15600  Avg mIoU:  73.62  
[Epoch: 145] [Batch: 0301/0580] Loss: 0.14988  Avg Loss: 0.15657  Avg mIoU:  73.73  
[Epoch: 145] [Batch: 0351/0580] Loss: 0.10667  Avg Loss: 0.15646  Avg mIoU:  73.51  
[Epoch: 145] [Batch: 0401/0580] Loss: 0.19763  Avg Loss: 0.15721  Avg mIoU:  73.50  
[Epoch: 145] [Batch: 0451/0580] Loss: 0.14599  Avg Loss: 0.15799  Avg mIoU:  73.37  
[Epoch: 145] [Batch: 0501/0580] Loss: 0.16749  Avg Loss: 0.15747  Avg mIoU:  73.40  
[Epoch: 145] [Batch: 0551/0580] Loss: 0.18398  Avg Loss: 0.15690  Avg mIoU:  73.52  

*** Training [@Epoch 145] Avg Loss: 0.15715  Avg mIoU:  73.54  ***

[Epoch: 145] [Batch: 0001/0050] Loss: 0.15775  Avg Loss: 0.15775  Avg mIoU:  57.88  

*** Validation [@Epoch 145] Avg Loss: 0.24381  Avg mIoU:  58.87  ***

[Epoch: 146] [Batch: 0001/0580] Loss: 0.13778  Avg Loss: 0.13778  Avg mIoU:  43.98  
[Epoch: 146] [Batch: 0051/0580] Loss: 0.14550  Avg Loss: 0.15684  Avg mIoU:  73.04  
[Epoch: 146] [Batch: 0101/0580] Loss: 0.11449  Avg Loss: 0.15907  Avg mIoU:  73.17  
[Epoch: 146] [Batch: 0151/0580] Loss: 0.19021  Avg Loss: 0.15937  Avg mIoU:  73.02  
[Epoch: 146] [Batch: 0201/0580] Loss: 0.20642  Avg Loss: 0.16096  Avg mIoU:  72.97  
[Epoch: 146] [Batch: 0251/0580] Loss: 0.20661  Avg Loss: 0.15953  Avg mIoU:  73.06  
[Epoch: 146] [Batch: 0301/0580] Loss: 0.21100  Avg Loss: 0.15969  Avg mIoU:  72.76  
[Epoch: 146] [Batch: 0351/0580] Loss: 0.15982  Avg Loss: 0.15928  Avg mIoU:  72.77  
[Epoch: 146] [Batch: 0401/0580] Loss: 0.13566  Avg Loss: 0.15885  Avg mIoU:  73.00  
[Epoch: 146] [Batch: 0451/0580] Loss: 0.14673  Avg Loss: 0.15785  Avg mIoU:  73.26  
[Epoch: 146] [Batch: 0501/0580] Loss: 0.18253  Avg Loss: 0.15755  Avg mIoU:  73.37  
[Epoch: 146] [Batch: 0551/0580] Loss: 0.12166  Avg Loss: 0.15690  Avg mIoU:  73.28  

*** Training [@Epoch 146] Avg Loss: 0.15706  Avg mIoU:  73.30  ***

[Epoch: 146] [Batch: 0001/0050] Loss: 0.17513  Avg Loss: 0.17513  Avg mIoU:  57.85  

*** Validation [@Epoch 146] Avg Loss: 0.25115  Avg mIoU:  60.53  ***

[Epoch: 147] [Batch: 0001/0580] Loss: 0.16045  Avg Loss: 0.16045  Avg mIoU:  47.72  
[Epoch: 147] [Batch: 0051/0580] Loss: 0.14706  Avg Loss: 0.14572  Avg mIoU:  74.99  
[Epoch: 147] [Batch: 0101/0580] Loss: 0.13077  Avg Loss: 0.14797  Avg mIoU:  75.23  
[Epoch: 147] [Batch: 0151/0580] Loss: 0.14897  Avg Loss: 0.14967  Avg mIoU:  74.51  
[Epoch: 147] [Batch: 0201/0580] Loss: 0.19091  Avg Loss: 0.15112  Avg mIoU:  74.31  
[Epoch: 147] [Batch: 0251/0580] Loss: 0.15327  Avg Loss: 0.15171  Avg mIoU:  74.14  
[Epoch: 147] [Batch: 0301/0580] Loss: 0.13536  Avg Loss: 0.15239  Avg mIoU:  74.10  
[Epoch: 147] [Batch: 0351/0580] Loss: 0.14795  Avg Loss: 0.15268  Avg mIoU:  74.10  
[Epoch: 147] [Batch: 0401/0580] Loss: 0.12350  Avg Loss: 0.15491  Avg mIoU:  73.91  
[Epoch: 147] [Batch: 0451/0580] Loss: 0.14196  Avg Loss: 0.15421  Avg mIoU:  73.96  
[Epoch: 147] [Batch: 0501/0580] Loss: 0.15712  Avg Loss: 0.15444  Avg mIoU:  73.76  
[Epoch: 147] [Batch: 0551/0580] Loss: 0.18683  Avg Loss: 0.15530  Avg mIoU:  73.66  

*** Training [@Epoch 147] Avg Loss: 0.15526  Avg mIoU:  73.79  ***

[Epoch: 147] [Batch: 0001/0050] Loss: 0.16254  Avg Loss: 0.16254  Avg mIoU:  59.59  

*** Validation [@Epoch 147] Avg Loss: 0.23986  Avg mIoU:  58.28  ***

[Epoch: 148] [Batch: 0001/0580] Loss: 0.16625  Avg Loss: 0.16625  Avg mIoU:  45.81  
[Epoch: 148] [Batch: 0051/0580] Loss: 0.10797  Avg Loss: 0.15990  Avg mIoU:  74.20  
[Epoch: 148] [Batch: 0101/0580] Loss: 0.14357  Avg Loss: 0.15541  Avg mIoU:  73.47  
[Epoch: 148] [Batch: 0151/0580] Loss: 0.20114  Avg Loss: 0.15395  Avg mIoU:  73.20  
[Epoch: 148] [Batch: 0201/0580] Loss: 0.19388  Avg Loss: 0.15539  Avg mIoU:  73.15  
[Epoch: 148] [Batch: 0251/0580] Loss: 0.13490  Avg Loss: 0.15566  Avg mIoU:  73.09  
[Epoch: 148] [Batch: 0301/0580] Loss: 0.11014  Avg Loss: 0.15637  Avg mIoU:  72.92  
[Epoch: 148] [Batch: 0351/0580] Loss: 0.15952  Avg Loss: 0.15704  Avg mIoU:  72.98  
[Epoch: 148] [Batch: 0401/0580] Loss: 0.17382  Avg Loss: 0.15699  Avg mIoU:  72.97  
[Epoch: 148] [Batch: 0451/0580] Loss: 0.21204  Avg Loss: 0.15713  Avg mIoU:  73.13  
[Epoch: 148] [Batch: 0501/0580] Loss: 0.15687  Avg Loss: 0.15747  Avg mIoU:  73.11  
[Epoch: 148] [Batch: 0551/0580] Loss: 0.11331  Avg Loss: 0.15702  Avg mIoU:  73.36  

*** Training [@Epoch 148] Avg Loss: 0.15573  Avg mIoU:  73.47  ***

[Epoch: 148] [Batch: 0001/0050] Loss: 0.16116  Avg Loss: 0.16116  Avg mIoU:  61.74  

*** Validation [@Epoch 148] Avg Loss: 0.24231  Avg mIoU:  61.68  ***

[Epoch: 149] [Batch: 0001/0580] Loss: 0.20379  Avg Loss: 0.20379  Avg mIoU:  43.17  
[Epoch: 149] [Batch: 0051/0580] Loss: 0.10292  Avg Loss: 0.15144  Avg mIoU:  75.07  
[Epoch: 149] [Batch: 0101/0580] Loss: 0.13348  Avg Loss: 0.15248  Avg mIoU:  74.12  
[Epoch: 149] [Batch: 0151/0580] Loss: 0.17428  Avg Loss: 0.15075  Avg mIoU:  74.00  
[Epoch: 149] [Batch: 0201/0580] Loss: 0.15226  Avg Loss: 0.15321  Avg mIoU:  73.85  
[Epoch: 149] [Batch: 0251/0580] Loss: 0.20500  Avg Loss: 0.15322  Avg mIoU:  73.65  
[Epoch: 149] [Batch: 0301/0580] Loss: 0.18869  Avg Loss: 0.15581  Avg mIoU:  73.67  
[Epoch: 149] [Batch: 0351/0580] Loss: 0.13193  Avg Loss: 0.15613  Avg mIoU:  73.66  
[Epoch: 149] [Batch: 0401/0580] Loss: 0.11842  Avg Loss: 0.15486  Avg mIoU:  73.68  
[Epoch: 149] [Batch: 0451/0580] Loss: 0.20377  Avg Loss: 0.15315  Avg mIoU:  73.99  
[Epoch: 149] [Batch: 0501/0580] Loss: 0.12056  Avg Loss: 0.15281  Avg mIoU:  73.99  
[Epoch: 149] [Batch: 0551/0580] Loss: 0.11483  Avg Loss: 0.15256  Avg mIoU:  74.01  

*** Training [@Epoch 149] Avg Loss: 0.15273  Avg mIoU:  73.99  ***

[Epoch: 149] [Batch: 0001/0050] Loss: 0.16062  Avg Loss: 0.16062  Avg mIoU:  59.52  

*** Validation [@Epoch 149] Avg Loss: 0.25178  Avg mIoU:  60.71  ***

[Epoch: 150] [Batch: 0001/0580] Loss: 0.14820  Avg Loss: 0.14820  Avg mIoU:  37.04  
[Epoch: 150] [Batch: 0051/0580] Loss: 0.14885  Avg Loss: 0.15105  Avg mIoU:  74.51  
[Epoch: 150] [Batch: 0101/0580] Loss: 0.14125  Avg Loss: 0.14821  Avg mIoU:  74.52  
[Epoch: 150] [Batch: 0151/0580] Loss: 0.18699  Avg Loss: 0.15025  Avg mIoU:  74.16  
[Epoch: 150] [Batch: 0201/0580] Loss: 0.12422  Avg Loss: 0.15002  Avg mIoU:  74.03  
[Epoch: 150] [Batch: 0251/0580] Loss: 0.15362  Avg Loss: 0.14942  Avg mIoU:  74.33  
[Epoch: 150] [Batch: 0301/0580] Loss: 0.13509  Avg Loss: 0.14956  Avg mIoU:  74.14  
[Epoch: 150] [Batch: 0351/0580] Loss: 0.12233  Avg Loss: 0.15211  Avg mIoU:  73.75  
[Epoch: 150] [Batch: 0401/0580] Loss: 0.17392  Avg Loss: 0.15254  Avg mIoU:  73.59  
[Epoch: 150] [Batch: 0451/0580] Loss: 0.17362  Avg Loss: 0.15304  Avg mIoU:  73.65  
[Epoch: 150] [Batch: 0501/0580] Loss: 0.14953  Avg Loss: 0.15318  Avg mIoU:  73.82  
[Epoch: 150] [Batch: 0551/0580] Loss: 0.14934  Avg Loss: 0.15335  Avg mIoU:  73.96  

*** Training [@Epoch 150] Avg Loss: 0.15357  Avg mIoU:  73.88  ***

[Epoch: 150] [Batch: 0001/0050] Loss: 0.16099  Avg Loss: 0.16099  Avg mIoU:  60.93  

*** Validation [@Epoch 150] Avg Loss: 0.24417  Avg mIoU:  60.23  ***

[Epoch: 151] [Batch: 0001/0580] Loss: 0.15980  Avg Loss: 0.15980  Avg mIoU:  44.62  
[Epoch: 151] [Batch: 0051/0580] Loss: 0.18526  Avg Loss: 0.14640  Avg mIoU:  75.33  
[Epoch: 151] [Batch: 0101/0580] Loss: 0.17118  Avg Loss: 0.15086  Avg mIoU:  73.82  
[Epoch: 151] [Batch: 0151/0580] Loss: 0.16239  Avg Loss: 0.15025  Avg mIoU:  74.14  
[Epoch: 151] [Batch: 0201/0580] Loss: 0.14325  Avg Loss: 0.15125  Avg mIoU:  74.32  
[Epoch: 151] [Batch: 0251/0580] Loss: 0.14619  Avg Loss: 0.15270  Avg mIoU:  74.17  
[Epoch: 151] [Batch: 0301/0580] Loss: 0.12784  Avg Loss: 0.15304  Avg mIoU:  73.86  
[Epoch: 151] [Batch: 0351/0580] Loss: 0.13695  Avg Loss: 0.15358  Avg mIoU:  73.87  
[Epoch: 151] [Batch: 0401/0580] Loss: 0.14830  Avg Loss: 0.15403  Avg mIoU:  73.86  
[Epoch: 151] [Batch: 0451/0580] Loss: 0.21373  Avg Loss: 0.15463  Avg mIoU:  73.98  
[Epoch: 151] [Batch: 0501/0580] Loss: 0.15598  Avg Loss: 0.15480  Avg mIoU:  73.90  
[Epoch: 151] [Batch: 0551/0580] Loss: 0.15170  Avg Loss: 0.15499  Avg mIoU:  73.83  

*** Training [@Epoch 151] Avg Loss: 0.15453  Avg mIoU:  73.89  ***

[Epoch: 151] [Batch: 0001/0050] Loss: 0.15488  Avg Loss: 0.15488  Avg mIoU:  63.53  

*** Validation [@Epoch 151] Avg Loss: 0.25125  Avg mIoU:  61.96  ***

[Epoch: 152] [Batch: 0001/0580] Loss: 0.15576  Avg Loss: 0.15576  Avg mIoU:  29.40  
[Epoch: 152] [Batch: 0051/0580] Loss: 0.20364  Avg Loss: 0.15519  Avg mIoU:  73.04  
[Epoch: 152] [Batch: 0101/0580] Loss: 0.11693  Avg Loss: 0.15025  Avg mIoU:  74.16  
[Epoch: 152] [Batch: 0151/0580] Loss: 0.13354  Avg Loss: 0.14991  Avg mIoU:  74.05  
[Epoch: 152] [Batch: 0201/0580] Loss: 0.15549  Avg Loss: 0.15127  Avg mIoU:  74.18  
[Epoch: 152] [Batch: 0251/0580] Loss: 0.12875  Avg Loss: 0.15195  Avg mIoU:  73.94  
[Epoch: 152] [Batch: 0301/0580] Loss: 0.18240  Avg Loss: 0.15384  Avg mIoU:  74.06  
[Epoch: 152] [Batch: 0351/0580] Loss: 0.10389  Avg Loss: 0.15225  Avg mIoU:  74.17  
[Epoch: 152] [Batch: 0401/0580] Loss: 0.17025  Avg Loss: 0.15246  Avg mIoU:  73.86  
[Epoch: 152] [Batch: 0451/0580] Loss: 0.15084  Avg Loss: 0.15299  Avg mIoU:  73.82  
[Epoch: 152] [Batch: 0501/0580] Loss: 0.18064  Avg Loss: 0.15364  Avg mIoU:  73.94  
[Epoch: 152] [Batch: 0551/0580] Loss: 0.11502  Avg Loss: 0.15393  Avg mIoU:  74.11  

*** Training [@Epoch 152] Avg Loss: 0.15360  Avg mIoU:  74.16  ***

[Epoch: 152] [Batch: 0001/0050] Loss: 0.19463  Avg Loss: 0.19463  Avg mIoU:  56.65  

*** Validation [@Epoch 152] Avg Loss: 0.26825  Avg mIoU:  59.59  ***

[Epoch: 153] [Batch: 0001/0580] Loss: 0.16815  Avg Loss: 0.16815  Avg mIoU:  52.77  
[Epoch: 153] [Batch: 0051/0580] Loss: 0.11930  Avg Loss: 0.15078  Avg mIoU:  72.57  
[Epoch: 153] [Batch: 0101/0580] Loss: 0.14388  Avg Loss: 0.15220  Avg mIoU:  73.01  
[Epoch: 153] [Batch: 0151/0580] Loss: 0.14217  Avg Loss: 0.15399  Avg mIoU:  73.02  
[Epoch: 153] [Batch: 0201/0580] Loss: 0.16404  Avg Loss: 0.15143  Avg mIoU:  73.52  
[Epoch: 153] [Batch: 0251/0580] Loss: 0.14056  Avg Loss: 0.15112  Avg mIoU:  73.77  
[Epoch: 153] [Batch: 0301/0580] Loss: 0.20411  Avg Loss: 0.15207  Avg mIoU:  73.60  
[Epoch: 153] [Batch: 0351/0580] Loss: 0.20625  Avg Loss: 0.15413  Avg mIoU:  73.73  
[Epoch: 153] [Batch: 0401/0580] Loss: 0.17467  Avg Loss: 0.15482  Avg mIoU:  73.80  
[Epoch: 153] [Batch: 0451/0580] Loss: 0.14787  Avg Loss: 0.15479  Avg mIoU:  73.69  
[Epoch: 153] [Batch: 0501/0580] Loss: 0.17143  Avg Loss: 0.15399  Avg mIoU:  73.83  
[Epoch: 153] [Batch: 0551/0580] Loss: 0.18977  Avg Loss: 0.15350  Avg mIoU:  73.89  

*** Training [@Epoch 153] Avg Loss: 0.15357  Avg mIoU:  73.82  ***

[Epoch: 153] [Batch: 0001/0050] Loss: 0.17344  Avg Loss: 0.17344  Avg mIoU:  62.34  

*** Validation [@Epoch 153] Avg Loss: 0.24878  Avg mIoU:  60.38  ***

[Epoch: 154] [Batch: 0001/0580] Loss: 0.12301  Avg Loss: 0.12301  Avg mIoU:  48.74  
[Epoch: 154] [Batch: 0051/0580] Loss: 0.13523  Avg Loss: 0.14267  Avg mIoU:  75.36  
[Epoch: 154] [Batch: 0101/0580] Loss: 0.16754  Avg Loss: 0.15110  Avg mIoU:  75.26  
[Epoch: 154] [Batch: 0151/0580] Loss: 0.15345  Avg Loss: 0.15239  Avg mIoU:  74.42  
[Epoch: 154] [Batch: 0201/0580] Loss: 0.09664  Avg Loss: 0.15070  Avg mIoU:  74.79  
[Epoch: 154] [Batch: 0251/0580] Loss: 0.16993  Avg Loss: 0.15079  Avg mIoU:  74.87  
[Epoch: 154] [Batch: 0301/0580] Loss: 0.21225  Avg Loss: 0.15249  Avg mIoU:  74.78  
[Epoch: 154] [Batch: 0351/0580] Loss: 0.16177  Avg Loss: 0.15323  Avg mIoU:  74.41  
[Epoch: 154] [Batch: 0401/0580] Loss: 0.14886  Avg Loss: 0.15362  Avg mIoU:  74.16  
[Epoch: 154] [Batch: 0451/0580] Loss: 0.14972  Avg Loss: 0.15329  Avg mIoU:  74.31  
[Epoch: 154] [Batch: 0501/0580] Loss: 0.13366  Avg Loss: 0.15285  Avg mIoU:  74.27  
[Epoch: 154] [Batch: 0551/0580] Loss: 0.10283  Avg Loss: 0.15344  Avg mIoU:  74.23  

*** Training [@Epoch 154] Avg Loss: 0.15295  Avg mIoU:  74.20  ***

[Epoch: 154] [Batch: 0001/0050] Loss: 0.16035  Avg Loss: 0.16035  Avg mIoU:  60.12  

*** Validation [@Epoch 154] Avg Loss: 0.23430  Avg mIoU:  59.93  ***

[Epoch: 155] [Batch: 0001/0580] Loss: 0.14199  Avg Loss: 0.14199  Avg mIoU:  44.91  
[Epoch: 155] [Batch: 0051/0580] Loss: 0.15615  Avg Loss: 0.16034  Avg mIoU:  73.50  
[Epoch: 155] [Batch: 0101/0580] Loss: 0.12901  Avg Loss: 0.15429  Avg mIoU:  74.19  
[Epoch: 155] [Batch: 0151/0580] Loss: 0.14341  Avg Loss: 0.15338  Avg mIoU:  74.51  
[Epoch: 155] [Batch: 0201/0580] Loss: 0.18649  Avg Loss: 0.15312  Avg mIoU:  74.22  
[Epoch: 155] [Batch: 0251/0580] Loss: 0.15641  Avg Loss: 0.15392  Avg mIoU:  74.02  
[Epoch: 155] [Batch: 0301/0580] Loss: 0.20624  Avg Loss: 0.15392  Avg mIoU:  73.97  
[Epoch: 155] [Batch: 0351/0580] Loss: 0.11024  Avg Loss: 0.15501  Avg mIoU:  73.89  
[Epoch: 155] [Batch: 0401/0580] Loss: 0.25188  Avg Loss: 0.15547  Avg mIoU:  73.61  
[Epoch: 155] [Batch: 0451/0580] Loss: 0.14502  Avg Loss: 0.15494  Avg mIoU:  73.68  
[Epoch: 155] [Batch: 0501/0580] Loss: 0.17961  Avg Loss: 0.15414  Avg mIoU:  73.81  
[Epoch: 155] [Batch: 0551/0580] Loss: 0.10549  Avg Loss: 0.15447  Avg mIoU:  73.94  

*** Training [@Epoch 155] Avg Loss: 0.15491  Avg mIoU:  73.92  ***

[Epoch: 155] [Batch: 0001/0050] Loss: 0.16759  Avg Loss: 0.16759  Avg mIoU:  59.96  

*** Validation [@Epoch 155] Avg Loss: 0.23810  Avg mIoU:  59.94  ***

[Epoch: 156] [Batch: 0001/0580] Loss: 0.18633  Avg Loss: 0.18633  Avg mIoU:  49.74  
[Epoch: 156] [Batch: 0051/0580] Loss: 0.10062  Avg Loss: 0.14561  Avg mIoU:  74.77  
[Epoch: 156] [Batch: 0101/0580] Loss: 0.18172  Avg Loss: 0.14722  Avg mIoU:  74.79  
[Epoch: 156] [Batch: 0151/0580] Loss: 0.11181  Avg Loss: 0.14615  Avg mIoU:  74.53  
[Epoch: 156] [Batch: 0201/0580] Loss: 0.19853  Avg Loss: 0.14835  Avg mIoU:  74.22  
[Epoch: 156] [Batch: 0251/0580] Loss: 0.17086  Avg Loss: 0.14974  Avg mIoU:  74.36  
[Epoch: 156] [Batch: 0301/0580] Loss: 0.17587  Avg Loss: 0.14919  Avg mIoU:  74.53  
[Epoch: 156] [Batch: 0351/0580] Loss: 0.15011  Avg Loss: 0.14869  Avg mIoU:  74.81  
[Epoch: 156] [Batch: 0401/0580] Loss: 0.15101  Avg Loss: 0.14940  Avg mIoU:  74.61  
[Epoch: 156] [Batch: 0451/0580] Loss: 0.10482  Avg Loss: 0.15090  Avg mIoU:  74.28  
[Epoch: 156] [Batch: 0501/0580] Loss: 0.14878  Avg Loss: 0.15025  Avg mIoU:  74.29  
[Epoch: 156] [Batch: 0551/0580] Loss: 0.14604  Avg Loss: 0.15122  Avg mIoU:  74.35  

*** Training [@Epoch 156] Avg Loss: 0.15106  Avg mIoU:  74.35  ***

[Epoch: 156] [Batch: 0001/0050] Loss: 0.15980  Avg Loss: 0.15980  Avg mIoU:  61.52  

*** Validation [@Epoch 156] Avg Loss: 0.25664  Avg mIoU:  60.84  ***

[Epoch: 157] [Batch: 0001/0580] Loss: 0.13474  Avg Loss: 0.13474  Avg mIoU:  53.19  
[Epoch: 157] [Batch: 0051/0580] Loss: 0.12830  Avg Loss: 0.15596  Avg mIoU:  74.69  
[Epoch: 157] [Batch: 0101/0580] Loss: 0.11226  Avg Loss: 0.15105  Avg mIoU:  74.49  
[Epoch: 157] [Batch: 0151/0580] Loss: 0.14848  Avg Loss: 0.14857  Avg mIoU:  74.90  
[Epoch: 157] [Batch: 0201/0580] Loss: 0.17935  Avg Loss: 0.14934  Avg mIoU:  74.27  
[Epoch: 157] [Batch: 0251/0580] Loss: 0.11335  Avg Loss: 0.14901  Avg mIoU:  74.39  
[Epoch: 157] [Batch: 0301/0580] Loss: 0.15673  Avg Loss: 0.14920  Avg mIoU:  74.08  
[Epoch: 157] [Batch: 0351/0580] Loss: 0.19779  Avg Loss: 0.15043  Avg mIoU:  74.09  
[Epoch: 157] [Batch: 0401/0580] Loss: 0.11403  Avg Loss: 0.15081  Avg mIoU:  74.10  
[Epoch: 157] [Batch: 0451/0580] Loss: 0.14590  Avg Loss: 0.15059  Avg mIoU:  74.08  
[Epoch: 157] [Batch: 0501/0580] Loss: 0.19068  Avg Loss: 0.15131  Avg mIoU:  74.07  
[Epoch: 157] [Batch: 0551/0580] Loss: 0.12827  Avg Loss: 0.15225  Avg mIoU:  74.04  

*** Training [@Epoch 157] Avg Loss: 0.15248  Avg mIoU:  74.05  ***

[Epoch: 157] [Batch: 0001/0050] Loss: 0.15129  Avg Loss: 0.15129  Avg mIoU:  57.98  

*** Validation [@Epoch 157] Avg Loss: 0.24040  Avg mIoU:  59.32  ***

[Epoch: 158] [Batch: 0001/0580] Loss: 0.13250  Avg Loss: 0.13250  Avg mIoU:  43.67  
[Epoch: 158] [Batch: 0051/0580] Loss: 0.10812  Avg Loss: 0.14634  Avg mIoU:  74.83  
[Epoch: 158] [Batch: 0101/0580] Loss: 0.10476  Avg Loss: 0.14695  Avg mIoU:  74.22  
[Epoch: 158] [Batch: 0151/0580] Loss: 0.14484  Avg Loss: 0.14760  Avg mIoU:  74.20  
[Epoch: 158] [Batch: 0201/0580] Loss: 0.15529  Avg Loss: 0.14870  Avg mIoU:  74.31  
[Epoch: 158] [Batch: 0251/0580] Loss: 0.11409  Avg Loss: 0.15018  Avg mIoU:  74.42  
[Epoch: 158] [Batch: 0301/0580] Loss: 0.16312  Avg Loss: 0.14987  Avg mIoU:  74.52  
[Epoch: 158] [Batch: 0351/0580] Loss: 0.13643  Avg Loss: 0.15079  Avg mIoU:  74.59  
[Epoch: 158] [Batch: 0401/0580] Loss: 0.16222  Avg Loss: 0.15083  Avg mIoU:  74.40  
[Epoch: 158] [Batch: 0451/0580] Loss: 0.24716  Avg Loss: 0.15185  Avg mIoU:  74.26  
[Epoch: 158] [Batch: 0501/0580] Loss: 0.16026  Avg Loss: 0.15284  Avg mIoU:  74.02  
[Epoch: 158] [Batch: 0551/0580] Loss: 0.15037  Avg Loss: 0.15294  Avg mIoU:  73.93  

*** Training [@Epoch 158] Avg Loss: 0.15310  Avg mIoU:  73.95  ***

[Epoch: 158] [Batch: 0001/0050] Loss: 0.15389  Avg Loss: 0.15389  Avg mIoU:  65.18  

*** Validation [@Epoch 158] Avg Loss: 0.24433  Avg mIoU:  61.34  ***

[Epoch: 159] [Batch: 0001/0580] Loss: 0.14102  Avg Loss: 0.14102  Avg mIoU:  51.41  
[Epoch: 159] [Batch: 0051/0580] Loss: 0.18183  Avg Loss: 0.14934  Avg mIoU:  75.58  
[Epoch: 159] [Batch: 0101/0580] Loss: 0.09386  Avg Loss: 0.14974  Avg mIoU:  75.07  
[Epoch: 159] [Batch: 0151/0580] Loss: 0.14111  Avg Loss: 0.15067  Avg mIoU:  74.50  
[Epoch: 159] [Batch: 0201/0580] Loss: 0.09874  Avg Loss: 0.14909  Avg mIoU:  74.94  
[Epoch: 159] [Batch: 0251/0580] Loss: 0.21679  Avg Loss: 0.15020  Avg mIoU:  74.84  
[Epoch: 159] [Batch: 0301/0580] Loss: 0.14983  Avg Loss: 0.15099  Avg mIoU:  74.37  
[Epoch: 159] [Batch: 0351/0580] Loss: 0.14046  Avg Loss: 0.15015  Avg mIoU:  74.46  
[Epoch: 159] [Batch: 0401/0580] Loss: 0.12946  Avg Loss: 0.15097  Avg mIoU:  74.47  
[Epoch: 159] [Batch: 0451/0580] Loss: 0.15363  Avg Loss: 0.15118  Avg mIoU:  74.56  
[Epoch: 159] [Batch: 0501/0580] Loss: 0.09843  Avg Loss: 0.15183  Avg mIoU:  74.60  
[Epoch: 159] [Batch: 0551/0580] Loss: 0.26344  Avg Loss: 0.15226  Avg mIoU:  74.53  

*** Training [@Epoch 159] Avg Loss: 0.15195  Avg mIoU:  74.50  ***

[Epoch: 159] [Batch: 0001/0050] Loss: 0.15021  Avg Loss: 0.15021  Avg mIoU:  59.17  

*** Validation [@Epoch 159] Avg Loss: 0.24036  Avg mIoU:  58.82  ***

[Epoch: 160] [Batch: 0001/0580] Loss: 0.25568  Avg Loss: 0.25568  Avg mIoU:  35.58  
[Epoch: 160] [Batch: 0051/0580] Loss: 0.11830  Avg Loss: 0.15306  Avg mIoU:  75.79  
[Epoch: 160] [Batch: 0101/0580] Loss: 0.18176  Avg Loss: 0.14925  Avg mIoU:  75.38  
[Epoch: 160] [Batch: 0151/0580] Loss: 0.20833  Avg Loss: 0.15030  Avg mIoU:  74.72  
[Epoch: 160] [Batch: 0201/0580] Loss: 0.18599  Avg Loss: 0.15188  Avg mIoU:  74.60  
[Epoch: 160] [Batch: 0251/0580] Loss: 0.13272  Avg Loss: 0.15118  Avg mIoU:  74.79  
[Epoch: 160] [Batch: 0301/0580] Loss: 0.18858  Avg Loss: 0.15239  Avg mIoU:  74.38  
[Epoch: 160] [Batch: 0351/0580] Loss: 0.14472  Avg Loss: 0.15265  Avg mIoU:  74.31  
[Epoch: 160] [Batch: 0401/0580] Loss: 0.15007  Avg Loss: 0.15301  Avg mIoU:  74.04  
[Epoch: 160] [Batch: 0451/0580] Loss: 0.15427  Avg Loss: 0.15246  Avg mIoU:  74.07  
[Epoch: 160] [Batch: 0501/0580] Loss: 0.14285  Avg Loss: 0.15327  Avg mIoU:  73.99  
[Epoch: 160] [Batch: 0551/0580] Loss: 0.13236  Avg Loss: 0.15275  Avg mIoU:  73.90  

*** Training [@Epoch 160] Avg Loss: 0.15229  Avg mIoU:  74.03  ***

[Epoch: 160] [Batch: 0001/0050] Loss: 0.15547  Avg Loss: 0.15547  Avg mIoU:  61.63  

*** Validation [@Epoch 160] Avg Loss: 0.25113  Avg mIoU:  60.90  ***

[Epoch: 161] [Batch: 0001/0580] Loss: 0.14154  Avg Loss: 0.14154  Avg mIoU:  54.58  
[Epoch: 161] [Batch: 0051/0580] Loss: 0.15391  Avg Loss: 0.15290  Avg mIoU:  74.47  
[Epoch: 161] [Batch: 0101/0580] Loss: 0.13195  Avg Loss: 0.15329  Avg mIoU:  74.10  
[Epoch: 161] [Batch: 0151/0580] Loss: 0.13911  Avg Loss: 0.15166  Avg mIoU:  74.78  
[Epoch: 161] [Batch: 0201/0580] Loss: 0.13695  Avg Loss: 0.15167  Avg mIoU:  74.96  
[Epoch: 161] [Batch: 0251/0580] Loss: 0.13671  Avg Loss: 0.15211  Avg mIoU:  74.89  
[Epoch: 161] [Batch: 0301/0580] Loss: 0.12757  Avg Loss: 0.15200  Avg mIoU:  74.84  
[Epoch: 161] [Batch: 0351/0580] Loss: 0.11093  Avg Loss: 0.15217  Avg mIoU:  74.73  
[Epoch: 161] [Batch: 0401/0580] Loss: 0.16450  Avg Loss: 0.15230  Avg mIoU:  74.55  
[Epoch: 161] [Batch: 0451/0580] Loss: 0.15796  Avg Loss: 0.15189  Avg mIoU:  74.73  
[Epoch: 161] [Batch: 0501/0580] Loss: 0.17656  Avg Loss: 0.15172  Avg mIoU:  74.63  
[Epoch: 161] [Batch: 0551/0580] Loss: 0.12798  Avg Loss: 0.15138  Avg mIoU:  74.60  

*** Training [@Epoch 161] Avg Loss: 0.15125  Avg mIoU:  74.60  ***

[Epoch: 161] [Batch: 0001/0050] Loss: 0.15718  Avg Loss: 0.15718  Avg mIoU:  58.13  

*** Validation [@Epoch 161] Avg Loss: 0.25217  Avg mIoU:  58.18  ***

[Epoch: 162] [Batch: 0001/0580] Loss: 0.07194  Avg Loss: 0.07194  Avg mIoU:  57.27  
[Epoch: 162] [Batch: 0051/0580] Loss: 0.18621  Avg Loss: 0.15064  Avg mIoU:  76.06  
[Epoch: 162] [Batch: 0101/0580] Loss: 0.12983  Avg Loss: 0.15141  Avg mIoU:  74.82  
[Epoch: 162] [Batch: 0151/0580] Loss: 0.11439  Avg Loss: 0.15188  Avg mIoU:  74.19  
[Epoch: 162] [Batch: 0201/0580] Loss: 0.12926  Avg Loss: 0.15021  Avg mIoU:  73.75  
[Epoch: 162] [Batch: 0251/0580] Loss: 0.13582  Avg Loss: 0.14934  Avg mIoU:  74.39  
[Epoch: 162] [Batch: 0301/0580] Loss: 0.16862  Avg Loss: 0.14946  Avg mIoU:  74.46  
[Epoch: 162] [Batch: 0351/0580] Loss: 0.11233  Avg Loss: 0.14872  Avg mIoU:  74.61  
[Epoch: 162] [Batch: 0401/0580] Loss: 0.17415  Avg Loss: 0.14992  Avg mIoU:  74.35  
[Epoch: 162] [Batch: 0451/0580] Loss: 0.17668  Avg Loss: 0.14986  Avg mIoU:  74.29  
[Epoch: 162] [Batch: 0501/0580] Loss: 0.18382  Avg Loss: 0.14982  Avg mIoU:  74.42  
[Epoch: 162] [Batch: 0551/0580] Loss: 0.13754  Avg Loss: 0.14990  Avg mIoU:  74.46  

*** Training [@Epoch 162] Avg Loss: 0.15050  Avg mIoU:  74.47  ***

[Epoch: 162] [Batch: 0001/0050] Loss: 0.16772  Avg Loss: 0.16772  Avg mIoU:  58.75  

*** Validation [@Epoch 162] Avg Loss: 0.24644  Avg mIoU:  60.92  ***

[Epoch: 163] [Batch: 0001/0580] Loss: 0.17996  Avg Loss: 0.17996  Avg mIoU:  36.91  
[Epoch: 163] [Batch: 0051/0580] Loss: 0.14608  Avg Loss: 0.15912  Avg mIoU:  73.57  
[Epoch: 163] [Batch: 0101/0580] Loss: 0.18616  Avg Loss: 0.15433  Avg mIoU:  73.76  
[Epoch: 163] [Batch: 0151/0580] Loss: 0.16018  Avg Loss: 0.15237  Avg mIoU:  74.17  
[Epoch: 163] [Batch: 0201/0580] Loss: 0.21766  Avg Loss: 0.15151  Avg mIoU:  74.09  
[Epoch: 163] [Batch: 0251/0580] Loss: 0.14661  Avg Loss: 0.15143  Avg mIoU:  74.08  
[Epoch: 163] [Batch: 0301/0580] Loss: 0.15704  Avg Loss: 0.15289  Avg mIoU:  74.04  
[Epoch: 163] [Batch: 0351/0580] Loss: 0.28139  Avg Loss: 0.15372  Avg mIoU:  73.84  
[Epoch: 163] [Batch: 0401/0580] Loss: 0.14324  Avg Loss: 0.15390  Avg mIoU:  73.79  
[Epoch: 163] [Batch: 0451/0580] Loss: 0.10322  Avg Loss: 0.15267  Avg mIoU:  73.78  
[Epoch: 163] [Batch: 0501/0580] Loss: 0.10120  Avg Loss: 0.15251  Avg mIoU:  73.88  
[Epoch: 163] [Batch: 0551/0580] Loss: 0.18812  Avg Loss: 0.15221  Avg mIoU:  74.00  

*** Training [@Epoch 163] Avg Loss: 0.15226  Avg mIoU:  74.03  ***

[Epoch: 163] [Batch: 0001/0050] Loss: 0.15471  Avg Loss: 0.15471  Avg mIoU:  58.29  

*** Validation [@Epoch 163] Avg Loss: 0.24182  Avg mIoU:  58.44  ***

[Epoch: 164] [Batch: 0001/0580] Loss: 0.13816  Avg Loss: 0.13816  Avg mIoU:  46.05  
[Epoch: 164] [Batch: 0051/0580] Loss: 0.09321  Avg Loss: 0.14368  Avg mIoU:  75.53  
[Epoch: 164] [Batch: 0101/0580] Loss: 0.12395  Avg Loss: 0.14913  Avg mIoU:  75.62  
[Epoch: 164] [Batch: 0151/0580] Loss: 0.16082  Avg Loss: 0.14731  Avg mIoU:  75.18  
[Epoch: 164] [Batch: 0201/0580] Loss: 0.15806  Avg Loss: 0.14952  Avg mIoU:  74.49  
[Epoch: 164] [Batch: 0251/0580] Loss: 0.12348  Avg Loss: 0.14963  Avg mIoU:  74.66  
[Epoch: 164] [Batch: 0301/0580] Loss: 0.13678  Avg Loss: 0.14974  Avg mIoU:  74.93  
[Epoch: 164] [Batch: 0351/0580] Loss: 0.17081  Avg Loss: 0.15047  Avg mIoU:  74.87  
[Epoch: 164] [Batch: 0401/0580] Loss: 0.22858  Avg Loss: 0.15133  Avg mIoU:  74.63  
[Epoch: 164] [Batch: 0451/0580] Loss: 0.13658  Avg Loss: 0.15114  Avg mIoU:  74.55  
[Epoch: 164] [Batch: 0501/0580] Loss: 0.18049  Avg Loss: 0.15135  Avg mIoU:  74.47  
[Epoch: 164] [Batch: 0551/0580] Loss: 0.14078  Avg Loss: 0.15054  Avg mIoU:  74.57  

*** Training [@Epoch 164] Avg Loss: 0.15084  Avg mIoU:  74.58  ***

[Epoch: 164] [Batch: 0001/0050] Loss: 0.16920  Avg Loss: 0.16920  Avg mIoU:  61.44  

*** Validation [@Epoch 164] Avg Loss: 0.25606  Avg mIoU:  62.01  ***

[Epoch: 165] [Batch: 0001/0580] Loss: 0.24162  Avg Loss: 0.24162  Avg mIoU:  60.39  
[Epoch: 165] [Batch: 0051/0580] Loss: 0.14734  Avg Loss: 0.15056  Avg mIoU:  75.70  
[Epoch: 165] [Batch: 0101/0580] Loss: 0.09634  Avg Loss: 0.15196  Avg mIoU:  74.99  
[Epoch: 165] [Batch: 0151/0580] Loss: 0.17931  Avg Loss: 0.15091  Avg mIoU:  74.76  
[Epoch: 165] [Batch: 0201/0580] Loss: 0.11414  Avg Loss: 0.14840  Avg mIoU:  74.77  
[Epoch: 165] [Batch: 0251/0580] Loss: 0.18283  Avg Loss: 0.15080  Avg mIoU:  74.63  
[Epoch: 165] [Batch: 0301/0580] Loss: 0.14307  Avg Loss: 0.14956  Avg mIoU:  74.49  
[Epoch: 165] [Batch: 0351/0580] Loss: 0.19179  Avg Loss: 0.15014  Avg mIoU:  74.45  
[Epoch: 165] [Batch: 0401/0580] Loss: 0.13296  Avg Loss: 0.14949  Avg mIoU:  74.78  
[Epoch: 165] [Batch: 0451/0580] Loss: 0.21403  Avg Loss: 0.14853  Avg mIoU:  74.89  
[Epoch: 165] [Batch: 0501/0580] Loss: 0.17057  Avg Loss: 0.14940  Avg mIoU:  74.59  
[Epoch: 165] [Batch: 0551/0580] Loss: 0.14843  Avg Loss: 0.14942  Avg mIoU:  74.62  

*** Training [@Epoch 165] Avg Loss: 0.14962  Avg mIoU:  74.53  ***

[Epoch: 165] [Batch: 0001/0050] Loss: 0.16318  Avg Loss: 0.16318  Avg mIoU:  58.75  

*** Validation [@Epoch 165] Avg Loss: 0.25344  Avg mIoU:  58.46  ***

[Epoch: 166] [Batch: 0001/0580] Loss: 0.19138  Avg Loss: 0.19138  Avg mIoU:  49.16  
[Epoch: 166] [Batch: 0051/0580] Loss: 0.21554  Avg Loss: 0.15545  Avg mIoU:  75.05  
[Epoch: 166] [Batch: 0101/0580] Loss: 0.17545  Avg Loss: 0.15661  Avg mIoU:  74.50  
[Epoch: 166] [Batch: 0151/0580] Loss: 0.17233  Avg Loss: 0.15520  Avg mIoU:  74.24  
[Epoch: 166] [Batch: 0201/0580] Loss: 0.12905  Avg Loss: 0.15450  Avg mIoU:  74.05  
[Epoch: 166] [Batch: 0251/0580] Loss: 0.14114  Avg Loss: 0.15406  Avg mIoU:  73.95  
[Epoch: 166] [Batch: 0301/0580] Loss: 0.18686  Avg Loss: 0.15298  Avg mIoU:  74.11  
[Epoch: 166] [Batch: 0351/0580] Loss: 0.14524  Avg Loss: 0.15170  Avg mIoU:  74.25  
[Epoch: 166] [Batch: 0401/0580] Loss: 0.11441  Avg Loss: 0.15185  Avg mIoU:  74.46  
[Epoch: 166] [Batch: 0451/0580] Loss: 0.12295  Avg Loss: 0.15144  Avg mIoU:  74.50  
[Epoch: 166] [Batch: 0501/0580] Loss: 0.14792  Avg Loss: 0.15040  Avg mIoU:  74.55  
[Epoch: 166] [Batch: 0551/0580] Loss: 0.17071  Avg Loss: 0.15096  Avg mIoU:  74.31  

*** Training [@Epoch 166] Avg Loss: 0.15079  Avg mIoU:  74.37  ***

[Epoch: 166] [Batch: 0001/0050] Loss: 0.17289  Avg Loss: 0.17289  Avg mIoU:  57.35  

*** Validation [@Epoch 166] Avg Loss: 0.25007  Avg mIoU:  60.20  ***

[Epoch: 167] [Batch: 0001/0580] Loss: 0.13520  Avg Loss: 0.13520  Avg mIoU:  35.35  
[Epoch: 167] [Batch: 0051/0580] Loss: 0.18328  Avg Loss: 0.15431  Avg mIoU:  75.21  
[Epoch: 167] [Batch: 0101/0580] Loss: 0.21170  Avg Loss: 0.15446  Avg mIoU:  73.67  
[Epoch: 167] [Batch: 0151/0580] Loss: 0.14936  Avg Loss: 0.15259  Avg mIoU:  73.96  
[Epoch: 167] [Batch: 0201/0580] Loss: 0.14156  Avg Loss: 0.15303  Avg mIoU:  73.94  
[Epoch: 167] [Batch: 0251/0580] Loss: 0.09648  Avg Loss: 0.15281  Avg mIoU:  74.10  
[Epoch: 167] [Batch: 0301/0580] Loss: 0.11996  Avg Loss: 0.15079  Avg mIoU:  74.48  
[Epoch: 167] [Batch: 0351/0580] Loss: 0.13429  Avg Loss: 0.15050  Avg mIoU:  74.47  
[Epoch: 167] [Batch: 0401/0580] Loss: 0.16047  Avg Loss: 0.15118  Avg mIoU:  74.60  
[Epoch: 167] [Batch: 0451/0580] Loss: 0.15893  Avg Loss: 0.15308  Avg mIoU:  74.26  
[Epoch: 167] [Batch: 0501/0580] Loss: 0.15165  Avg Loss: 0.15218  Avg mIoU:  74.34  
[Epoch: 167] [Batch: 0551/0580] Loss: 0.15787  Avg Loss: 0.15248  Avg mIoU:  74.28  

*** Training [@Epoch 167] Avg Loss: 0.15229  Avg mIoU:  74.29  ***

[Epoch: 167] [Batch: 0001/0050] Loss: 0.17043  Avg Loss: 0.17043  Avg mIoU:  58.23  

*** Validation [@Epoch 167] Avg Loss: 0.24067  Avg mIoU:  61.68  ***

[Epoch: 168] [Batch: 0001/0580] Loss: 0.19227  Avg Loss: 0.19227  Avg mIoU:  34.64  
[Epoch: 168] [Batch: 0051/0580] Loss: 0.13031  Avg Loss: 0.15648  Avg mIoU:  72.61  
[Epoch: 168] [Batch: 0101/0580] Loss: 0.11719  Avg Loss: 0.15223  Avg mIoU:  73.44  
[Epoch: 168] [Batch: 0151/0580] Loss: 0.13208  Avg Loss: 0.15058  Avg mIoU:  73.50  
[Epoch: 168] [Batch: 0201/0580] Loss: 0.10874  Avg Loss: 0.14937  Avg mIoU:  73.82  
[Epoch: 168] [Batch: 0251/0580] Loss: 0.13141  Avg Loss: 0.15065  Avg mIoU:  73.86  
[Epoch: 168] [Batch: 0301/0580] Loss: 0.18469  Avg Loss: 0.15140  Avg mIoU:  73.96  
[Epoch: 168] [Batch: 0351/0580] Loss: 0.14839  Avg Loss: 0.15268  Avg mIoU:  73.86  
[Epoch: 168] [Batch: 0401/0580] Loss: 0.12241  Avg Loss: 0.15076  Avg mIoU:  73.76  
[Epoch: 168] [Batch: 0451/0580] Loss: 0.17366  Avg Loss: 0.14985  Avg mIoU:  74.14  
[Epoch: 168] [Batch: 0501/0580] Loss: 0.14402  Avg Loss: 0.14920  Avg mIoU:  74.22  
[Epoch: 168] [Batch: 0551/0580] Loss: 0.13372  Avg Loss: 0.14934  Avg mIoU:  74.44  

*** Training [@Epoch 168] Avg Loss: 0.14986  Avg mIoU:  74.43  ***

[Epoch: 168] [Batch: 0001/0050] Loss: 0.16102  Avg Loss: 0.16102  Avg mIoU:  60.54  

*** Validation [@Epoch 168] Avg Loss: 0.24589  Avg mIoU:  60.77  ***

[Epoch: 169] [Batch: 0001/0580] Loss: 0.12476  Avg Loss: 0.12476  Avg mIoU:  53.73  
[Epoch: 169] [Batch: 0051/0580] Loss: 0.11184  Avg Loss: 0.14372  Avg mIoU:  75.78  
[Epoch: 169] [Batch: 0101/0580] Loss: 0.14537  Avg Loss: 0.14388  Avg mIoU:  75.87  
[Epoch: 169] [Batch: 0151/0580] Loss: 0.12670  Avg Loss: 0.14495  Avg mIoU:  75.33  
[Epoch: 169] [Batch: 0201/0580] Loss: 0.17733  Avg Loss: 0.14587  Avg mIoU:  75.36  
[Epoch: 169] [Batch: 0251/0580] Loss: 0.15747  Avg Loss: 0.14536  Avg mIoU:  75.24  
[Epoch: 169] [Batch: 0301/0580] Loss: 0.13989  Avg Loss: 0.14563  Avg mIoU:  74.98  
[Epoch: 169] [Batch: 0351/0580] Loss: 0.11700  Avg Loss: 0.14777  Avg mIoU:  74.78  
[Epoch: 169] [Batch: 0401/0580] Loss: 0.12037  Avg Loss: 0.14886  Avg mIoU:  74.70  
[Epoch: 169] [Batch: 0451/0580] Loss: 0.12148  Avg Loss: 0.14881  Avg mIoU:  74.89  
[Epoch: 169] [Batch: 0501/0580] Loss: 0.17828  Avg Loss: 0.14876  Avg mIoU:  74.76  
[Epoch: 169] [Batch: 0551/0580] Loss: 0.12113  Avg Loss: 0.14919  Avg mIoU:  74.56  

*** Training [@Epoch 169] Avg Loss: 0.14916  Avg mIoU:  74.55  ***

[Epoch: 169] [Batch: 0001/0050] Loss: 0.16406  Avg Loss: 0.16406  Avg mIoU:  59.40  

*** Validation [@Epoch 169] Avg Loss: 0.24413  Avg mIoU:  61.44  ***

[Epoch: 170] [Batch: 0001/0580] Loss: 0.11541  Avg Loss: 0.11541  Avg mIoU:  50.95  
[Epoch: 170] [Batch: 0051/0580] Loss: 0.12696  Avg Loss: 0.14184  Avg mIoU:  75.93  
[Epoch: 170] [Batch: 0101/0580] Loss: 0.13415  Avg Loss: 0.14572  Avg mIoU:  76.33  
[Epoch: 170] [Batch: 0151/0580] Loss: 0.17009  Avg Loss: 0.14688  Avg mIoU:  76.03  
[Epoch: 170] [Batch: 0201/0580] Loss: 0.15599  Avg Loss: 0.14663  Avg mIoU:  75.53  
[Epoch: 170] [Batch: 0251/0580] Loss: 0.12819  Avg Loss: 0.14719  Avg mIoU:  75.38  
[Epoch: 170] [Batch: 0301/0580] Loss: 0.13213  Avg Loss: 0.14700  Avg mIoU:  75.36  
[Epoch: 170] [Batch: 0351/0580] Loss: 0.11716  Avg Loss: 0.14673  Avg mIoU:  75.44  
[Epoch: 170] [Batch: 0401/0580] Loss: 0.13981  Avg Loss: 0.14733  Avg mIoU:  75.47  
[Epoch: 170] [Batch: 0451/0580] Loss: 0.10716  Avg Loss: 0.14710  Avg mIoU:  75.43  
[Epoch: 170] [Batch: 0501/0580] Loss: 0.18894  Avg Loss: 0.14772  Avg mIoU:  75.21  
[Epoch: 170] [Batch: 0551/0580] Loss: 0.12901  Avg Loss: 0.14818  Avg mIoU:  75.01  

*** Training [@Epoch 170] Avg Loss: 0.14794  Avg mIoU:  75.06  ***

[Epoch: 170] [Batch: 0001/0050] Loss: 0.18643  Avg Loss: 0.18643  Avg mIoU:  58.12  

*** Validation [@Epoch 170] Avg Loss: 0.26654  Avg mIoU:  60.67  ***

[Epoch: 171] [Batch: 0001/0580] Loss: 0.11134  Avg Loss: 0.11134  Avg mIoU:  43.31  
[Epoch: 171] [Batch: 0051/0580] Loss: 0.15523  Avg Loss: 0.14517  Avg mIoU:  73.63  
[Epoch: 171] [Batch: 0101/0580] Loss: 0.15015  Avg Loss: 0.14904  Avg mIoU:  73.28  
[Epoch: 171] [Batch: 0151/0580] Loss: 0.11817  Avg Loss: 0.15062  Avg mIoU:  73.68  
[Epoch: 171] [Batch: 0201/0580] Loss: 0.15834  Avg Loss: 0.14998  Avg mIoU:  73.93  
[Epoch: 171] [Batch: 0251/0580] Loss: 0.13071  Avg Loss: 0.14944  Avg mIoU:  74.16  
[Epoch: 171] [Batch: 0301/0580] Loss: 0.14877  Avg Loss: 0.15013  Avg mIoU:  74.01  
[Epoch: 171] [Batch: 0351/0580] Loss: 0.10198  Avg Loss: 0.14938  Avg mIoU:  74.10  
[Epoch: 171] [Batch: 0401/0580] Loss: 0.11113  Avg Loss: 0.14942  Avg mIoU:  74.10  
[Epoch: 171] [Batch: 0451/0580] Loss: 0.12101  Avg Loss: 0.14980  Avg mIoU:  74.35  
[Epoch: 171] [Batch: 0501/0580] Loss: 0.22314  Avg Loss: 0.14984  Avg mIoU:  74.53  
[Epoch: 171] [Batch: 0551/0580] Loss: 0.10433  Avg Loss: 0.14943  Avg mIoU:  74.50  

*** Training [@Epoch 171] Avg Loss: 0.14970  Avg mIoU:  74.38  ***

[Epoch: 171] [Batch: 0001/0050] Loss: 0.17346  Avg Loss: 0.17346  Avg mIoU:  56.13  

*** Validation [@Epoch 171] Avg Loss: 0.23993  Avg mIoU:  60.20  ***

[Epoch: 172] [Batch: 0001/0580] Loss: 0.13613  Avg Loss: 0.13613  Avg mIoU:  36.09  
[Epoch: 172] [Batch: 0051/0580] Loss: 0.13756  Avg Loss: 0.15205  Avg mIoU:  74.22  
[Epoch: 172] [Batch: 0101/0580] Loss: 0.18133  Avg Loss: 0.14972  Avg mIoU:  74.39  
[Epoch: 172] [Batch: 0151/0580] Loss: 0.17261  Avg Loss: 0.14790  Avg mIoU:  74.51  
[Epoch: 172] [Batch: 0201/0580] Loss: 0.16999  Avg Loss: 0.15118  Avg mIoU:  74.69  
[Epoch: 172] [Batch: 0251/0580] Loss: 0.14168  Avg Loss: 0.15057  Avg mIoU:  74.66  
[Epoch: 172] [Batch: 0301/0580] Loss: 0.16375  Avg Loss: 0.15151  Avg mIoU:  74.58  
[Epoch: 172] [Batch: 0351/0580] Loss: 0.10431  Avg Loss: 0.15120  Avg mIoU:  74.67  
[Epoch: 172] [Batch: 0401/0580] Loss: 0.14372  Avg Loss: 0.15216  Avg mIoU:  74.50  
[Epoch: 172] [Batch: 0451/0580] Loss: 0.21029  Avg Loss: 0.15274  Avg mIoU:  74.48  
[Epoch: 172] [Batch: 0501/0580] Loss: 0.18680  Avg Loss: 0.15208  Avg mIoU:  74.35  
[Epoch: 172] [Batch: 0551/0580] Loss: 0.13524  Avg Loss: 0.15272  Avg mIoU:  74.22  

*** Training [@Epoch 172] Avg Loss: 0.15196  Avg mIoU:  74.26  ***

[Epoch: 172] [Batch: 0001/0050] Loss: 0.15088  Avg Loss: 0.15088  Avg mIoU:  62.34  

*** Validation [@Epoch 172] Avg Loss: 0.26344  Avg mIoU:  58.62  ***

[Epoch: 173] [Batch: 0001/0580] Loss: 0.26002  Avg Loss: 0.26002  Avg mIoU:  43.22  
[Epoch: 173] [Batch: 0051/0580] Loss: 0.11095  Avg Loss: 0.15030  Avg mIoU:  73.64  
[Epoch: 173] [Batch: 0101/0580] Loss: 0.09129  Avg Loss: 0.14602  Avg mIoU:  74.19  
[Epoch: 173] [Batch: 0151/0580] Loss: 0.15677  Avg Loss: 0.14501  Avg mIoU:  73.99  
[Epoch: 173] [Batch: 0201/0580] Loss: 0.12130  Avg Loss: 0.14547  Avg mIoU:  74.29  
[Epoch: 173] [Batch: 0251/0580] Loss: 0.12635  Avg Loss: 0.14436  Avg mIoU:  74.61  
[Epoch: 173] [Batch: 0301/0580] Loss: 0.12411  Avg Loss: 0.14682  Avg mIoU:  74.78  
[Epoch: 173] [Batch: 0351/0580] Loss: 0.16096  Avg Loss: 0.14789  Avg mIoU:  74.71  
[Epoch: 173] [Batch: 0401/0580] Loss: 0.14400  Avg Loss: 0.14824  Avg mIoU:  74.76  
[Epoch: 173] [Batch: 0451/0580] Loss: 0.13958  Avg Loss: 0.14888  Avg mIoU:  74.69  
[Epoch: 173] [Batch: 0501/0580] Loss: 0.16759  Avg Loss: 0.14894  Avg mIoU:  74.73  
[Epoch: 173] [Batch: 0551/0580] Loss: 0.13532  Avg Loss: 0.14909  Avg mIoU:  74.78  

*** Training [@Epoch 173] Avg Loss: 0.14931  Avg mIoU:  74.71  ***

[Epoch: 173] [Batch: 0001/0050] Loss: 0.16351  Avg Loss: 0.16351  Avg mIoU:  61.05  

*** Validation [@Epoch 173] Avg Loss: 0.25423  Avg mIoU:  60.07  ***

[Epoch: 174] [Batch: 0001/0580] Loss: 0.12786  Avg Loss: 0.12786  Avg mIoU:  46.60  
[Epoch: 174] [Batch: 0051/0580] Loss: 0.15534  Avg Loss: 0.14974  Avg mIoU:  74.64  
[Epoch: 174] [Batch: 0101/0580] Loss: 0.13335  Avg Loss: 0.14742  Avg mIoU:  74.58  
[Epoch: 174] [Batch: 0151/0580] Loss: 0.12296  Avg Loss: 0.14847  Avg mIoU:  74.60  
[Epoch: 174] [Batch: 0201/0580] Loss: 0.11356  Avg Loss: 0.14835  Avg mIoU:  75.07  
[Epoch: 174] [Batch: 0251/0580] Loss: 0.17882  Avg Loss: 0.15059  Avg mIoU:  74.97  
[Epoch: 174] [Batch: 0301/0580] Loss: 0.14650  Avg Loss: 0.14934  Avg mIoU:  74.98  
[Epoch: 174] [Batch: 0351/0580] Loss: 0.18888  Avg Loss: 0.14961  Avg mIoU:  74.85  
[Epoch: 174] [Batch: 0401/0580] Loss: 0.12573  Avg Loss: 0.14946  Avg mIoU:  74.85  
[Epoch: 174] [Batch: 0451/0580] Loss: 0.11043  Avg Loss: 0.14872  Avg mIoU:  74.73  
[Epoch: 174] [Batch: 0501/0580] Loss: 0.13008  Avg Loss: 0.14815  Avg mIoU:  74.75  
[Epoch: 174] [Batch: 0551/0580] Loss: 0.19160  Avg Loss: 0.14863  Avg mIoU:  74.65  

*** Training [@Epoch 174] Avg Loss: 0.14877  Avg mIoU:  74.65  ***

[Epoch: 174] [Batch: 0001/0050] Loss: 0.15625  Avg Loss: 0.15625  Avg mIoU:  58.22  

*** Validation [@Epoch 174] Avg Loss: 0.24721  Avg mIoU:  59.02  ***

[Epoch: 175] [Batch: 0001/0580] Loss: 0.14644  Avg Loss: 0.14644  Avg mIoU:  48.98  
[Epoch: 175] [Batch: 0051/0580] Loss: 0.14083  Avg Loss: 0.14584  Avg mIoU:  75.75  
[Epoch: 175] [Batch: 0101/0580] Loss: 0.09875  Avg Loss: 0.14457  Avg mIoU:  75.81  
[Epoch: 175] [Batch: 0151/0580] Loss: 0.16544  Avg Loss: 0.14227  Avg mIoU:  76.01  
[Epoch: 175] [Batch: 0201/0580] Loss: 0.13149  Avg Loss: 0.14589  Avg mIoU:  75.60  
[Epoch: 175] [Batch: 0251/0580] Loss: 0.13536  Avg Loss: 0.14863  Avg mIoU:  75.16  
[Epoch: 175] [Batch: 0301/0580] Loss: 0.15403  Avg Loss: 0.14844  Avg mIoU:  74.88  
[Epoch: 175] [Batch: 0351/0580] Loss: 0.14545  Avg Loss: 0.14886  Avg mIoU:  74.91  
[Epoch: 175] [Batch: 0401/0580] Loss: 0.12309  Avg Loss: 0.14898  Avg mIoU:  74.84  
[Epoch: 175] [Batch: 0451/0580] Loss: 0.13165  Avg Loss: 0.14876  Avg mIoU:  74.94  
[Epoch: 175] [Batch: 0501/0580] Loss: 0.19663  Avg Loss: 0.14881  Avg mIoU:  74.94  
[Epoch: 175] [Batch: 0551/0580] Loss: 0.15061  Avg Loss: 0.14854  Avg mIoU:  75.00  

*** Training [@Epoch 175] Avg Loss: 0.14904  Avg mIoU:  74.92  ***

[Epoch: 175] [Batch: 0001/0050] Loss: 0.16499  Avg Loss: 0.16499  Avg mIoU:  59.26  

*** Validation [@Epoch 175] Avg Loss: 0.25283  Avg mIoU:  61.05  ***

[Epoch: 176] [Batch: 0001/0580] Loss: 0.14485  Avg Loss: 0.14485  Avg mIoU:  41.03  
[Epoch: 176] [Batch: 0051/0580] Loss: 0.15387  Avg Loss: 0.15764  Avg mIoU:  74.18  
[Epoch: 176] [Batch: 0101/0580] Loss: 0.13925  Avg Loss: 0.15302  Avg mIoU:  73.76  
[Epoch: 176] [Batch: 0151/0580] Loss: 0.27119  Avg Loss: 0.15184  Avg mIoU:  73.71  
[Epoch: 176] [Batch: 0201/0580] Loss: 0.10094  Avg Loss: 0.15004  Avg mIoU:  74.13  
[Epoch: 176] [Batch: 0251/0580] Loss: 0.17787  Avg Loss: 0.14878  Avg mIoU:  74.55  
[Epoch: 176] [Batch: 0301/0580] Loss: 0.15472  Avg Loss: 0.14836  Avg mIoU:  74.98  
[Epoch: 176] [Batch: 0351/0580] Loss: 0.17243  Avg Loss: 0.14759  Avg mIoU:  74.94  
[Epoch: 176] [Batch: 0401/0580] Loss: 0.16534  Avg Loss: 0.14719  Avg mIoU:  74.92  
[Epoch: 176] [Batch: 0451/0580] Loss: 0.16914  Avg Loss: 0.14741  Avg mIoU:  74.90  
[Epoch: 176] [Batch: 0501/0580] Loss: 0.13986  Avg Loss: 0.14765  Avg mIoU:  74.86  
[Epoch: 176] [Batch: 0551/0580] Loss: 0.14454  Avg Loss: 0.14836  Avg mIoU:  74.90  

*** Training [@Epoch 176] Avg Loss: 0.14773  Avg mIoU:  74.91  ***

[Epoch: 176] [Batch: 0001/0050] Loss: 0.16990  Avg Loss: 0.16990  Avg mIoU:  58.86  

*** Validation [@Epoch 176] Avg Loss: 0.25295  Avg mIoU:  58.82  ***

[Epoch: 177] [Batch: 0001/0580] Loss: 0.11294  Avg Loss: 0.11294  Avg mIoU:  41.19  
[Epoch: 177] [Batch: 0051/0580] Loss: 0.11644  Avg Loss: 0.14843  Avg mIoU:  74.39  
[Epoch: 177] [Batch: 0101/0580] Loss: 0.16323  Avg Loss: 0.14913  Avg mIoU:  75.09  
[Epoch: 177] [Batch: 0151/0580] Loss: 0.17558  Avg Loss: 0.14990  Avg mIoU:  74.48  
[Epoch: 177] [Batch: 0201/0580] Loss: 0.13795  Avg Loss: 0.15013  Avg mIoU:  74.22  
[Epoch: 177] [Batch: 0251/0580] Loss: 0.14478  Avg Loss: 0.14961  Avg mIoU:  74.35  
[Epoch: 177] [Batch: 0301/0580] Loss: 0.10833  Avg Loss: 0.14849  Avg mIoU:  74.47  
[Epoch: 177] [Batch: 0351/0580] Loss: 0.14234  Avg Loss: 0.14936  Avg mIoU:  74.41  
[Epoch: 177] [Batch: 0401/0580] Loss: 0.14553  Avg Loss: 0.14892  Avg mIoU:  74.78  
[Epoch: 177] [Batch: 0451/0580] Loss: 0.16188  Avg Loss: 0.14810  Avg mIoU:  74.96  
[Epoch: 177] [Batch: 0501/0580] Loss: 0.15947  Avg Loss: 0.14749  Avg mIoU:  74.97  
[Epoch: 177] [Batch: 0551/0580] Loss: 0.18732  Avg Loss: 0.14783  Avg mIoU:  74.88  

*** Training [@Epoch 177] Avg Loss: 0.14785  Avg mIoU:  74.88  ***

[Epoch: 177] [Batch: 0001/0050] Loss: 0.17632  Avg Loss: 0.17632  Avg mIoU:  58.24  

*** Validation [@Epoch 177] Avg Loss: 0.24717  Avg mIoU:  60.80  ***

[Epoch: 178] [Batch: 0001/0580] Loss: 0.11565  Avg Loss: 0.11565  Avg mIoU:  48.79  
[Epoch: 178] [Batch: 0051/0580] Loss: 0.19240  Avg Loss: 0.14275  Avg mIoU:  75.00  
[Epoch: 178] [Batch: 0101/0580] Loss: 0.14633  Avg Loss: 0.14630  Avg mIoU:  74.14  
[Epoch: 178] [Batch: 0151/0580] Loss: 0.14204  Avg Loss: 0.14626  Avg mIoU:  74.63  
[Epoch: 178] [Batch: 0201/0580] Loss: 0.14479  Avg Loss: 0.14637  Avg mIoU:  74.57  
[Epoch: 178] [Batch: 0251/0580] Loss: 0.11000  Avg Loss: 0.14587  Avg mIoU:  74.49  
[Epoch: 178] [Batch: 0301/0580] Loss: 0.14453  Avg Loss: 0.14562  Avg mIoU:  74.39  
[Epoch: 178] [Batch: 0351/0580] Loss: 0.11152  Avg Loss: 0.14593  Avg mIoU:  74.57  
[Epoch: 178] [Batch: 0401/0580] Loss: 0.10558  Avg Loss: 0.14522  Avg mIoU:  74.69  
[Epoch: 178] [Batch: 0451/0580] Loss: 0.15076  Avg Loss: 0.14501  Avg mIoU:  74.81  
[Epoch: 178] [Batch: 0501/0580] Loss: 0.14034  Avg Loss: 0.14616  Avg mIoU:  74.90  
[Epoch: 178] [Batch: 0551/0580] Loss: 0.13577  Avg Loss: 0.14571  Avg mIoU:  75.02  

*** Training [@Epoch 178] Avg Loss: 0.14606  Avg mIoU:  75.02  ***

[Epoch: 178] [Batch: 0001/0050] Loss: 0.16393  Avg Loss: 0.16393  Avg mIoU:  57.49  

*** Validation [@Epoch 178] Avg Loss: 0.25513  Avg mIoU:  56.07  ***

[Epoch: 179] [Batch: 0001/0580] Loss: 0.21631  Avg Loss: 0.21631  Avg mIoU:  45.02  
[Epoch: 179] [Batch: 0051/0580] Loss: 0.17496  Avg Loss: 0.14881  Avg mIoU:  75.37  
[Epoch: 179] [Batch: 0101/0580] Loss: 0.29039  Avg Loss: 0.14870  Avg mIoU:  75.57  
[Epoch: 179] [Batch: 0151/0580] Loss: 0.10341  Avg Loss: 0.14618  Avg mIoU:  75.27  
[Epoch: 179] [Batch: 0201/0580] Loss: 0.10197  Avg Loss: 0.14753  Avg mIoU:  74.99  
[Epoch: 179] [Batch: 0251/0580] Loss: 0.19564  Avg Loss: 0.14669  Avg mIoU:  75.03  
[Epoch: 179] [Batch: 0301/0580] Loss: 0.16434  Avg Loss: 0.14558  Avg mIoU:  75.33  
[Epoch: 179] [Batch: 0351/0580] Loss: 0.15070  Avg Loss: 0.14610  Avg mIoU:  75.15  
[Epoch: 179] [Batch: 0401/0580] Loss: 0.15797  Avg Loss: 0.14690  Avg mIoU:  74.81  
[Epoch: 179] [Batch: 0451/0580] Loss: 0.15859  Avg Loss: 0.14728  Avg mIoU:  74.79  
[Epoch: 179] [Batch: 0501/0580] Loss: 0.10659  Avg Loss: 0.14752  Avg mIoU:  74.83  
[Epoch: 179] [Batch: 0551/0580] Loss: 0.13072  Avg Loss: 0.14747  Avg mIoU:  74.75  

*** Training [@Epoch 179] Avg Loss: 0.14730  Avg mIoU:  74.90  ***

[Epoch: 179] [Batch: 0001/0050] Loss: 0.17162  Avg Loss: 0.17162  Avg mIoU:  57.65  

*** Validation [@Epoch 179] Avg Loss: 0.23779  Avg mIoU:  60.19  ***

[Epoch: 180] [Batch: 0001/0580] Loss: 0.12393  Avg Loss: 0.12393  Avg mIoU:  38.89  
[Epoch: 180] [Batch: 0051/0580] Loss: 0.17070  Avg Loss: 0.15230  Avg mIoU:  74.50  
[Epoch: 180] [Batch: 0101/0580] Loss: 0.17849  Avg Loss: 0.15112  Avg mIoU:  74.90  
[Epoch: 180] [Batch: 0151/0580] Loss: 0.16109  Avg Loss: 0.14765  Avg mIoU:  75.16  
[Epoch: 180] [Batch: 0201/0580] Loss: 0.16319  Avg Loss: 0.14682  Avg mIoU:  75.67  
[Epoch: 180] [Batch: 0251/0580] Loss: 0.15742  Avg Loss: 0.14667  Avg mIoU:  75.43  
[Epoch: 180] [Batch: 0301/0580] Loss: 0.23343  Avg Loss: 0.14756  Avg mIoU:  75.40  
[Epoch: 180] [Batch: 0351/0580] Loss: 0.14109  Avg Loss: 0.14689  Avg mIoU:  75.45  
[Epoch: 180] [Batch: 0401/0580] Loss: 0.10064  Avg Loss: 0.14712  Avg mIoU:  75.41  
[Epoch: 180] [Batch: 0451/0580] Loss: 0.13885  Avg Loss: 0.14713  Avg mIoU:  75.29  
[Epoch: 180] [Batch: 0501/0580] Loss: 0.16169  Avg Loss: 0.14643  Avg mIoU:  75.34  
[Epoch: 180] [Batch: 0551/0580] Loss: 0.19875  Avg Loss: 0.14676  Avg mIoU:  75.32  

*** Training [@Epoch 180] Avg Loss: 0.14719  Avg mIoU:  75.19  ***

[Epoch: 180] [Batch: 0001/0050] Loss: 0.15120  Avg Loss: 0.15120  Avg mIoU:  64.07  

*** Validation [@Epoch 180] Avg Loss: 0.24061  Avg mIoU:  60.91  ***

[Epoch: 181] [Batch: 0001/0580] Loss: 0.12714  Avg Loss: 0.12714  Avg mIoU:  39.79  
[Epoch: 181] [Batch: 0051/0580] Loss: 0.13189  Avg Loss: 0.14565  Avg mIoU:  75.52  
[Epoch: 181] [Batch: 0101/0580] Loss: 0.15064  Avg Loss: 0.15131  Avg mIoU:  74.98  
[Epoch: 181] [Batch: 0151/0580] Loss: 0.15707  Avg Loss: 0.14997  Avg mIoU:  74.86  
[Epoch: 181] [Batch: 0201/0580] Loss: 0.11729  Avg Loss: 0.14844  Avg mIoU:  74.82  
[Epoch: 181] [Batch: 0251/0580] Loss: 0.21351  Avg Loss: 0.15091  Avg mIoU:  74.58  
[Epoch: 181] [Batch: 0301/0580] Loss: 0.13020  Avg Loss: 0.14971  Avg mIoU:  74.69  
[Epoch: 181] [Batch: 0351/0580] Loss: 0.12982  Avg Loss: 0.14902  Avg mIoU:  74.84  
[Epoch: 181] [Batch: 0401/0580] Loss: 0.12518  Avg Loss: 0.14824  Avg mIoU:  74.85  
[Epoch: 181] [Batch: 0451/0580] Loss: 0.13034  Avg Loss: 0.14827  Avg mIoU:  74.78  
[Epoch: 181] [Batch: 0501/0580] Loss: 0.14855  Avg Loss: 0.14818  Avg mIoU:  74.66  
[Epoch: 181] [Batch: 0551/0580] Loss: 0.10922  Avg Loss: 0.14772  Avg mIoU:  74.76  

*** Training [@Epoch 181] Avg Loss: 0.14810  Avg mIoU:  74.69  ***

[Epoch: 181] [Batch: 0001/0050] Loss: 0.18006  Avg Loss: 0.18006  Avg mIoU:  60.89  

*** Validation [@Epoch 181] Avg Loss: 0.25308  Avg mIoU:  61.71  ***

[Epoch: 182] [Batch: 0001/0580] Loss: 0.08885  Avg Loss: 0.08885  Avg mIoU:  57.17  
[Epoch: 182] [Batch: 0051/0580] Loss: 0.16511  Avg Loss: 0.14222  Avg mIoU:  75.84  
[Epoch: 182] [Batch: 0101/0580] Loss: 0.10196  Avg Loss: 0.14560  Avg mIoU:  75.71  
[Epoch: 182] [Batch: 0151/0580] Loss: 0.14993  Avg Loss: 0.14539  Avg mIoU:  75.10  
[Epoch: 182] [Batch: 0201/0580] Loss: 0.21562  Avg Loss: 0.14700  Avg mIoU:  74.82  
[Epoch: 182] [Batch: 0251/0580] Loss: 0.16598  Avg Loss: 0.14581  Avg mIoU:  75.20  
[Epoch: 182] [Batch: 0301/0580] Loss: 0.08622  Avg Loss: 0.14712  Avg mIoU:  75.09  
[Epoch: 182] [Batch: 0351/0580] Loss: 0.11738  Avg Loss: 0.14785  Avg mIoU:  74.83  
[Epoch: 182] [Batch: 0401/0580] Loss: 0.13744  Avg Loss: 0.14654  Avg mIoU:  75.10  
[Epoch: 182] [Batch: 0451/0580] Loss: 0.15561  Avg Loss: 0.14632  Avg mIoU:  75.14  
[Epoch: 182] [Batch: 0501/0580] Loss: 0.16871  Avg Loss: 0.14673  Avg mIoU:  75.32  
[Epoch: 182] [Batch: 0551/0580] Loss: 0.16563  Avg Loss: 0.14683  Avg mIoU:  75.32  

*** Training [@Epoch 182] Avg Loss: 0.14741  Avg mIoU:  75.22  ***

[Epoch: 182] [Batch: 0001/0050] Loss: 0.18256  Avg Loss: 0.18256  Avg mIoU:  55.69  

*** Validation [@Epoch 182] Avg Loss: 0.25133  Avg mIoU:  59.76  ***

[Epoch: 183] [Batch: 0001/0580] Loss: 0.12281  Avg Loss: 0.12281  Avg mIoU:  51.68  
[Epoch: 183] [Batch: 0051/0580] Loss: 0.18919  Avg Loss: 0.15329  Avg mIoU:  74.63  
[Epoch: 183] [Batch: 0101/0580] Loss: 0.11634  Avg Loss: 0.14949  Avg mIoU:  75.50  
[Epoch: 183] [Batch: 0151/0580] Loss: 0.16657  Avg Loss: 0.15061  Avg mIoU:  75.16  
[Epoch: 183] [Batch: 0201/0580] Loss: 0.12937  Avg Loss: 0.14979  Avg mIoU:  74.99  
[Epoch: 183] [Batch: 0251/0580] Loss: 0.14002  Avg Loss: 0.14855  Avg mIoU:  75.22  
[Epoch: 183] [Batch: 0301/0580] Loss: 0.13500  Avg Loss: 0.14748  Avg mIoU:  75.19  
[Epoch: 183] [Batch: 0351/0580] Loss: 0.14217  Avg Loss: 0.14726  Avg mIoU:  75.12  
[Epoch: 183] [Batch: 0401/0580] Loss: 0.13507  Avg Loss: 0.14667  Avg mIoU:  75.15  
[Epoch: 183] [Batch: 0451/0580] Loss: 0.18525  Avg Loss: 0.14761  Avg mIoU:  74.85  
[Epoch: 183] [Batch: 0501/0580] Loss: 0.08910  Avg Loss: 0.14752  Avg mIoU:  75.03  
[Epoch: 183] [Batch: 0551/0580] Loss: 0.17930  Avg Loss: 0.14735  Avg mIoU:  75.00  

*** Training [@Epoch 183] Avg Loss: 0.14747  Avg mIoU:  74.94  ***

[Epoch: 183] [Batch: 0001/0050] Loss: 0.15384  Avg Loss: 0.15384  Avg mIoU:  60.59  

*** Validation [@Epoch 183] Avg Loss: 0.24325  Avg mIoU:  61.11  ***

[Epoch: 184] [Batch: 0001/0580] Loss: 0.11194  Avg Loss: 0.11194  Avg mIoU:  43.50  
[Epoch: 184] [Batch: 0051/0580] Loss: 0.14206  Avg Loss: 0.14257  Avg mIoU:  74.49  
[Epoch: 184] [Batch: 0101/0580] Loss: 0.22652  Avg Loss: 0.14288  Avg mIoU:  75.33  
[Epoch: 184] [Batch: 0151/0580] Loss: 0.17986  Avg Loss: 0.14474  Avg mIoU:  75.68  
[Epoch: 184] [Batch: 0201/0580] Loss: 0.17401  Avg Loss: 0.14485  Avg mIoU:  75.44  
[Epoch: 184] [Batch: 0251/0580] Loss: 0.12240  Avg Loss: 0.14519  Avg mIoU:  75.42  
[Epoch: 184] [Batch: 0301/0580] Loss: 0.13722  Avg Loss: 0.14600  Avg mIoU:  75.17  
[Epoch: 184] [Batch: 0351/0580] Loss: 0.16356  Avg Loss: 0.14686  Avg mIoU:  75.30  
[Epoch: 184] [Batch: 0401/0580] Loss: 0.12662  Avg Loss: 0.14539  Avg mIoU:  75.39  
[Epoch: 184] [Batch: 0451/0580] Loss: 0.17978  Avg Loss: 0.14530  Avg mIoU:  75.20  
[Epoch: 184] [Batch: 0501/0580] Loss: 0.13715  Avg Loss: 0.14540  Avg mIoU:  75.28  
[Epoch: 184] [Batch: 0551/0580] Loss: 0.08992  Avg Loss: 0.14528  Avg mIoU:  75.36  

*** Training [@Epoch 184] Avg Loss: 0.14610  Avg mIoU:  75.31  ***

[Epoch: 184] [Batch: 0001/0050] Loss: 0.15869  Avg Loss: 0.15869  Avg mIoU:  59.76  

*** Validation [@Epoch 184] Avg Loss: 0.24929  Avg mIoU:  60.16  ***

[Epoch: 185] [Batch: 0001/0580] Loss: 0.13375  Avg Loss: 0.13375  Avg mIoU:  44.99  
[Epoch: 185] [Batch: 0051/0580] Loss: 0.15316  Avg Loss: 0.15166  Avg mIoU:  73.13  
[Epoch: 185] [Batch: 0101/0580] Loss: 0.19349  Avg Loss: 0.14783  Avg mIoU:  74.23  
[Epoch: 185] [Batch: 0151/0580] Loss: 0.13024  Avg Loss: 0.14707  Avg mIoU:  74.11  
[Epoch: 185] [Batch: 0201/0580] Loss: 0.14864  Avg Loss: 0.14617  Avg mIoU:  74.58  
[Epoch: 185] [Batch: 0251/0580] Loss: 0.19729  Avg Loss: 0.14560  Avg mIoU:  74.89  
[Epoch: 185] [Batch: 0301/0580] Loss: 0.16180  Avg Loss: 0.14596  Avg mIoU:  74.87  
[Epoch: 185] [Batch: 0351/0580] Loss: 0.14429  Avg Loss: 0.14631  Avg mIoU:  75.00  
[Epoch: 185] [Batch: 0401/0580] Loss: 0.19344  Avg Loss: 0.14631  Avg mIoU:  74.99  
[Epoch: 185] [Batch: 0451/0580] Loss: 0.15286  Avg Loss: 0.14607  Avg mIoU:  74.93  
[Epoch: 185] [Batch: 0501/0580] Loss: 0.11463  Avg Loss: 0.14592  Avg mIoU:  75.01  
[Epoch: 185] [Batch: 0551/0580] Loss: 0.09537  Avg Loss: 0.14623  Avg mIoU:  75.06  

*** Training [@Epoch 185] Avg Loss: 0.14617  Avg mIoU:  75.04  ***

[Epoch: 185] [Batch: 0001/0050] Loss: 0.15641  Avg Loss: 0.15641  Avg mIoU:  59.31  

*** Validation [@Epoch 185] Avg Loss: 0.23910  Avg mIoU:  61.07  ***

[Epoch: 186] [Batch: 0001/0580] Loss: 0.10150  Avg Loss: 0.10150  Avg mIoU:  50.03  
[Epoch: 186] [Batch: 0051/0580] Loss: 0.15599  Avg Loss: 0.13712  Avg mIoU:  75.93  
[Epoch: 186] [Batch: 0101/0580] Loss: 0.20591  Avg Loss: 0.14027  Avg mIoU:  75.49  
[Epoch: 186] [Batch: 0151/0580] Loss: 0.17108  Avg Loss: 0.14128  Avg mIoU:  75.77  
[Epoch: 186] [Batch: 0201/0580] Loss: 0.10934  Avg Loss: 0.14255  Avg mIoU:  75.96  
[Epoch: 186] [Batch: 0251/0580] Loss: 0.11707  Avg Loss: 0.14219  Avg mIoU:  75.91  
[Epoch: 186] [Batch: 0301/0580] Loss: 0.12895  Avg Loss: 0.14339  Avg mIoU:  75.63  
[Epoch: 186] [Batch: 0351/0580] Loss: 0.16586  Avg Loss: 0.14390  Avg mIoU:  75.59  
[Epoch: 186] [Batch: 0401/0580] Loss: 0.12903  Avg Loss: 0.14344  Avg mIoU:  75.62  
[Epoch: 186] [Batch: 0451/0580] Loss: 0.15548  Avg Loss: 0.14414  Avg mIoU:  75.53  
[Epoch: 186] [Batch: 0501/0580] Loss: 0.16174  Avg Loss: 0.14472  Avg mIoU:  75.24  
[Epoch: 186] [Batch: 0551/0580] Loss: 0.16780  Avg Loss: 0.14511  Avg mIoU:  75.28  

*** Training [@Epoch 186] Avg Loss: 0.14546  Avg mIoU:  75.27  ***

[Epoch: 186] [Batch: 0001/0050] Loss: 0.16338  Avg Loss: 0.16338  Avg mIoU:  56.72  

*** Validation [@Epoch 186] Avg Loss: 0.23327  Avg mIoU:  60.66  ***

[Epoch: 187] [Batch: 0001/0580] Loss: 0.17839  Avg Loss: 0.17839  Avg mIoU:  42.53  
[Epoch: 187] [Batch: 0051/0580] Loss: 0.18224  Avg Loss: 0.14520  Avg mIoU:  75.42  
[Epoch: 187] [Batch: 0101/0580] Loss: 0.14164  Avg Loss: 0.14239  Avg mIoU:  75.89  
[Epoch: 187] [Batch: 0151/0580] Loss: 0.10230  Avg Loss: 0.14478  Avg mIoU:  75.12  
[Epoch: 187] [Batch: 0201/0580] Loss: 0.19594  Avg Loss: 0.14579  Avg mIoU:  75.33  
[Epoch: 187] [Batch: 0251/0580] Loss: 0.11393  Avg Loss: 0.14552  Avg mIoU:  75.29  
[Epoch: 187] [Batch: 0301/0580] Loss: 0.14753  Avg Loss: 0.14594  Avg mIoU:  75.14  
[Epoch: 187] [Batch: 0351/0580] Loss: 0.10748  Avg Loss: 0.14482  Avg mIoU:  75.29  
[Epoch: 187] [Batch: 0401/0580] Loss: 0.13296  Avg Loss: 0.14504  Avg mIoU:  75.47  
[Epoch: 187] [Batch: 0451/0580] Loss: 0.16385  Avg Loss: 0.14512  Avg mIoU:  75.49  
[Epoch: 187] [Batch: 0501/0580] Loss: 0.18292  Avg Loss: 0.14553  Avg mIoU:  75.12  
[Epoch: 187] [Batch: 0551/0580] Loss: 0.11105  Avg Loss: 0.14499  Avg mIoU:  75.24  

*** Training [@Epoch 187] Avg Loss: 0.14532  Avg mIoU:  75.24  ***

[Epoch: 187] [Batch: 0001/0050] Loss: 0.16647  Avg Loss: 0.16647  Avg mIoU:  56.37  

*** Validation [@Epoch 187] Avg Loss: 0.24822  Avg mIoU:  59.26  ***

[Epoch: 188] [Batch: 0001/0580] Loss: 0.11034  Avg Loss: 0.11034  Avg mIoU:  44.47  
[Epoch: 188] [Batch: 0051/0580] Loss: 0.15501  Avg Loss: 0.14183  Avg mIoU:  75.68  
[Epoch: 188] [Batch: 0101/0580] Loss: 0.13454  Avg Loss: 0.14680  Avg mIoU:  75.13  
[Epoch: 188] [Batch: 0151/0580] Loss: 0.21163  Avg Loss: 0.14830  Avg mIoU:  74.37  
[Epoch: 188] [Batch: 0201/0580] Loss: 0.10699  Avg Loss: 0.14606  Avg mIoU:  74.58  
[Epoch: 188] [Batch: 0251/0580] Loss: 0.11761  Avg Loss: 0.14771  Avg mIoU:  74.36  
[Epoch: 188] [Batch: 0301/0580] Loss: 0.12005  Avg Loss: 0.14695  Avg mIoU:  74.70  
[Epoch: 188] [Batch: 0351/0580] Loss: 0.09962  Avg Loss: 0.14633  Avg mIoU:  74.70  
[Epoch: 188] [Batch: 0401/0580] Loss: 0.14875  Avg Loss: 0.14551  Avg mIoU:  74.91  
[Epoch: 188] [Batch: 0451/0580] Loss: 0.13555  Avg Loss: 0.14580  Avg mIoU:  74.92  
[Epoch: 188] [Batch: 0501/0580] Loss: 0.16157  Avg Loss: 0.14584  Avg mIoU:  74.87  
[Epoch: 188] [Batch: 0551/0580] Loss: 0.13440  Avg Loss: 0.14563  Avg mIoU:  74.91  

*** Training [@Epoch 188] Avg Loss: 0.14522  Avg mIoU:  75.01  ***

[Epoch: 188] [Batch: 0001/0050] Loss: 0.16017  Avg Loss: 0.16017  Avg mIoU:  59.72  

*** Validation [@Epoch 188] Avg Loss: 0.24777  Avg mIoU:  60.31  ***

[Epoch: 189] [Batch: 0001/0580] Loss: 0.22012  Avg Loss: 0.22012  Avg mIoU:  35.24  
[Epoch: 189] [Batch: 0051/0580] Loss: 0.11543  Avg Loss: 0.14736  Avg mIoU:  73.84  
[Epoch: 189] [Batch: 0101/0580] Loss: 0.17281  Avg Loss: 0.14238  Avg mIoU:  75.07  
[Epoch: 189] [Batch: 0151/0580] Loss: 0.21581  Avg Loss: 0.14241  Avg mIoU:  75.50  
[Epoch: 189] [Batch: 0201/0580] Loss: 0.12665  Avg Loss: 0.14171  Avg mIoU:  75.66  
[Epoch: 189] [Batch: 0251/0580] Loss: 0.10977  Avg Loss: 0.14165  Avg mIoU:  75.85  
[Epoch: 189] [Batch: 0301/0580] Loss: 0.12642  Avg Loss: 0.14385  Avg mIoU:  75.70  
[Epoch: 189] [Batch: 0351/0580] Loss: 0.16064  Avg Loss: 0.14449  Avg mIoU:  75.51  
[Epoch: 189] [Batch: 0401/0580] Loss: 0.15931  Avg Loss: 0.14605  Avg mIoU:  75.25  
[Epoch: 189] [Batch: 0451/0580] Loss: 0.14698  Avg Loss: 0.14648  Avg mIoU:  75.06  
[Epoch: 189] [Batch: 0501/0580] Loss: 0.23323  Avg Loss: 0.14643  Avg mIoU:  74.97  
[Epoch: 189] [Batch: 0551/0580] Loss: 0.11518  Avg Loss: 0.14685  Avg mIoU:  74.87  

*** Training [@Epoch 189] Avg Loss: 0.14666  Avg mIoU:  74.84  ***

[Epoch: 189] [Batch: 0001/0050] Loss: 0.14755  Avg Loss: 0.14755  Avg mIoU:  60.98  

*** Validation [@Epoch 189] Avg Loss: 0.25097  Avg mIoU:  59.18  ***

[Epoch: 190] [Batch: 0001/0580] Loss: 0.18124  Avg Loss: 0.18124  Avg mIoU:  36.88  
[Epoch: 190] [Batch: 0051/0580] Loss: 0.14624  Avg Loss: 0.14188  Avg mIoU:  75.39  
[Epoch: 190] [Batch: 0101/0580] Loss: 0.21394  Avg Loss: 0.14444  Avg mIoU:  74.95  
[Epoch: 190] [Batch: 0151/0580] Loss: 0.17205  Avg Loss: 0.14407  Avg mIoU:  75.44  
[Epoch: 190] [Batch: 0201/0580] Loss: 0.17865  Avg Loss: 0.14265  Avg mIoU:  75.64  
[Epoch: 190] [Batch: 0251/0580] Loss: 0.14039  Avg Loss: 0.14226  Avg mIoU:  75.73  
[Epoch: 190] [Batch: 0301/0580] Loss: 0.13479  Avg Loss: 0.14289  Avg mIoU:  75.75  
[Epoch: 190] [Batch: 0351/0580] Loss: 0.10752  Avg Loss: 0.14297  Avg mIoU:  75.73  
[Epoch: 190] [Batch: 0401/0580] Loss: 0.17967  Avg Loss: 0.14394  Avg mIoU:  75.53  
[Epoch: 190] [Batch: 0451/0580] Loss: 0.13140  Avg Loss: 0.14393  Avg mIoU:  75.46  
[Epoch: 190] [Batch: 0501/0580] Loss: 0.13799  Avg Loss: 0.14479  Avg mIoU:  75.27  
[Epoch: 190] [Batch: 0551/0580] Loss: 0.17767  Avg Loss: 0.14441  Avg mIoU:  75.29  

*** Training [@Epoch 190] Avg Loss: 0.14415  Avg mIoU:  75.38  ***

[Epoch: 190] [Batch: 0001/0050] Loss: 0.16950  Avg Loss: 0.16950  Avg mIoU:  57.35  

*** Validation [@Epoch 190] Avg Loss: 0.24202  Avg mIoU:  59.52  ***

[Epoch: 191] [Batch: 0001/0580] Loss: 0.16246  Avg Loss: 0.16246  Avg mIoU:  45.81  
[Epoch: 191] [Batch: 0051/0580] Loss: 0.10113  Avg Loss: 0.14661  Avg mIoU:  75.46  
[Epoch: 191] [Batch: 0101/0580] Loss: 0.12570  Avg Loss: 0.14230  Avg mIoU:  75.52  
[Epoch: 191] [Batch: 0151/0580] Loss: 0.09913  Avg Loss: 0.14043  Avg mIoU:  75.84  
[Epoch: 191] [Batch: 0201/0580] Loss: 0.16081  Avg Loss: 0.14247  Avg mIoU:  75.88  
[Epoch: 191] [Batch: 0251/0580] Loss: 0.15316  Avg Loss: 0.14245  Avg mIoU:  75.83  
[Epoch: 191] [Batch: 0301/0580] Loss: 0.16398  Avg Loss: 0.14344  Avg mIoU:  75.63  
[Epoch: 191] [Batch: 0351/0580] Loss: 0.15494  Avg Loss: 0.14437  Avg mIoU:  75.57  
[Epoch: 191] [Batch: 0401/0580] Loss: 0.12783  Avg Loss: 0.14378  Avg mIoU:  75.60  
[Epoch: 191] [Batch: 0451/0580] Loss: 0.11870  Avg Loss: 0.14393  Avg mIoU:  75.58  
[Epoch: 191] [Batch: 0501/0580] Loss: 0.20497  Avg Loss: 0.14454  Avg mIoU:  75.49  
[Epoch: 191] [Batch: 0551/0580] Loss: 0.14611  Avg Loss: 0.14448  Avg mIoU:  75.51  

*** Training [@Epoch 191] Avg Loss: 0.14463  Avg mIoU:  75.55  ***

[Epoch: 191] [Batch: 0001/0050] Loss: 0.15218  Avg Loss: 0.15218  Avg mIoU:  63.24  

*** Validation [@Epoch 191] Avg Loss: 0.25303  Avg mIoU:  59.59  ***

[Epoch: 192] [Batch: 0001/0580] Loss: 0.16762  Avg Loss: 0.16762  Avg mIoU:  42.03  
[Epoch: 192] [Batch: 0051/0580] Loss: 0.14834  Avg Loss: 0.13831  Avg mIoU:  77.09  
[Epoch: 192] [Batch: 0101/0580] Loss: 0.15568  Avg Loss: 0.14349  Avg mIoU:  76.07  
[Epoch: 192] [Batch: 0151/0580] Loss: 0.15771  Avg Loss: 0.14349  Avg mIoU:  76.00  
[Epoch: 192] [Batch: 0201/0580] Loss: 0.12938  Avg Loss: 0.14423  Avg mIoU:  75.91  
[Epoch: 192] [Batch: 0251/0580] Loss: 0.13201  Avg Loss: 0.14434  Avg mIoU:  75.60  
[Epoch: 192] [Batch: 0301/0580] Loss: 0.15005  Avg Loss: 0.14459  Avg mIoU:  75.67  
[Epoch: 192] [Batch: 0351/0580] Loss: 0.13773  Avg Loss: 0.14570  Avg mIoU:  75.31  
[Epoch: 192] [Batch: 0401/0580] Loss: 0.21834  Avg Loss: 0.14552  Avg mIoU:  75.37  
[Epoch: 192] [Batch: 0451/0580] Loss: 0.18651  Avg Loss: 0.14575  Avg mIoU:  75.34  
[Epoch: 192] [Batch: 0501/0580] Loss: 0.12664  Avg Loss: 0.14629  Avg mIoU:  75.35  
[Epoch: 192] [Batch: 0551/0580] Loss: 0.10906  Avg Loss: 0.14607  Avg mIoU:  75.21  

*** Training [@Epoch 192] Avg Loss: 0.14585  Avg mIoU:  75.17  ***

[Epoch: 192] [Batch: 0001/0050] Loss: 0.16514  Avg Loss: 0.16514  Avg mIoU:  61.52  

*** Validation [@Epoch 192] Avg Loss: 0.24727  Avg mIoU:  62.11  ***

[Epoch: 193] [Batch: 0001/0580] Loss: 0.23904  Avg Loss: 0.23904  Avg mIoU:  36.85  
[Epoch: 193] [Batch: 0051/0580] Loss: 0.10885  Avg Loss: 0.14361  Avg mIoU:  74.78  
[Epoch: 193] [Batch: 0101/0580] Loss: 0.11357  Avg Loss: 0.14578  Avg mIoU:  74.78  
[Epoch: 193] [Batch: 0151/0580] Loss: 0.18457  Avg Loss: 0.14405  Avg mIoU:  75.20  
[Epoch: 193] [Batch: 0201/0580] Loss: 0.11794  Avg Loss: 0.14232  Avg mIoU:  75.69  
[Epoch: 193] [Batch: 0251/0580] Loss: 0.16386  Avg Loss: 0.14516  Avg mIoU:  75.73  
[Epoch: 193] [Batch: 0301/0580] Loss: 0.13789  Avg Loss: 0.14439  Avg mIoU:  75.87  
[Epoch: 193] [Batch: 0351/0580] Loss: 0.11603  Avg Loss: 0.14437  Avg mIoU:  76.08  
[Epoch: 193] [Batch: 0401/0580] Loss: 0.19078  Avg Loss: 0.14479  Avg mIoU:  75.91  
[Epoch: 193] [Batch: 0451/0580] Loss: 0.14836  Avg Loss: 0.14534  Avg mIoU:  75.79  
[Epoch: 193] [Batch: 0501/0580] Loss: 0.11938  Avg Loss: 0.14558  Avg mIoU:  75.71  
[Epoch: 193] [Batch: 0551/0580] Loss: 0.15304  Avg Loss: 0.14503  Avg mIoU:  75.77  

*** Training [@Epoch 193] Avg Loss: 0.14472  Avg mIoU:  75.74  ***

[Epoch: 193] [Batch: 0001/0050] Loss: 0.16048  Avg Loss: 0.16048  Avg mIoU:  55.90  

*** Validation [@Epoch 193] Avg Loss: 0.25465  Avg mIoU:  59.41  ***

[Epoch: 194] [Batch: 0001/0580] Loss: 0.18923  Avg Loss: 0.18923  Avg mIoU:  41.16  
[Epoch: 194] [Batch: 0051/0580] Loss: 0.13648  Avg Loss: 0.14893  Avg mIoU:  74.51  
[Epoch: 194] [Batch: 0101/0580] Loss: 0.15251  Avg Loss: 0.14652  Avg mIoU:  75.16  
[Epoch: 194] [Batch: 0151/0580] Loss: 0.11020  Avg Loss: 0.14317  Avg mIoU:  75.71  
[Epoch: 194] [Batch: 0201/0580] Loss: 0.15480  Avg Loss: 0.14215  Avg mIoU:  76.08  
[Epoch: 194] [Batch: 0251/0580] Loss: 0.13239  Avg Loss: 0.14149  Avg mIoU:  75.70  
[Epoch: 194] [Batch: 0301/0580] Loss: 0.14717  Avg Loss: 0.14236  Avg mIoU:  75.64  
[Epoch: 194] [Batch: 0351/0580] Loss: 0.13141  Avg Loss: 0.14264  Avg mIoU:  75.59  
[Epoch: 194] [Batch: 0401/0580] Loss: 0.23130  Avg Loss: 0.14358  Avg mIoU:  75.44  
[Epoch: 194] [Batch: 0451/0580] Loss: 0.11639  Avg Loss: 0.14394  Avg mIoU:  75.45  
[Epoch: 194] [Batch: 0501/0580] Loss: 0.13552  Avg Loss: 0.14348  Avg mIoU:  75.53  
[Epoch: 194] [Batch: 0551/0580] Loss: 0.14139  Avg Loss: 0.14371  Avg mIoU:  75.65  

*** Training [@Epoch 194] Avg Loss: 0.14405  Avg mIoU:  75.63  ***

[Epoch: 194] [Batch: 0001/0050] Loss: 0.17456  Avg Loss: 0.17456  Avg mIoU:  63.65  

*** Validation [@Epoch 194] Avg Loss: 0.25880  Avg mIoU:  62.05  ***

[Epoch: 195] [Batch: 0001/0580] Loss: 0.11517  Avg Loss: 0.11517  Avg mIoU:  44.13  
[Epoch: 195] [Batch: 0051/0580] Loss: 0.16034  Avg Loss: 0.14185  Avg mIoU:  76.23  
[Epoch: 195] [Batch: 0101/0580] Loss: 0.25594  Avg Loss: 0.14771  Avg mIoU:  75.78  
[Epoch: 195] [Batch: 0151/0580] Loss: 0.19838  Avg Loss: 0.14843  Avg mIoU:  75.56  
[Epoch: 195] [Batch: 0201/0580] Loss: 0.08416  Avg Loss: 0.14846  Avg mIoU:  75.20  
[Epoch: 195] [Batch: 0251/0580] Loss: 0.15531  Avg Loss: 0.14760  Avg mIoU:  75.09  
[Epoch: 195] [Batch: 0301/0580] Loss: 0.16417  Avg Loss: 0.14585  Avg mIoU:  75.24  
[Epoch: 195] [Batch: 0351/0580] Loss: 0.13119  Avg Loss: 0.14375  Avg mIoU:  75.59  
[Epoch: 195] [Batch: 0401/0580] Loss: 0.15290  Avg Loss: 0.14513  Avg mIoU:  75.27  
[Epoch: 195] [Batch: 0451/0580] Loss: 0.11551  Avg Loss: 0.14471  Avg mIoU:  75.30  
[Epoch: 195] [Batch: 0501/0580] Loss: 0.13049  Avg Loss: 0.14471  Avg mIoU:  75.22  
[Epoch: 195] [Batch: 0551/0580] Loss: 0.14274  Avg Loss: 0.14413  Avg mIoU:  75.38  

*** Training [@Epoch 195] Avg Loss: 0.14418  Avg mIoU:  75.44  ***

[Epoch: 195] [Batch: 0001/0050] Loss: 0.16082  Avg Loss: 0.16082  Avg mIoU:  54.94  

*** Validation [@Epoch 195] Avg Loss: 0.24577  Avg mIoU:  57.45  ***

[Epoch: 196] [Batch: 0001/0580] Loss: 0.20289  Avg Loss: 0.20289  Avg mIoU:  40.78  
[Epoch: 196] [Batch: 0051/0580] Loss: 0.19888  Avg Loss: 0.16259  Avg mIoU:  72.91  
[Epoch: 196] [Batch: 0101/0580] Loss: 0.11321  Avg Loss: 0.15271  Avg mIoU:  74.28  
[Epoch: 196] [Batch: 0151/0580] Loss: 0.12425  Avg Loss: 0.14756  Avg mIoU:  74.61  
[Epoch: 196] [Batch: 0201/0580] Loss: 0.14544  Avg Loss: 0.14651  Avg mIoU:  75.22  
[Epoch: 196] [Batch: 0251/0580] Loss: 0.11928  Avg Loss: 0.14589  Avg mIoU:  75.25  
[Epoch: 196] [Batch: 0301/0580] Loss: 0.10801  Avg Loss: 0.14573  Avg mIoU:  75.41  
[Epoch: 196] [Batch: 0351/0580] Loss: 0.13671  Avg Loss: 0.14618  Avg mIoU:  75.38  
[Epoch: 196] [Batch: 0401/0580] Loss: 0.11005  Avg Loss: 0.14564  Avg mIoU:  75.61  
[Epoch: 196] [Batch: 0451/0580] Loss: 0.08974  Avg Loss: 0.14493  Avg mIoU:  75.69  
[Epoch: 196] [Batch: 0501/0580] Loss: 0.11251  Avg Loss: 0.14472  Avg mIoU:  75.80  
[Epoch: 196] [Batch: 0551/0580] Loss: 0.11985  Avg Loss: 0.14431  Avg mIoU:  75.62  

*** Training [@Epoch 196] Avg Loss: 0.14386  Avg mIoU:  75.67  ***

[Epoch: 196] [Batch: 0001/0050] Loss: 0.16106  Avg Loss: 0.16106  Avg mIoU:  53.13  

*** Validation [@Epoch 196] Avg Loss: 0.26793  Avg mIoU:  54.36  ***

[Epoch: 197] [Batch: 0001/0580] Loss: 0.17800  Avg Loss: 0.17800  Avg mIoU:  44.21  
[Epoch: 197] [Batch: 0051/0580] Loss: 0.14094  Avg Loss: 0.14054  Avg mIoU:  74.42  
[Epoch: 197] [Batch: 0101/0580] Loss: 0.10376  Avg Loss: 0.14291  Avg mIoU:  75.81  
[Epoch: 197] [Batch: 0151/0580] Loss: 0.12696  Avg Loss: 0.13852  Avg mIoU:  76.18  
[Epoch: 197] [Batch: 0201/0580] Loss: 0.14463  Avg Loss: 0.14003  Avg mIoU:  76.21  
[Epoch: 197] [Batch: 0251/0580] Loss: 0.09494  Avg Loss: 0.14135  Avg mIoU:  75.77  
[Epoch: 197] [Batch: 0301/0580] Loss: 0.09580  Avg Loss: 0.14186  Avg mIoU:  75.85  
[Epoch: 197] [Batch: 0351/0580] Loss: 0.10231  Avg Loss: 0.14254  Avg mIoU:  75.55  
[Epoch: 197] [Batch: 0401/0580] Loss: 0.10430  Avg Loss: 0.14302  Avg mIoU:  75.34  
[Epoch: 197] [Batch: 0451/0580] Loss: 0.12207  Avg Loss: 0.14336  Avg mIoU:  75.23  
[Epoch: 197] [Batch: 0501/0580] Loss: 0.31708  Avg Loss: 0.14321  Avg mIoU:  75.31  
[Epoch: 197] [Batch: 0551/0580] Loss: 0.09736  Avg Loss: 0.14354  Avg mIoU:  75.32  

*** Training [@Epoch 197] Avg Loss: 0.14401  Avg mIoU:  75.38  ***

[Epoch: 197] [Batch: 0001/0050] Loss: 0.15157  Avg Loss: 0.15157  Avg mIoU:  56.07  

*** Validation [@Epoch 197] Avg Loss: 0.24434  Avg mIoU:  57.87  ***

[Epoch: 198] [Batch: 0001/0580] Loss: 0.13156  Avg Loss: 0.13156  Avg mIoU:  40.47  
[Epoch: 198] [Batch: 0051/0580] Loss: 0.11466  Avg Loss: 0.14292  Avg mIoU:  75.03  
[Epoch: 198] [Batch: 0101/0580] Loss: 0.13988  Avg Loss: 0.14572  Avg mIoU:  74.95  
[Epoch: 198] [Batch: 0151/0580] Loss: 0.12875  Avg Loss: 0.14662  Avg mIoU:  75.44  
[Epoch: 198] [Batch: 0201/0580] Loss: 0.13121  Avg Loss: 0.14557  Avg mIoU:  75.62  
[Epoch: 198] [Batch: 0251/0580] Loss: 0.16975  Avg Loss: 0.14445  Avg mIoU:  75.88  
[Epoch: 198] [Batch: 0301/0580] Loss: 0.09137  Avg Loss: 0.14365  Avg mIoU:  75.79  
[Epoch: 198] [Batch: 0351/0580] Loss: 0.12356  Avg Loss: 0.14250  Avg mIoU:  75.85  
[Epoch: 198] [Batch: 0401/0580] Loss: 0.21068  Avg Loss: 0.14304  Avg mIoU:  76.01  
[Epoch: 198] [Batch: 0451/0580] Loss: 0.13207  Avg Loss: 0.14338  Avg mIoU:  75.89  
[Epoch: 198] [Batch: 0501/0580] Loss: 0.11764  Avg Loss: 0.14227  Avg mIoU:  75.97  
[Epoch: 198] [Batch: 0551/0580] Loss: 0.14190  Avg Loss: 0.14240  Avg mIoU:  75.87  

*** Training [@Epoch 198] Avg Loss: 0.14301  Avg mIoU:  75.78  ***

[Epoch: 198] [Batch: 0001/0050] Loss: 0.16520  Avg Loss: 0.16520  Avg mIoU:  59.60  

*** Validation [@Epoch 198] Avg Loss: 0.23869  Avg mIoU:  63.13  ***

Model saved @198 w/ val. mIoU: 63.13.

[Epoch: 199] [Batch: 0001/0580] Loss: 0.13419  Avg Loss: 0.13419  Avg mIoU:  50.27  
[Epoch: 199] [Batch: 0051/0580] Loss: 0.09987  Avg Loss: 0.13848  Avg mIoU:  75.32  
[Epoch: 199] [Batch: 0101/0580] Loss: 0.17339  Avg Loss: 0.14298  Avg mIoU:  75.71  
[Epoch: 199] [Batch: 0151/0580] Loss: 0.17240  Avg Loss: 0.14238  Avg mIoU:  76.20  
[Epoch: 199] [Batch: 0201/0580] Loss: 0.11341  Avg Loss: 0.14204  Avg mIoU:  76.83  
[Epoch: 199] [Batch: 0251/0580] Loss: 0.17770  Avg Loss: 0.14170  Avg mIoU:  76.55  
[Epoch: 199] [Batch: 0301/0580] Loss: 0.25484  Avg Loss: 0.14366  Avg mIoU:  75.85  
[Epoch: 199] [Batch: 0351/0580] Loss: 0.14261  Avg Loss: 0.14422  Avg mIoU:  75.57  
[Epoch: 199] [Batch: 0401/0580] Loss: 0.12337  Avg Loss: 0.14411  Avg mIoU:  75.54  
[Epoch: 199] [Batch: 0451/0580] Loss: 0.17225  Avg Loss: 0.14271  Avg mIoU:  75.87  
[Epoch: 199] [Batch: 0501/0580] Loss: 0.12173  Avg Loss: 0.14163  Avg mIoU:  76.00  
[Epoch: 199] [Batch: 0551/0580] Loss: 0.16569  Avg Loss: 0.14197  Avg mIoU:  75.93  

*** Training [@Epoch 199] Avg Loss: 0.14210  Avg mIoU:  75.94  ***

[Epoch: 199] [Batch: 0001/0050] Loss: 0.16878  Avg Loss: 0.16878  Avg mIoU:  54.61  

*** Validation [@Epoch 199] Avg Loss: 0.24485  Avg mIoU:  57.42  ***

[Epoch: 200] [Batch: 0001/0580] Loss: 0.12888  Avg Loss: 0.12888  Avg mIoU:  42.79  
[Epoch: 200] [Batch: 0051/0580] Loss: 0.13763  Avg Loss: 0.14851  Avg mIoU:  74.36  
[Epoch: 200] [Batch: 0101/0580] Loss: 0.11568  Avg Loss: 0.14092  Avg mIoU:  75.45  
[Epoch: 200] [Batch: 0151/0580] Loss: 0.21943  Avg Loss: 0.14596  Avg mIoU:  75.45  
[Epoch: 200] [Batch: 0201/0580] Loss: 0.20179  Avg Loss: 0.14410  Avg mIoU:  75.67  
[Epoch: 200] [Batch: 0251/0580] Loss: 0.14609  Avg Loss: 0.14454  Avg mIoU:  75.78  
[Epoch: 200] [Batch: 0301/0580] Loss: 0.20826  Avg Loss: 0.14532  Avg mIoU:  75.50  
[Epoch: 200] [Batch: 0351/0580] Loss: 0.14628  Avg Loss: 0.14430  Avg mIoU:  75.83  
[Epoch: 200] [Batch: 0401/0580] Loss: 0.10617  Avg Loss: 0.14412  Avg mIoU:  75.97  
[Epoch: 200] [Batch: 0451/0580] Loss: 0.12880  Avg Loss: 0.14373  Avg mIoU:  75.92  
[Epoch: 200] [Batch: 0501/0580] Loss: 0.18777  Avg Loss: 0.14376  Avg mIoU:  75.83  
[Epoch: 200] [Batch: 0551/0580] Loss: 0.20942  Avg Loss: 0.14363  Avg mIoU:  75.70  

*** Training [@Epoch 200] Avg Loss: 0.14322  Avg mIoU:  75.80  ***

[Epoch: 200] [Batch: 0001/0050] Loss: 0.15694  Avg Loss: 0.15694  Avg mIoU:  56.01  

*** Validation [@Epoch 200] Avg Loss: 0.24343  Avg mIoU:  60.20  ***

[Epoch: 201] [Batch: 0001/0580] Loss: 0.11041  Avg Loss: 0.11041  Avg mIoU:  43.27  
[Epoch: 201] [Batch: 0051/0580] Loss: 0.12903  Avg Loss: 0.13678  Avg mIoU:  76.76  
[Epoch: 201] [Batch: 0101/0580] Loss: 0.12817  Avg Loss: 0.13776  Avg mIoU:  76.04  
[Epoch: 201] [Batch: 0151/0580] Loss: 0.19690  Avg Loss: 0.13943  Avg mIoU:  75.53  
[Epoch: 201] [Batch: 0201/0580] Loss: 0.10074  Avg Loss: 0.14002  Avg mIoU:  75.41  
[Epoch: 201] [Batch: 0251/0580] Loss: 0.12941  Avg Loss: 0.14233  Avg mIoU:  75.39  
[Epoch: 201] [Batch: 0301/0580] Loss: 0.13551  Avg Loss: 0.14152  Avg mIoU:  75.36  
[Epoch: 201] [Batch: 0351/0580] Loss: 0.12444  Avg Loss: 0.14098  Avg mIoU:  75.49  
[Epoch: 201] [Batch: 0401/0580] Loss: 0.15723  Avg Loss: 0.14068  Avg mIoU:  75.53  
[Epoch: 201] [Batch: 0451/0580] Loss: 0.12642  Avg Loss: 0.14185  Avg mIoU:  75.39  
[Epoch: 201] [Batch: 0501/0580] Loss: 0.16124  Avg Loss: 0.14285  Avg mIoU:  75.50  
[Epoch: 201] [Batch: 0551/0580] Loss: 0.11983  Avg Loss: 0.14245  Avg mIoU:  75.53  

*** Training [@Epoch 201] Avg Loss: 0.14300  Avg mIoU:  75.53  ***

[Epoch: 201] [Batch: 0001/0050] Loss: 0.18076  Avg Loss: 0.18076  Avg mIoU:  52.56  

*** Validation [@Epoch 201] Avg Loss: 0.25239  Avg mIoU:  58.88  ***

[Epoch: 202] [Batch: 0001/0580] Loss: 0.13924  Avg Loss: 0.13924  Avg mIoU:  39.35  
[Epoch: 202] [Batch: 0051/0580] Loss: 0.11879  Avg Loss: 0.13095  Avg mIoU:  76.71  
[Epoch: 202] [Batch: 0101/0580] Loss: 0.17005  Avg Loss: 0.13532  Avg mIoU:  76.55  
[Epoch: 202] [Batch: 0151/0580] Loss: 0.15194  Avg Loss: 0.13706  Avg mIoU:  76.16  
[Epoch: 202] [Batch: 0201/0580] Loss: 0.10687  Avg Loss: 0.13829  Avg mIoU:  75.86  
[Epoch: 202] [Batch: 0251/0580] Loss: 0.14630  Avg Loss: 0.13991  Avg mIoU:  75.75  
[Epoch: 202] [Batch: 0301/0580] Loss: 0.16770  Avg Loss: 0.14082  Avg mIoU:  75.65  
[Epoch: 202] [Batch: 0351/0580] Loss: 0.16077  Avg Loss: 0.13993  Avg mIoU:  75.79  
[Epoch: 202] [Batch: 0401/0580] Loss: 0.16565  Avg Loss: 0.14062  Avg mIoU:  75.76  
[Epoch: 202] [Batch: 0451/0580] Loss: 0.13986  Avg Loss: 0.14117  Avg mIoU:  75.49  
[Epoch: 202] [Batch: 0501/0580] Loss: 0.11033  Avg Loss: 0.14150  Avg mIoU:  75.55  
[Epoch: 202] [Batch: 0551/0580] Loss: 0.14479  Avg Loss: 0.14226  Avg mIoU:  75.57  

*** Training [@Epoch 202] Avg Loss: 0.14207  Avg mIoU:  75.66  ***

[Epoch: 202] [Batch: 0001/0050] Loss: 0.15449  Avg Loss: 0.15449  Avg mIoU:  57.61  

*** Validation [@Epoch 202] Avg Loss: 0.25211  Avg mIoU:  59.27  ***

[Epoch: 203] [Batch: 0001/0580] Loss: 0.11466  Avg Loss: 0.11466  Avg mIoU:  41.62  
[Epoch: 203] [Batch: 0051/0580] Loss: 0.15304  Avg Loss: 0.13727  Avg mIoU:  77.61  
[Epoch: 203] [Batch: 0101/0580] Loss: 0.15457  Avg Loss: 0.14055  Avg mIoU:  76.52  
[Epoch: 203] [Batch: 0151/0580] Loss: 0.11478  Avg Loss: 0.14179  Avg mIoU:  75.73  
[Epoch: 203] [Batch: 0201/0580] Loss: 0.19706  Avg Loss: 0.14260  Avg mIoU:  75.38  
[Epoch: 203] [Batch: 0251/0580] Loss: 0.11949  Avg Loss: 0.14312  Avg mIoU:  75.38  
[Epoch: 203] [Batch: 0301/0580] Loss: 0.09813  Avg Loss: 0.14362  Avg mIoU:  75.40  
[Epoch: 203] [Batch: 0351/0580] Loss: 0.18480  Avg Loss: 0.14426  Avg mIoU:  75.12  
[Epoch: 203] [Batch: 0401/0580] Loss: 0.18639  Avg Loss: 0.14488  Avg mIoU:  74.97  
[Epoch: 203] [Batch: 0451/0580] Loss: 0.13739  Avg Loss: 0.14378  Avg mIoU:  74.96  
[Epoch: 203] [Batch: 0501/0580] Loss: 0.09329  Avg Loss: 0.14458  Avg mIoU:  74.87  
[Epoch: 203] [Batch: 0551/0580] Loss: 0.12876  Avg Loss: 0.14472  Avg mIoU:  74.99  

*** Training [@Epoch 203] Avg Loss: 0.14468  Avg mIoU:  75.06  ***

[Epoch: 203] [Batch: 0001/0050] Loss: 0.17250  Avg Loss: 0.17250  Avg mIoU:  56.84  

*** Validation [@Epoch 203] Avg Loss: 0.23979  Avg mIoU:  59.58  ***

[Epoch: 204] [Batch: 0001/0580] Loss: 0.13488  Avg Loss: 0.13488  Avg mIoU:  51.07  
[Epoch: 204] [Batch: 0051/0580] Loss: 0.12709  Avg Loss: 0.14641  Avg mIoU:  75.11  
[Epoch: 204] [Batch: 0101/0580] Loss: 0.10936  Avg Loss: 0.14519  Avg mIoU:  75.08  
[Epoch: 204] [Batch: 0151/0580] Loss: 0.15252  Avg Loss: 0.14655  Avg mIoU:  74.78  
[Epoch: 204] [Batch: 0201/0580] Loss: 0.17747  Avg Loss: 0.14463  Avg mIoU:  75.14  
[Epoch: 204] [Batch: 0251/0580] Loss: 0.11280  Avg Loss: 0.14332  Avg mIoU:  76.06  
[Epoch: 204] [Batch: 0301/0580] Loss: 0.15037  Avg Loss: 0.14363  Avg mIoU:  75.96  
[Epoch: 204] [Batch: 0351/0580] Loss: 0.11634  Avg Loss: 0.14363  Avg mIoU:  75.66  
[Epoch: 204] [Batch: 0401/0580] Loss: 0.21293  Avg Loss: 0.14246  Avg mIoU:  75.85  
[Epoch: 204] [Batch: 0451/0580] Loss: 0.14063  Avg Loss: 0.14289  Avg mIoU:  75.76  
[Epoch: 204] [Batch: 0501/0580] Loss: 0.05654  Avg Loss: 0.14337  Avg mIoU:  75.74  
[Epoch: 204] [Batch: 0551/0580] Loss: 0.15172  Avg Loss: 0.14236  Avg mIoU:  75.84  

*** Training [@Epoch 204] Avg Loss: 0.14177  Avg mIoU:  75.89  ***

[Epoch: 204] [Batch: 0001/0050] Loss: 0.15175  Avg Loss: 0.15175  Avg mIoU:  63.93  

*** Validation [@Epoch 204] Avg Loss: 0.25928  Avg mIoU:  61.91  ***

[Epoch: 205] [Batch: 0001/0580] Loss: 0.11086  Avg Loss: 0.11086  Avg mIoU:  35.74  
[Epoch: 205] [Batch: 0051/0580] Loss: 0.13638  Avg Loss: 0.13509  Avg mIoU:  76.42  
[Epoch: 205] [Batch: 0101/0580] Loss: 0.10072  Avg Loss: 0.13910  Avg mIoU:  75.05  
[Epoch: 205] [Batch: 0151/0580] Loss: 0.09521  Avg Loss: 0.13942  Avg mIoU:  75.00  
[Epoch: 205] [Batch: 0201/0580] Loss: 0.10674  Avg Loss: 0.14016  Avg mIoU:  75.55  
[Epoch: 205] [Batch: 0251/0580] Loss: 0.22317  Avg Loss: 0.13970  Avg mIoU:  75.79  
[Epoch: 205] [Batch: 0301/0580] Loss: 0.12094  Avg Loss: 0.13888  Avg mIoU:  76.05  
[Epoch: 205] [Batch: 0351/0580] Loss: 0.15033  Avg Loss: 0.13944  Avg mIoU:  75.95  
[Epoch: 205] [Batch: 0401/0580] Loss: 0.14077  Avg Loss: 0.14105  Avg mIoU:  75.82  
[Epoch: 205] [Batch: 0451/0580] Loss: 0.15274  Avg Loss: 0.14216  Avg mIoU:  75.67  
[Epoch: 205] [Batch: 0501/0580] Loss: 0.10380  Avg Loss: 0.14290  Avg mIoU:  75.60  
[Epoch: 205] [Batch: 0551/0580] Loss: 0.17990  Avg Loss: 0.14310  Avg mIoU:  75.48  

*** Training [@Epoch 205] Avg Loss: 0.14299  Avg mIoU:  75.58  ***

[Epoch: 205] [Batch: 0001/0050] Loss: 0.16360  Avg Loss: 0.16360  Avg mIoU:  59.14  

*** Validation [@Epoch 205] Avg Loss: 0.24654  Avg mIoU:  61.07  ***

[Epoch: 206] [Batch: 0001/0580] Loss: 0.12267  Avg Loss: 0.12267  Avg mIoU:  42.91  
[Epoch: 206] [Batch: 0051/0580] Loss: 0.09557  Avg Loss: 0.13983  Avg mIoU:  75.15  
[Epoch: 206] [Batch: 0101/0580] Loss: 0.14250  Avg Loss: 0.13507  Avg mIoU:  75.94  
[Epoch: 206] [Batch: 0151/0580] Loss: 0.21560  Avg Loss: 0.13827  Avg mIoU:  75.69  
[Epoch: 206] [Batch: 0201/0580] Loss: 0.17616  Avg Loss: 0.13871  Avg mIoU:  76.03  
[Epoch: 206] [Batch: 0251/0580] Loss: 0.14498  Avg Loss: 0.13831  Avg mIoU:  75.75  
[Epoch: 206] [Batch: 0301/0580] Loss: 0.13508  Avg Loss: 0.14052  Avg mIoU:  75.44  
[Epoch: 206] [Batch: 0351/0580] Loss: 0.15006  Avg Loss: 0.14047  Avg mIoU:  75.58  
[Epoch: 206] [Batch: 0401/0580] Loss: 0.21498  Avg Loss: 0.14068  Avg mIoU:  75.76  
[Epoch: 206] [Batch: 0451/0580] Loss: 0.14497  Avg Loss: 0.14004  Avg mIoU:  76.00  
[Epoch: 206] [Batch: 0501/0580] Loss: 0.12463  Avg Loss: 0.14022  Avg mIoU:  75.96  
[Epoch: 206] [Batch: 0551/0580] Loss: 0.17284  Avg Loss: 0.14141  Avg mIoU:  75.85  

*** Training [@Epoch 206] Avg Loss: 0.14171  Avg mIoU:  75.82  ***

[Epoch: 206] [Batch: 0001/0050] Loss: 0.20440  Avg Loss: 0.20440  Avg mIoU:  61.91  

*** Validation [@Epoch 206] Avg Loss: 0.30176  Avg mIoU:  61.45  ***

[Epoch: 207] [Batch: 0001/0580] Loss: 0.10884  Avg Loss: 0.10884  Avg mIoU:  65.80  
[Epoch: 207] [Batch: 0051/0580] Loss: 0.15604  Avg Loss: 0.14447  Avg mIoU:  75.02  
[Epoch: 207] [Batch: 0101/0580] Loss: 0.09456  Avg Loss: 0.13791  Avg mIoU:  76.03  
[Epoch: 207] [Batch: 0151/0580] Loss: 0.10716  Avg Loss: 0.14111  Avg mIoU:  75.30  
[Epoch: 207] [Batch: 0201/0580] Loss: 0.12419  Avg Loss: 0.14168  Avg mIoU:  75.27  
[Epoch: 207] [Batch: 0251/0580] Loss: 0.09866  Avg Loss: 0.14247  Avg mIoU:  75.87  
[Epoch: 207] [Batch: 0301/0580] Loss: 0.18939  Avg Loss: 0.14227  Avg mIoU:  75.60  
[Epoch: 207] [Batch: 0351/0580] Loss: 0.11507  Avg Loss: 0.14172  Avg mIoU:  75.95  
[Epoch: 207] [Batch: 0401/0580] Loss: 0.08750  Avg Loss: 0.14235  Avg mIoU:  75.85  
[Epoch: 207] [Batch: 0451/0580] Loss: 0.11279  Avg Loss: 0.14367  Avg mIoU:  75.41  
[Epoch: 207] [Batch: 0501/0580] Loss: 0.14317  Avg Loss: 0.14340  Avg mIoU:  75.35  
[Epoch: 207] [Batch: 0551/0580] Loss: 0.10189  Avg Loss: 0.14293  Avg mIoU:  75.56  

*** Training [@Epoch 207] Avg Loss: 0.14255  Avg mIoU:  75.56  ***

[Epoch: 207] [Batch: 0001/0050] Loss: 0.16519  Avg Loss: 0.16519  Avg mIoU:  56.89  

*** Validation [@Epoch 207] Avg Loss: 0.25373  Avg mIoU:  58.55  ***

[Epoch: 208] [Batch: 0001/0580] Loss: 0.10521  Avg Loss: 0.10521  Avg mIoU:  43.11  
[Epoch: 208] [Batch: 0051/0580] Loss: 0.16032  Avg Loss: 0.13438  Avg mIoU:  75.08  
[Epoch: 208] [Batch: 0101/0580] Loss: 0.10269  Avg Loss: 0.13787  Avg mIoU:  76.13  
[Epoch: 208] [Batch: 0151/0580] Loss: 0.23265  Avg Loss: 0.13933  Avg mIoU:  76.13  
[Epoch: 208] [Batch: 0201/0580] Loss: 0.11892  Avg Loss: 0.13846  Avg mIoU:  76.32  
[Epoch: 208] [Batch: 0251/0580] Loss: 0.11374  Avg Loss: 0.13948  Avg mIoU:  76.48  
[Epoch: 208] [Batch: 0301/0580] Loss: 0.10185  Avg Loss: 0.13966  Avg mIoU:  76.46  
[Epoch: 208] [Batch: 0351/0580] Loss: 0.10527  Avg Loss: 0.14074  Avg mIoU:  76.42  
[Epoch: 208] [Batch: 0401/0580] Loss: 0.09731  Avg Loss: 0.14019  Avg mIoU:  76.29  
[Epoch: 208] [Batch: 0451/0580] Loss: 0.10157  Avg Loss: 0.14039  Avg mIoU:  76.26  
[Epoch: 208] [Batch: 0501/0580] Loss: 0.12083  Avg Loss: 0.14025  Avg mIoU:  76.29  
[Epoch: 208] [Batch: 0551/0580] Loss: 0.16068  Avg Loss: 0.13962  Avg mIoU:  76.18  

*** Training [@Epoch 208] Avg Loss: 0.13961  Avg mIoU:  76.16  ***

[Epoch: 208] [Batch: 0001/0050] Loss: 0.15815  Avg Loss: 0.15815  Avg mIoU:  57.06  

*** Validation [@Epoch 208] Avg Loss: 0.24419  Avg mIoU:  59.00  ***

[Epoch: 209] [Batch: 0001/0580] Loss: 0.13136  Avg Loss: 0.13136  Avg mIoU:  79.73  
[Epoch: 209] [Batch: 0051/0580] Loss: 0.12746  Avg Loss: 0.13252  Avg mIoU:  77.97  
[Epoch: 209] [Batch: 0101/0580] Loss: 0.14285  Avg Loss: 0.13852  Avg mIoU:  76.85  
[Epoch: 209] [Batch: 0151/0580] Loss: 0.11017  Avg Loss: 0.13928  Avg mIoU:  76.83  
[Epoch: 209] [Batch: 0201/0580] Loss: 0.09655  Avg Loss: 0.13960  Avg mIoU:  76.80  
[Epoch: 209] [Batch: 0251/0580] Loss: 0.15763  Avg Loss: 0.13916  Avg mIoU:  76.65  
[Epoch: 209] [Batch: 0301/0580] Loss: 0.12183  Avg Loss: 0.14008  Avg mIoU:  76.56  
[Epoch: 209] [Batch: 0351/0580] Loss: 0.15545  Avg Loss: 0.14030  Avg mIoU:  76.48  
[Epoch: 209] [Batch: 0401/0580] Loss: 0.10668  Avg Loss: 0.14024  Avg mIoU:  76.32  
[Epoch: 209] [Batch: 0451/0580] Loss: 0.14091  Avg Loss: 0.14117  Avg mIoU:  76.22  
[Epoch: 209] [Batch: 0501/0580] Loss: 0.13149  Avg Loss: 0.14027  Avg mIoU:  76.10  
[Epoch: 209] [Batch: 0551/0580] Loss: 0.11825  Avg Loss: 0.14036  Avg mIoU:  75.98  

*** Training [@Epoch 209] Avg Loss: 0.14050  Avg mIoU:  75.99  ***

[Epoch: 209] [Batch: 0001/0050] Loss: 0.16345  Avg Loss: 0.16345  Avg mIoU:  55.84  

*** Validation [@Epoch 209] Avg Loss: 0.23340  Avg mIoU:  58.88  ***

[Epoch: 210] [Batch: 0001/0580] Loss: 0.14009  Avg Loss: 0.14009  Avg mIoU:  46.04  
[Epoch: 210] [Batch: 0051/0580] Loss: 0.11960  Avg Loss: 0.14553  Avg mIoU:  76.39  
[Epoch: 210] [Batch: 0101/0580] Loss: 0.12920  Avg Loss: 0.14480  Avg mIoU:  76.04  
[Epoch: 210] [Batch: 0151/0580] Loss: 0.12265  Avg Loss: 0.14402  Avg mIoU:  75.90  
[Epoch: 210] [Batch: 0201/0580] Loss: 0.11889  Avg Loss: 0.14353  Avg mIoU:  75.98  
[Epoch: 210] [Batch: 0251/0580] Loss: 0.13445  Avg Loss: 0.14238  Avg mIoU:  76.33  
[Epoch: 210] [Batch: 0301/0580] Loss: 0.14802  Avg Loss: 0.14290  Avg mIoU:  76.30  
[Epoch: 210] [Batch: 0351/0580] Loss: 0.12119  Avg Loss: 0.14271  Avg mIoU:  76.24  
[Epoch: 210] [Batch: 0401/0580] Loss: 0.11682  Avg Loss: 0.14227  Avg mIoU:  76.11  
[Epoch: 210] [Batch: 0451/0580] Loss: 0.13003  Avg Loss: 0.14321  Avg mIoU:  76.06  
[Epoch: 210] [Batch: 0501/0580] Loss: 0.14088  Avg Loss: 0.14315  Avg mIoU:  75.99  
[Epoch: 210] [Batch: 0551/0580] Loss: 0.13783  Avg Loss: 0.14342  Avg mIoU:  75.91  

*** Training [@Epoch 210] Avg Loss: 0.14345  Avg mIoU:  75.75  ***

[Epoch: 210] [Batch: 0001/0050] Loss: 0.15333  Avg Loss: 0.15333  Avg mIoU:  61.24  

*** Validation [@Epoch 210] Avg Loss: 0.25114  Avg mIoU:  60.81  ***

[Epoch: 211] [Batch: 0001/0580] Loss: 0.15714  Avg Loss: 0.15714  Avg mIoU:  41.90  
[Epoch: 211] [Batch: 0051/0580] Loss: 0.13110  Avg Loss: 0.13088  Avg mIoU:  77.53  
[Epoch: 211] [Batch: 0101/0580] Loss: 0.19432  Avg Loss: 0.13358  Avg mIoU:  77.19  
[Epoch: 211] [Batch: 0151/0580] Loss: 0.11692  Avg Loss: 0.13799  Avg mIoU:  76.64  
[Epoch: 211] [Batch: 0201/0580] Loss: 0.13178  Avg Loss: 0.14060  Avg mIoU:  76.39  
[Epoch: 211] [Batch: 0251/0580] Loss: 0.17195  Avg Loss: 0.14241  Avg mIoU:  75.88  
[Epoch: 211] [Batch: 0301/0580] Loss: 0.15359  Avg Loss: 0.14200  Avg mIoU:  75.78  
[Epoch: 211] [Batch: 0351/0580] Loss: 0.17806  Avg Loss: 0.14259  Avg mIoU:  75.76  
[Epoch: 211] [Batch: 0401/0580] Loss: 0.16181  Avg Loss: 0.14271  Avg mIoU:  75.66  
[Epoch: 211] [Batch: 0451/0580] Loss: 0.16051  Avg Loss: 0.14232  Avg mIoU:  75.73  
[Epoch: 211] [Batch: 0501/0580] Loss: 0.12802  Avg Loss: 0.14112  Avg mIoU:  75.64  
[Epoch: 211] [Batch: 0551/0580] Loss: 0.16773  Avg Loss: 0.14212  Avg mIoU:  75.58  

*** Training [@Epoch 211] Avg Loss: 0.14236  Avg mIoU:  75.60  ***

[Epoch: 211] [Batch: 0001/0050] Loss: 0.16775  Avg Loss: 0.16775  Avg mIoU:  62.12  

*** Validation [@Epoch 211] Avg Loss: 0.26035  Avg mIoU:  61.72  ***

[Epoch: 212] [Batch: 0001/0580] Loss: 0.20260  Avg Loss: 0.20260  Avg mIoU:  39.00  
[Epoch: 212] [Batch: 0051/0580] Loss: 0.13407  Avg Loss: 0.14164  Avg mIoU:  76.40  
[Epoch: 212] [Batch: 0101/0580] Loss: 0.16712  Avg Loss: 0.14096  Avg mIoU:  76.65  
[Epoch: 212] [Batch: 0151/0580] Loss: 0.16262  Avg Loss: 0.14142  Avg mIoU:  76.27  
[Epoch: 212] [Batch: 0201/0580] Loss: 0.14108  Avg Loss: 0.14149  Avg mIoU:  76.21  
[Epoch: 212] [Batch: 0251/0580] Loss: 0.11910  Avg Loss: 0.14007  Avg mIoU:  76.35  
[Epoch: 212] [Batch: 0301/0580] Loss: 0.11413  Avg Loss: 0.14027  Avg mIoU:  76.38  
[Epoch: 212] [Batch: 0351/0580] Loss: 0.13298  Avg Loss: 0.14022  Avg mIoU:  76.37  
[Epoch: 212] [Batch: 0401/0580] Loss: 0.13105  Avg Loss: 0.14018  Avg mIoU:  76.23  
[Epoch: 212] [Batch: 0451/0580] Loss: 0.11448  Avg Loss: 0.13892  Avg mIoU:  76.35  
[Epoch: 212] [Batch: 0501/0580] Loss: 0.15383  Avg Loss: 0.13894  Avg mIoU:  76.23  
[Epoch: 212] [Batch: 0551/0580] Loss: 0.11806  Avg Loss: 0.13940  Avg mIoU:  76.14  

*** Training [@Epoch 212] Avg Loss: 0.13923  Avg mIoU:  76.17  ***

[Epoch: 212] [Batch: 0001/0050] Loss: 0.14833  Avg Loss: 0.14833  Avg mIoU:  63.61  

*** Validation [@Epoch 212] Avg Loss: 0.25601  Avg mIoU:  61.67  ***

[Epoch: 213] [Batch: 0001/0580] Loss: 0.13500  Avg Loss: 0.13500  Avg mIoU:  53.93  
[Epoch: 213] [Batch: 0051/0580] Loss: 0.07935  Avg Loss: 0.13578  Avg mIoU:  77.84  
[Epoch: 213] [Batch: 0101/0580] Loss: 0.14480  Avg Loss: 0.13956  Avg mIoU:  76.38  
[Epoch: 213] [Batch: 0151/0580] Loss: 0.16949  Avg Loss: 0.14053  Avg mIoU:  76.39  
[Epoch: 213] [Batch: 0201/0580] Loss: 0.08757  Avg Loss: 0.13940  Avg mIoU:  76.14  
[Epoch: 213] [Batch: 0251/0580] Loss: 0.12004  Avg Loss: 0.13796  Avg mIoU:  76.32  
[Epoch: 213] [Batch: 0301/0580] Loss: 0.18902  Avg Loss: 0.13777  Avg mIoU:  76.38  
[Epoch: 213] [Batch: 0351/0580] Loss: 0.20511  Avg Loss: 0.13810  Avg mIoU:  76.43  
[Epoch: 213] [Batch: 0401/0580] Loss: 0.14760  Avg Loss: 0.13880  Avg mIoU:  76.47  
[Epoch: 213] [Batch: 0451/0580] Loss: 0.11188  Avg Loss: 0.13888  Avg mIoU:  76.30  
[Epoch: 213] [Batch: 0501/0580] Loss: 0.14892  Avg Loss: 0.13828  Avg mIoU:  76.39  
[Epoch: 213] [Batch: 0551/0580] Loss: 0.09323  Avg Loss: 0.13894  Avg mIoU:  76.29  

*** Training [@Epoch 213] Avg Loss: 0.13919  Avg mIoU:  76.30  ***

[Epoch: 213] [Batch: 0001/0050] Loss: 0.15533  Avg Loss: 0.15533  Avg mIoU:  55.89  

*** Validation [@Epoch 213] Avg Loss: 0.23819  Avg mIoU:  59.79  ***

[Epoch: 214] [Batch: 0001/0580] Loss: 0.12270  Avg Loss: 0.12270  Avg mIoU:  31.26  
[Epoch: 214] [Batch: 0051/0580] Loss: 0.20880  Avg Loss: 0.14553  Avg mIoU:  74.35  
[Epoch: 214] [Batch: 0101/0580] Loss: 0.14723  Avg Loss: 0.14312  Avg mIoU:  75.27  
[Epoch: 214] [Batch: 0151/0580] Loss: 0.11949  Avg Loss: 0.14258  Avg mIoU:  75.78  
[Epoch: 214] [Batch: 0201/0580] Loss: 0.15641  Avg Loss: 0.14161  Avg mIoU:  76.30  
[Epoch: 214] [Batch: 0251/0580] Loss: 0.15418  Avg Loss: 0.14131  Avg mIoU:  76.21  
[Epoch: 214] [Batch: 0301/0580] Loss: 0.14158  Avg Loss: 0.14092  Avg mIoU:  76.37  
[Epoch: 214] [Batch: 0351/0580] Loss: 0.13741  Avg Loss: 0.14039  Avg mIoU:  76.16  
[Epoch: 214] [Batch: 0401/0580] Loss: 0.10993  Avg Loss: 0.14065  Avg mIoU:  76.10  
[Epoch: 214] [Batch: 0451/0580] Loss: 0.11140  Avg Loss: 0.14113  Avg mIoU:  76.09  
[Epoch: 214] [Batch: 0501/0580] Loss: 0.12198  Avg Loss: 0.14199  Avg mIoU:  76.01  
[Epoch: 214] [Batch: 0551/0580] Loss: 0.24522  Avg Loss: 0.14151  Avg mIoU:  76.05  

*** Training [@Epoch 214] Avg Loss: 0.14153  Avg mIoU:  75.93  ***

[Epoch: 214] [Batch: 0001/0050] Loss: 0.16968  Avg Loss: 0.16968  Avg mIoU:  54.75  

*** Validation [@Epoch 214] Avg Loss: 0.25611  Avg mIoU:  56.18  ***

[Epoch: 215] [Batch: 0001/0580] Loss: 0.17676  Avg Loss: 0.17676  Avg mIoU:  39.67  
[Epoch: 215] [Batch: 0051/0580] Loss: 0.12314  Avg Loss: 0.13895  Avg mIoU:  75.71  
[Epoch: 215] [Batch: 0101/0580] Loss: 0.09242  Avg Loss: 0.13779  Avg mIoU:  76.14  
[Epoch: 215] [Batch: 0151/0580] Loss: 0.15240  Avg Loss: 0.14525  Avg mIoU:  75.37  
[Epoch: 215] [Batch: 0201/0580] Loss: 0.13303  Avg Loss: 0.14622  Avg mIoU:  74.94  
[Epoch: 215] [Batch: 0251/0580] Loss: 0.11943  Avg Loss: 0.14364  Avg mIoU:  75.51  
[Epoch: 215] [Batch: 0301/0580] Loss: 0.15948  Avg Loss: 0.14245  Avg mIoU:  75.65  
[Epoch: 215] [Batch: 0351/0580] Loss: 0.11664  Avg Loss: 0.14171  Avg mIoU:  75.74  
[Epoch: 215] [Batch: 0401/0580] Loss: 0.15804  Avg Loss: 0.14287  Avg mIoU:  75.68  
[Epoch: 215] [Batch: 0451/0580] Loss: 0.17347  Avg Loss: 0.14191  Avg mIoU:  75.78  
[Epoch: 215] [Batch: 0501/0580] Loss: 0.08945  Avg Loss: 0.14157  Avg mIoU:  75.81  
[Epoch: 215] [Batch: 0551/0580] Loss: 0.12004  Avg Loss: 0.14114  Avg mIoU:  75.95  

*** Training [@Epoch 215] Avg Loss: 0.14121  Avg mIoU:  75.90  ***

[Epoch: 215] [Batch: 0001/0050] Loss: 0.15003  Avg Loss: 0.15003  Avg mIoU:  62.19  

*** Validation [@Epoch 215] Avg Loss: 0.23714  Avg mIoU:  60.49  ***

[Epoch: 216] [Batch: 0001/0580] Loss: 0.13622  Avg Loss: 0.13622  Avg mIoU:  47.74  
[Epoch: 216] [Batch: 0051/0580] Loss: 0.09057  Avg Loss: 0.13401  Avg mIoU:  76.30  
[Epoch: 216] [Batch: 0101/0580] Loss: 0.21668  Avg Loss: 0.13642  Avg mIoU:  76.21  
[Epoch: 216] [Batch: 0151/0580] Loss: 0.19238  Avg Loss: 0.13639  Avg mIoU:  75.70  
[Epoch: 216] [Batch: 0201/0580] Loss: 0.09182  Avg Loss: 0.13834  Avg mIoU:  75.80  
[Epoch: 216] [Batch: 0251/0580] Loss: 0.15694  Avg Loss: 0.13920  Avg mIoU:  75.74  
[Epoch: 216] [Batch: 0301/0580] Loss: 0.14102  Avg Loss: 0.13850  Avg mIoU:  76.25  
[Epoch: 216] [Batch: 0351/0580] Loss: 0.11641  Avg Loss: 0.13718  Avg mIoU:  76.50  
[Epoch: 216] [Batch: 0401/0580] Loss: 0.11383  Avg Loss: 0.13732  Avg mIoU:  76.64  
[Epoch: 216] [Batch: 0451/0580] Loss: 0.14413  Avg Loss: 0.13934  Avg mIoU:  76.27  
[Epoch: 216] [Batch: 0501/0580] Loss: 0.13400  Avg Loss: 0.13964  Avg mIoU:  76.29  
[Epoch: 216] [Batch: 0551/0580] Loss: 0.12071  Avg Loss: 0.14033  Avg mIoU:  76.22  

*** Training [@Epoch 216] Avg Loss: 0.13995  Avg mIoU:  76.34  ***

[Epoch: 216] [Batch: 0001/0050] Loss: 0.16243  Avg Loss: 0.16243  Avg mIoU:  57.75  

*** Validation [@Epoch 216] Avg Loss: 0.24775  Avg mIoU:  59.03  ***

[Epoch: 217] [Batch: 0001/0580] Loss: 0.11431  Avg Loss: 0.11431  Avg mIoU:  49.01  
[Epoch: 217] [Batch: 0051/0580] Loss: 0.16199  Avg Loss: 0.13675  Avg mIoU:  76.61  
[Epoch: 217] [Batch: 0101/0580] Loss: 0.09834  Avg Loss: 0.14100  Avg mIoU:  76.57  
[Epoch: 217] [Batch: 0151/0580] Loss: 0.15375  Avg Loss: 0.13921  Avg mIoU:  75.99  
[Epoch: 217] [Batch: 0201/0580] Loss: 0.13045  Avg Loss: 0.13861  Avg mIoU:  76.01  
[Epoch: 217] [Batch: 0251/0580] Loss: 0.13962  Avg Loss: 0.13733  Avg mIoU:  76.83  
[Epoch: 217] [Batch: 0301/0580] Loss: 0.18212  Avg Loss: 0.13772  Avg mIoU:  76.71  
[Epoch: 217] [Batch: 0351/0580] Loss: 0.09149  Avg Loss: 0.13804  Avg mIoU:  76.66  
[Epoch: 217] [Batch: 0401/0580] Loss: 0.15484  Avg Loss: 0.13775  Avg mIoU:  76.59  
[Epoch: 217] [Batch: 0451/0580] Loss: 0.11585  Avg Loss: 0.13789  Avg mIoU:  76.57  
[Epoch: 217] [Batch: 0501/0580] Loss: 0.17298  Avg Loss: 0.13815  Avg mIoU:  76.58  
[Epoch: 217] [Batch: 0551/0580] Loss: 0.12932  Avg Loss: 0.13879  Avg mIoU:  76.41  

*** Training [@Epoch 217] Avg Loss: 0.13849  Avg mIoU:  76.49  ***

[Epoch: 217] [Batch: 0001/0050] Loss: 0.17115  Avg Loss: 0.17115  Avg mIoU:  54.98  

*** Validation [@Epoch 217] Avg Loss: 0.24749  Avg mIoU:  58.95  ***

[Epoch: 218] [Batch: 0001/0580] Loss: 0.16918  Avg Loss: 0.16918  Avg mIoU:  36.08  
[Epoch: 218] [Batch: 0051/0580] Loss: 0.12339  Avg Loss: 0.14511  Avg mIoU:  76.27  
[Epoch: 218] [Batch: 0101/0580] Loss: 0.10590  Avg Loss: 0.13938  Avg mIoU:  76.19  
[Epoch: 218] [Batch: 0151/0580] Loss: 0.10975  Avg Loss: 0.13789  Avg mIoU:  76.10  
[Epoch: 218] [Batch: 0201/0580] Loss: 0.11056  Avg Loss: 0.13800  Avg mIoU:  76.04  
[Epoch: 218] [Batch: 0251/0580] Loss: 0.13527  Avg Loss: 0.13821  Avg mIoU:  75.74  
[Epoch: 218] [Batch: 0301/0580] Loss: 0.12682  Avg Loss: 0.13806  Avg mIoU:  75.73  
[Epoch: 218] [Batch: 0351/0580] Loss: 0.08466  Avg Loss: 0.13894  Avg mIoU:  75.69  
[Epoch: 218] [Batch: 0401/0580] Loss: 0.12963  Avg Loss: 0.13895  Avg mIoU:  75.86  
[Epoch: 218] [Batch: 0451/0580] Loss: 0.11861  Avg Loss: 0.13875  Avg mIoU:  76.12  
[Epoch: 218] [Batch: 0501/0580] Loss: 0.13235  Avg Loss: 0.13934  Avg mIoU:  76.10  
[Epoch: 218] [Batch: 0551/0580] Loss: 0.17325  Avg Loss: 0.13925  Avg mIoU:  76.16  

*** Training [@Epoch 218] Avg Loss: 0.13964  Avg mIoU:  76.09  ***

[Epoch: 218] [Batch: 0001/0050] Loss: 0.15935  Avg Loss: 0.15935  Avg mIoU:  56.20  

*** Validation [@Epoch 218] Avg Loss: 0.23887  Avg mIoU:  59.83  ***

[Epoch: 219] [Batch: 0001/0580] Loss: 0.10721  Avg Loss: 0.10721  Avg mIoU:  54.70  
[Epoch: 219] [Batch: 0051/0580] Loss: 0.10044  Avg Loss: 0.13959  Avg mIoU:  75.10  
[Epoch: 219] [Batch: 0101/0580] Loss: 0.14115  Avg Loss: 0.13683  Avg mIoU:  76.66  
[Epoch: 219] [Batch: 0151/0580] Loss: 0.18149  Avg Loss: 0.13780  Avg mIoU:  76.33  
[Epoch: 219] [Batch: 0201/0580] Loss: 0.14195  Avg Loss: 0.13844  Avg mIoU:  76.36  
[Epoch: 219] [Batch: 0251/0580] Loss: 0.10567  Avg Loss: 0.13753  Avg mIoU:  76.75  
[Epoch: 219] [Batch: 0301/0580] Loss: 0.14627  Avg Loss: 0.13740  Avg mIoU:  76.77  
[Epoch: 219] [Batch: 0351/0580] Loss: 0.15522  Avg Loss: 0.13702  Avg mIoU:  76.78  
[Epoch: 219] [Batch: 0401/0580] Loss: 0.19639  Avg Loss: 0.13778  Avg mIoU:  76.59  
[Epoch: 219] [Batch: 0451/0580] Loss: 0.09611  Avg Loss: 0.13790  Avg mIoU:  76.60  
[Epoch: 219] [Batch: 0501/0580] Loss: 0.12542  Avg Loss: 0.13780  Avg mIoU:  76.64  
[Epoch: 219] [Batch: 0551/0580] Loss: 0.13790  Avg Loss: 0.13799  Avg mIoU:  76.64  

*** Training [@Epoch 219] Avg Loss: 0.13829  Avg mIoU:  76.48  ***

[Epoch: 219] [Batch: 0001/0050] Loss: 0.15800  Avg Loss: 0.15800  Avg mIoU:  60.67  

*** Validation [@Epoch 219] Avg Loss: 0.26004  Avg mIoU:  62.17  ***

[Epoch: 220] [Batch: 0001/0580] Loss: 0.13843  Avg Loss: 0.13843  Avg mIoU:  45.65  
[Epoch: 220] [Batch: 0051/0580] Loss: 0.11407  Avg Loss: 0.13831  Avg mIoU:  77.57  
[Epoch: 220] [Batch: 0101/0580] Loss: 0.13117  Avg Loss: 0.13455  Avg mIoU:  77.82  
[Epoch: 220] [Batch: 0151/0580] Loss: 0.12025  Avg Loss: 0.13695  Avg mIoU:  77.43  
[Epoch: 220] [Batch: 0201/0580] Loss: 0.14083  Avg Loss: 0.13937  Avg mIoU:  76.84  
[Epoch: 220] [Batch: 0251/0580] Loss: 0.14207  Avg Loss: 0.14048  Avg mIoU:  76.49  
[Epoch: 220] [Batch: 0301/0580] Loss: 0.14995  Avg Loss: 0.13930  Avg mIoU:  76.79  
[Epoch: 220] [Batch: 0351/0580] Loss: 0.11909  Avg Loss: 0.13838  Avg mIoU:  76.72  
[Epoch: 220] [Batch: 0401/0580] Loss: 0.11413  Avg Loss: 0.13898  Avg mIoU:  76.54  
[Epoch: 220] [Batch: 0451/0580] Loss: 0.17300  Avg Loss: 0.13963  Avg mIoU:  76.24  
[Epoch: 220] [Batch: 0501/0580] Loss: 0.14303  Avg Loss: 0.14054  Avg mIoU:  76.28  
[Epoch: 220] [Batch: 0551/0580] Loss: 0.13042  Avg Loss: 0.14055  Avg mIoU:  76.13  

*** Training [@Epoch 220] Avg Loss: 0.14016  Avg mIoU:  76.22  ***

[Epoch: 220] [Batch: 0001/0050] Loss: 0.16722  Avg Loss: 0.16722  Avg mIoU:  59.92  

*** Validation [@Epoch 220] Avg Loss: 0.26048  Avg mIoU:  59.30  ***

[Epoch: 221] [Batch: 0001/0580] Loss: 0.19297  Avg Loss: 0.19297  Avg mIoU:  44.70  
[Epoch: 221] [Batch: 0051/0580] Loss: 0.13309  Avg Loss: 0.13615  Avg mIoU:  76.80  
[Epoch: 221] [Batch: 0101/0580] Loss: 0.12056  Avg Loss: 0.13724  Avg mIoU:  75.97  
[Epoch: 221] [Batch: 0151/0580] Loss: 0.17071  Avg Loss: 0.13696  Avg mIoU:  76.15  
[Epoch: 221] [Batch: 0201/0580] Loss: 0.10894  Avg Loss: 0.13759  Avg mIoU:  76.10  
[Epoch: 221] [Batch: 0251/0580] Loss: 0.14894  Avg Loss: 0.13713  Avg mIoU:  76.24  
[Epoch: 221] [Batch: 0301/0580] Loss: 0.11038  Avg Loss: 0.13712  Avg mIoU:  76.02  
[Epoch: 221] [Batch: 0351/0580] Loss: 0.16019  Avg Loss: 0.13749  Avg mIoU:  76.22  
[Epoch: 221] [Batch: 0401/0580] Loss: 0.14963  Avg Loss: 0.13902  Avg mIoU:  76.07  
[Epoch: 221] [Batch: 0451/0580] Loss: 0.11265  Avg Loss: 0.13861  Avg mIoU:  76.13  
[Epoch: 221] [Batch: 0501/0580] Loss: 0.13255  Avg Loss: 0.13925  Avg mIoU:  76.10  
[Epoch: 221] [Batch: 0551/0580] Loss: 0.10929  Avg Loss: 0.13906  Avg mIoU:  76.12  

*** Training [@Epoch 221] Avg Loss: 0.13898  Avg mIoU:  76.14  ***

[Epoch: 221] [Batch: 0001/0050] Loss: 0.16677  Avg Loss: 0.16677  Avg mIoU:  58.87  

*** Validation [@Epoch 221] Avg Loss: 0.23962  Avg mIoU:  60.03  ***

[Epoch: 222] [Batch: 0001/0580] Loss: 0.13325  Avg Loss: 0.13325  Avg mIoU:  37.71  
[Epoch: 222] [Batch: 0051/0580] Loss: 0.12789  Avg Loss: 0.13270  Avg mIoU:  75.91  
[Epoch: 222] [Batch: 0101/0580] Loss: 0.15439  Avg Loss: 0.13577  Avg mIoU:  76.49  
[Epoch: 222] [Batch: 0151/0580] Loss: 0.09853  Avg Loss: 0.13502  Avg mIoU:  76.86  
[Epoch: 222] [Batch: 0201/0580] Loss: 0.13803  Avg Loss: 0.13500  Avg mIoU:  77.15  
[Epoch: 222] [Batch: 0251/0580] Loss: 0.15905  Avg Loss: 0.13517  Avg mIoU:  77.21  
[Epoch: 222] [Batch: 0301/0580] Loss: 0.13499  Avg Loss: 0.13679  Avg mIoU:  76.81  
[Epoch: 222] [Batch: 0351/0580] Loss: 0.14673  Avg Loss: 0.13665  Avg mIoU:  76.86  
[Epoch: 222] [Batch: 0401/0580] Loss: 0.10435  Avg Loss: 0.13619  Avg mIoU:  76.81  
[Epoch: 222] [Batch: 0451/0580] Loss: 0.19200  Avg Loss: 0.13701  Avg mIoU:  76.67  
[Epoch: 222] [Batch: 0501/0580] Loss: 0.16937  Avg Loss: 0.13676  Avg mIoU:  76.71  
[Epoch: 222] [Batch: 0551/0580] Loss: 0.16471  Avg Loss: 0.13812  Avg mIoU:  76.55  

*** Training [@Epoch 222] Avg Loss: 0.13852  Avg mIoU:  76.52  ***

[Epoch: 222] [Batch: 0001/0050] Loss: 0.16652  Avg Loss: 0.16652  Avg mIoU:  57.29  

*** Validation [@Epoch 222] Avg Loss: 0.25992  Avg mIoU:  59.68  ***

[Epoch: 223] [Batch: 0001/0580] Loss: 0.10010  Avg Loss: 0.10010  Avg mIoU:  49.96  
[Epoch: 223] [Batch: 0051/0580] Loss: 0.16591  Avg Loss: 0.14198  Avg mIoU:  74.57  
[Epoch: 223] [Batch: 0101/0580] Loss: 0.11695  Avg Loss: 0.14172  Avg mIoU:  74.46  
[Epoch: 223] [Batch: 0151/0580] Loss: 0.13280  Avg Loss: 0.14025  Avg mIoU:  75.48  
[Epoch: 223] [Batch: 0201/0580] Loss: 0.14476  Avg Loss: 0.13981  Avg mIoU:  75.41  
[Epoch: 223] [Batch: 0251/0580] Loss: 0.13221  Avg Loss: 0.14004  Avg mIoU:  75.95  
[Epoch: 223] [Batch: 0301/0580] Loss: 0.13497  Avg Loss: 0.13981  Avg mIoU:  75.93  
[Epoch: 223] [Batch: 0351/0580] Loss: 0.09938  Avg Loss: 0.14080  Avg mIoU:  75.94  
[Epoch: 223] [Batch: 0401/0580] Loss: 0.22442  Avg Loss: 0.13991  Avg mIoU:  75.95  
[Epoch: 223] [Batch: 0451/0580] Loss: 0.10734  Avg Loss: 0.13990  Avg mIoU:  76.10  
[Epoch: 223] [Batch: 0501/0580] Loss: 0.17254  Avg Loss: 0.13980  Avg mIoU:  76.17  
[Epoch: 223] [Batch: 0551/0580] Loss: 0.14995  Avg Loss: 0.13954  Avg mIoU:  76.17  

*** Training [@Epoch 223] Avg Loss: 0.13968  Avg mIoU:  76.18  ***

[Epoch: 223] [Batch: 0001/0050] Loss: 0.16400  Avg Loss: 0.16400  Avg mIoU:  55.00  

*** Validation [@Epoch 223] Avg Loss: 0.24196  Avg mIoU:  57.22  ***

[Epoch: 224] [Batch: 0001/0580] Loss: 0.11986  Avg Loss: 0.11986  Avg mIoU:  37.66  
[Epoch: 224] [Batch: 0051/0580] Loss: 0.13023  Avg Loss: 0.14225  Avg mIoU:  75.97  
[Epoch: 224] [Batch: 0101/0580] Loss: 0.18916  Avg Loss: 0.14187  Avg mIoU:  75.19  
[Epoch: 224] [Batch: 0151/0580] Loss: 0.13672  Avg Loss: 0.14143  Avg mIoU:  75.29  
[Epoch: 224] [Batch: 0201/0580] Loss: 0.11158  Avg Loss: 0.14208  Avg mIoU:  75.74  
[Epoch: 224] [Batch: 0251/0580] Loss: 0.13391  Avg Loss: 0.14145  Avg mIoU:  75.91  
[Epoch: 224] [Batch: 0301/0580] Loss: 0.13495  Avg Loss: 0.14073  Avg mIoU:  75.93  
[Epoch: 224] [Batch: 0351/0580] Loss: 0.07247  Avg Loss: 0.14033  Avg mIoU:  76.03  
[Epoch: 224] [Batch: 0401/0580] Loss: 0.15609  Avg Loss: 0.13958  Avg mIoU:  76.16  
[Epoch: 224] [Batch: 0451/0580] Loss: 0.12075  Avg Loss: 0.13944  Avg mIoU:  76.20  
[Epoch: 224] [Batch: 0501/0580] Loss: 0.10363  Avg Loss: 0.13981  Avg mIoU:  76.10  
[Epoch: 224] [Batch: 0551/0580] Loss: 0.10769  Avg Loss: 0.13910  Avg mIoU:  76.16  

*** Training [@Epoch 224] Avg Loss: 0.13961  Avg mIoU:  76.21  ***

[Epoch: 224] [Batch: 0001/0050] Loss: 0.17642  Avg Loss: 0.17642  Avg mIoU:  61.30  

*** Validation [@Epoch 224] Avg Loss: 0.27195  Avg mIoU:  59.97  ***

[Epoch: 225] [Batch: 0001/0580] Loss: 0.11374  Avg Loss: 0.11374  Avg mIoU:  47.98  
[Epoch: 225] [Batch: 0051/0580] Loss: 0.11611  Avg Loss: 0.13221  Avg mIoU:  76.27  
[Epoch: 225] [Batch: 0101/0580] Loss: 0.12103  Avg Loss: 0.13460  Avg mIoU:  76.23  
[Epoch: 225] [Batch: 0151/0580] Loss: 0.18256  Avg Loss: 0.13824  Avg mIoU:  76.57  
[Epoch: 225] [Batch: 0201/0580] Loss: 0.13389  Avg Loss: 0.13892  Avg mIoU:  76.27  
[Epoch: 225] [Batch: 0251/0580] Loss: 0.10990  Avg Loss: 0.13818  Avg mIoU:  76.02  
[Epoch: 225] [Batch: 0301/0580] Loss: 0.12864  Avg Loss: 0.13778  Avg mIoU:  76.14  
[Epoch: 225] [Batch: 0351/0580] Loss: 0.12240  Avg Loss: 0.13846  Avg mIoU:  76.19  
[Epoch: 225] [Batch: 0401/0580] Loss: 0.14559  Avg Loss: 0.13777  Avg mIoU:  76.58  
[Epoch: 225] [Batch: 0451/0580] Loss: 0.12180  Avg Loss: 0.13794  Avg mIoU:  76.46  
[Epoch: 225] [Batch: 0501/0580] Loss: 0.09009  Avg Loss: 0.13771  Avg mIoU:  76.52  
[Epoch: 225] [Batch: 0551/0580] Loss: 0.16148  Avg Loss: 0.13826  Avg mIoU:  76.43  

*** Training [@Epoch 225] Avg Loss: 0.13874  Avg mIoU:  76.39  ***

[Epoch: 225] [Batch: 0001/0050] Loss: 0.15950  Avg Loss: 0.15950  Avg mIoU:  56.22  

*** Validation [@Epoch 225] Avg Loss: 0.25285  Avg mIoU:  58.11  ***

[Epoch: 226] [Batch: 0001/0580] Loss: 0.15087  Avg Loss: 0.15087  Avg mIoU:  44.68  
[Epoch: 226] [Batch: 0051/0580] Loss: 0.11007  Avg Loss: 0.14069  Avg mIoU:  76.29  
[Epoch: 226] [Batch: 0101/0580] Loss: 0.11787  Avg Loss: 0.13915  Avg mIoU:  75.91  
[Epoch: 226] [Batch: 0151/0580] Loss: 0.21289  Avg Loss: 0.13940  Avg mIoU:  76.08  
[Epoch: 226] [Batch: 0201/0580] Loss: 0.10416  Avg Loss: 0.13826  Avg mIoU:  76.14  
[Epoch: 226] [Batch: 0251/0580] Loss: 0.14085  Avg Loss: 0.13775  Avg mIoU:  76.30  
[Epoch: 226] [Batch: 0301/0580] Loss: 0.13331  Avg Loss: 0.13841  Avg mIoU:  76.39  
[Epoch: 226] [Batch: 0351/0580] Loss: 0.13852  Avg Loss: 0.13828  Avg mIoU:  76.39  
[Epoch: 226] [Batch: 0401/0580] Loss: 0.13164  Avg Loss: 0.13903  Avg mIoU:  76.32  
[Epoch: 226] [Batch: 0451/0580] Loss: 0.10055  Avg Loss: 0.13863  Avg mIoU:  76.47  
[Epoch: 226] [Batch: 0501/0580] Loss: 0.12808  Avg Loss: 0.13775  Avg mIoU:  76.56  
[Epoch: 226] [Batch: 0551/0580] Loss: 0.21513  Avg Loss: 0.13787  Avg mIoU:  76.40  

*** Training [@Epoch 226] Avg Loss: 0.13772  Avg mIoU:  76.47  ***

[Epoch: 226] [Batch: 0001/0050] Loss: 0.16662  Avg Loss: 0.16662  Avg mIoU:  59.36  

*** Validation [@Epoch 226] Avg Loss: 0.26321  Avg mIoU:  60.21  ***

[Epoch: 227] [Batch: 0001/0580] Loss: 0.13992  Avg Loss: 0.13992  Avg mIoU:  49.75  
[Epoch: 227] [Batch: 0051/0580] Loss: 0.10877  Avg Loss: 0.13848  Avg mIoU:  75.06  
[Epoch: 227] [Batch: 0101/0580] Loss: 0.16787  Avg Loss: 0.13790  Avg mIoU:  76.16  
[Epoch: 227] [Batch: 0151/0580] Loss: 0.15725  Avg Loss: 0.13899  Avg mIoU:  76.12  
[Epoch: 227] [Batch: 0201/0580] Loss: 0.11846  Avg Loss: 0.13881  Avg mIoU:  75.84  
[Epoch: 227] [Batch: 0251/0580] Loss: 0.17537  Avg Loss: 0.13782  Avg mIoU:  76.29  
[Epoch: 227] [Batch: 0301/0580] Loss: 0.13247  Avg Loss: 0.13800  Avg mIoU:  76.09  
[Epoch: 227] [Batch: 0351/0580] Loss: 0.14527  Avg Loss: 0.13860  Avg mIoU:  75.96  
[Epoch: 227] [Batch: 0401/0580] Loss: 0.08218  Avg Loss: 0.13779  Avg mIoU:  76.24  
[Epoch: 227] [Batch: 0451/0580] Loss: 0.14128  Avg Loss: 0.13692  Avg mIoU:  76.23  
[Epoch: 227] [Batch: 0501/0580] Loss: 0.11268  Avg Loss: 0.13735  Avg mIoU:  76.41  
[Epoch: 227] [Batch: 0551/0580] Loss: 0.16006  Avg Loss: 0.13805  Avg mIoU:  76.22  

*** Training [@Epoch 227] Avg Loss: 0.13841  Avg mIoU:  76.22  ***

[Epoch: 227] [Batch: 0001/0050] Loss: 0.15234  Avg Loss: 0.15234  Avg mIoU:  57.01  

*** Validation [@Epoch 227] Avg Loss: 0.24066  Avg mIoU:  60.31  ***

[Epoch: 228] [Batch: 0001/0580] Loss: 0.13895  Avg Loss: 0.13895  Avg mIoU:  46.78  
[Epoch: 228] [Batch: 0051/0580] Loss: 0.11120  Avg Loss: 0.13583  Avg mIoU:  75.58  
[Epoch: 228] [Batch: 0101/0580] Loss: 0.15326  Avg Loss: 0.13263  Avg mIoU:  77.01  
[Epoch: 228] [Batch: 0151/0580] Loss: 0.10334  Avg Loss: 0.13446  Avg mIoU:  76.89  
[Epoch: 228] [Batch: 0201/0580] Loss: 0.18058  Avg Loss: 0.13467  Avg mIoU:  76.77  
[Epoch: 228] [Batch: 0251/0580] Loss: 0.15019  Avg Loss: 0.13365  Avg mIoU:  76.57  
[Epoch: 228] [Batch: 0301/0580] Loss: 0.13778  Avg Loss: 0.13508  Avg mIoU:  76.51  
[Epoch: 228] [Batch: 0351/0580] Loss: 0.16535  Avg Loss: 0.13591  Avg mIoU:  76.49  
[Epoch: 228] [Batch: 0401/0580] Loss: 0.13915  Avg Loss: 0.13681  Avg mIoU:  76.51  
[Epoch: 228] [Batch: 0451/0580] Loss: 0.13261  Avg Loss: 0.13712  Avg mIoU:  76.65  
[Epoch: 228] [Batch: 0501/0580] Loss: 0.13890  Avg Loss: 0.13756  Avg mIoU:  76.56  
[Epoch: 228] [Batch: 0551/0580] Loss: 0.10788  Avg Loss: 0.13719  Avg mIoU:  76.65  

*** Training [@Epoch 228] Avg Loss: 0.13717  Avg mIoU:  76.77  ***

[Epoch: 228] [Batch: 0001/0050] Loss: 0.16704  Avg Loss: 0.16704  Avg mIoU:  60.02  

*** Validation [@Epoch 228] Avg Loss: 0.27026  Avg mIoU:  61.30  ***

[Epoch: 229] [Batch: 0001/0580] Loss: 0.13536  Avg Loss: 0.13536  Avg mIoU:  40.41  
[Epoch: 229] [Batch: 0051/0580] Loss: 0.12671  Avg Loss: 0.12913  Avg mIoU:  76.99  
[Epoch: 229] [Batch: 0101/0580] Loss: 0.14858  Avg Loss: 0.14030  Avg mIoU:  76.15  
[Epoch: 229] [Batch: 0151/0580] Loss: 0.13008  Avg Loss: 0.13858  Avg mIoU:  77.26  
[Epoch: 229] [Batch: 0201/0580] Loss: 0.13709  Avg Loss: 0.13739  Avg mIoU:  77.49  
[Epoch: 229] [Batch: 0251/0580] Loss: 0.08605  Avg Loss: 0.13631  Avg mIoU:  77.18  
[Epoch: 229] [Batch: 0301/0580] Loss: 0.13514  Avg Loss: 0.13538  Avg mIoU:  77.21  
[Epoch: 229] [Batch: 0351/0580] Loss: 0.12934  Avg Loss: 0.13586  Avg mIoU:  77.10  
[Epoch: 229] [Batch: 0401/0580] Loss: 0.12953  Avg Loss: 0.13645  Avg mIoU:  76.70  
[Epoch: 229] [Batch: 0451/0580] Loss: 0.10926  Avg Loss: 0.13614  Avg mIoU:  76.52  
[Epoch: 229] [Batch: 0501/0580] Loss: 0.13313  Avg Loss: 0.13620  Avg mIoU:  76.62  
[Epoch: 229] [Batch: 0551/0580] Loss: 0.13867  Avg Loss: 0.13699  Avg mIoU:  76.51  

*** Training [@Epoch 229] Avg Loss: 0.13733  Avg mIoU:  76.52  ***

[Epoch: 229] [Batch: 0001/0050] Loss: 0.16366  Avg Loss: 0.16366  Avg mIoU:  54.54  

*** Validation [@Epoch 229] Avg Loss: 0.24422  Avg mIoU:  59.34  ***

[Epoch: 230] [Batch: 0001/0580] Loss: 0.16191  Avg Loss: 0.16191  Avg mIoU:  43.65  
[Epoch: 230] [Batch: 0051/0580] Loss: 0.15166  Avg Loss: 0.13333  Avg mIoU:  76.90  
[Epoch: 230] [Batch: 0101/0580] Loss: 0.13344  Avg Loss: 0.13344  Avg mIoU:  77.18  
[Epoch: 230] [Batch: 0151/0580] Loss: 0.10042  Avg Loss: 0.13750  Avg mIoU:  76.30  
[Epoch: 230] [Batch: 0201/0580] Loss: 0.13779  Avg Loss: 0.13769  Avg mIoU:  76.48  
[Epoch: 230] [Batch: 0251/0580] Loss: 0.16794  Avg Loss: 0.13753  Avg mIoU:  76.44  
[Epoch: 230] [Batch: 0301/0580] Loss: 0.16007  Avg Loss: 0.13755  Avg mIoU:  76.54  
[Epoch: 230] [Batch: 0351/0580] Loss: 0.18469  Avg Loss: 0.13669  Avg mIoU:  76.54  
[Epoch: 230] [Batch: 0401/0580] Loss: 0.14524  Avg Loss: 0.13548  Avg mIoU:  76.84  
[Epoch: 230] [Batch: 0451/0580] Loss: 0.13601  Avg Loss: 0.13589  Avg mIoU:  77.06  
[Epoch: 230] [Batch: 0501/0580] Loss: 0.13011  Avg Loss: 0.13656  Avg mIoU:  76.83  
[Epoch: 230] [Batch: 0551/0580] Loss: 0.14987  Avg Loss: 0.13676  Avg mIoU:  76.58  

*** Training [@Epoch 230] Avg Loss: 0.13639  Avg mIoU:  76.75  ***

[Epoch: 230] [Batch: 0001/0050] Loss: 0.17560  Avg Loss: 0.17560  Avg mIoU:  55.05  

*** Validation [@Epoch 230] Avg Loss: 0.25921  Avg mIoU:  57.79  ***

[Epoch: 231] [Batch: 0001/0580] Loss: 0.13934  Avg Loss: 0.13934  Avg mIoU:  60.81  
[Epoch: 231] [Batch: 0051/0580] Loss: 0.07909  Avg Loss: 0.12656  Avg mIoU:  77.72  
[Epoch: 231] [Batch: 0101/0580] Loss: 0.15210  Avg Loss: 0.12835  Avg mIoU:  77.53  
[Epoch: 231] [Batch: 0151/0580] Loss: 0.19446  Avg Loss: 0.13043  Avg mIoU:  76.95  
[Epoch: 231] [Batch: 0201/0580] Loss: 0.10159  Avg Loss: 0.13226  Avg mIoU:  77.05  
[Epoch: 231] [Batch: 0251/0580] Loss: 0.09973  Avg Loss: 0.13132  Avg mIoU:  77.38  
[Epoch: 231] [Batch: 0301/0580] Loss: 0.12282  Avg Loss: 0.13198  Avg mIoU:  77.52  
[Epoch: 231] [Batch: 0351/0580] Loss: 0.12223  Avg Loss: 0.13372  Avg mIoU:  77.52  
[Epoch: 231] [Batch: 0401/0580] Loss: 0.14752  Avg Loss: 0.13525  Avg mIoU:  77.36  
[Epoch: 231] [Batch: 0451/0580] Loss: 0.14554  Avg Loss: 0.13539  Avg mIoU:  77.22  
[Epoch: 231] [Batch: 0501/0580] Loss: 0.10742  Avg Loss: 0.13572  Avg mIoU:  76.99  
[Epoch: 231] [Batch: 0551/0580] Loss: 0.16768  Avg Loss: 0.13616  Avg mIoU:  77.04  

*** Training [@Epoch 231] Avg Loss: 0.13606  Avg mIoU:  77.03  ***

[Epoch: 231] [Batch: 0001/0050] Loss: 0.17036  Avg Loss: 0.17036  Avg mIoU:  56.15  

*** Validation [@Epoch 231] Avg Loss: 0.23717  Avg mIoU:  61.12  ***

[Epoch: 232] [Batch: 0001/0580] Loss: 0.12641  Avg Loss: 0.12641  Avg mIoU:  50.50  
[Epoch: 232] [Batch: 0051/0580] Loss: 0.11927  Avg Loss: 0.13587  Avg mIoU:  75.89  
[Epoch: 232] [Batch: 0101/0580] Loss: 0.10245  Avg Loss: 0.14079  Avg mIoU:  76.13  
[Epoch: 232] [Batch: 0151/0580] Loss: 0.13363  Avg Loss: 0.13883  Avg mIoU:  76.70  
[Epoch: 232] [Batch: 0201/0580] Loss: 0.15077  Avg Loss: 0.14033  Avg mIoU:  76.41  
[Epoch: 232] [Batch: 0251/0580] Loss: 0.17756  Avg Loss: 0.13911  Avg mIoU:  76.36  
[Epoch: 232] [Batch: 0301/0580] Loss: 0.13117  Avg Loss: 0.13888  Avg mIoU:  76.37  
[Epoch: 232] [Batch: 0351/0580] Loss: 0.13635  Avg Loss: 0.13895  Avg mIoU:  76.43  
[Epoch: 232] [Batch: 0401/0580] Loss: 0.12335  Avg Loss: 0.13829  Avg mIoU:  76.55  
[Epoch: 232] [Batch: 0451/0580] Loss: 0.11852  Avg Loss: 0.13871  Avg mIoU:  76.40  
[Epoch: 232] [Batch: 0501/0580] Loss: 0.10207  Avg Loss: 0.13812  Avg mIoU:  76.45  
[Epoch: 232] [Batch: 0551/0580] Loss: 0.19167  Avg Loss: 0.13797  Avg mIoU:  76.47  

*** Training [@Epoch 232] Avg Loss: 0.13773  Avg mIoU:  76.40  ***

[Epoch: 232] [Batch: 0001/0050] Loss: 0.16332  Avg Loss: 0.16332  Avg mIoU:  59.47  

*** Validation [@Epoch 232] Avg Loss: 0.24617  Avg mIoU:  61.11  ***

[Epoch: 233] [Batch: 0001/0580] Loss: 0.12001  Avg Loss: 0.12001  Avg mIoU:  44.54  
[Epoch: 233] [Batch: 0051/0580] Loss: 0.18420  Avg Loss: 0.13299  Avg mIoU:  77.11  
[Epoch: 233] [Batch: 0101/0580] Loss: 0.15108  Avg Loss: 0.13437  Avg mIoU:  77.18  
[Epoch: 233] [Batch: 0151/0580] Loss: 0.15212  Avg Loss: 0.13601  Avg mIoU:  76.37  
[Epoch: 233] [Batch: 0201/0580] Loss: 0.11641  Avg Loss: 0.13638  Avg mIoU:  76.03  
[Epoch: 233] [Batch: 0251/0580] Loss: 0.10862  Avg Loss: 0.13658  Avg mIoU:  76.11  
[Epoch: 233] [Batch: 0301/0580] Loss: 0.16200  Avg Loss: 0.13662  Avg mIoU:  76.44  
[Epoch: 233] [Batch: 0351/0580] Loss: 0.16585  Avg Loss: 0.13794  Avg mIoU:  76.32  
[Epoch: 233] [Batch: 0401/0580] Loss: 0.12557  Avg Loss: 0.13706  Avg mIoU:  76.39  
[Epoch: 233] [Batch: 0451/0580] Loss: 0.11674  Avg Loss: 0.13735  Avg mIoU:  76.55  
[Epoch: 233] [Batch: 0501/0580] Loss: 0.20131  Avg Loss: 0.13703  Avg mIoU:  76.82  
[Epoch: 233] [Batch: 0551/0580] Loss: 0.14436  Avg Loss: 0.13681  Avg mIoU:  76.72  

*** Training [@Epoch 233] Avg Loss: 0.13694  Avg mIoU:  76.75  ***

[Epoch: 233] [Batch: 0001/0050] Loss: 0.15700  Avg Loss: 0.15700  Avg mIoU:  56.39  

*** Validation [@Epoch 233] Avg Loss: 0.25134  Avg mIoU:  58.85  ***

[Epoch: 234] [Batch: 0001/0580] Loss: 0.15079  Avg Loss: 0.15079  Avg mIoU:  43.86  
[Epoch: 234] [Batch: 0051/0580] Loss: 0.08918  Avg Loss: 0.13354  Avg mIoU:  77.91  
[Epoch: 234] [Batch: 0101/0580] Loss: 0.14343  Avg Loss: 0.13951  Avg mIoU:  77.16  
[Epoch: 234] [Batch: 0151/0580] Loss: 0.19231  Avg Loss: 0.13962  Avg mIoU:  76.68  
[Epoch: 234] [Batch: 0201/0580] Loss: 0.11658  Avg Loss: 0.13817  Avg mIoU:  76.89  
[Epoch: 234] [Batch: 0251/0580] Loss: 0.20298  Avg Loss: 0.13750  Avg mIoU:  76.99  
[Epoch: 234] [Batch: 0301/0580] Loss: 0.12925  Avg Loss: 0.13723  Avg mIoU:  77.00  
[Epoch: 234] [Batch: 0351/0580] Loss: 0.14187  Avg Loss: 0.13558  Avg mIoU:  77.06  
[Epoch: 234] [Batch: 0401/0580] Loss: 0.11454  Avg Loss: 0.13478  Avg mIoU:  77.11  
[Epoch: 234] [Batch: 0451/0580] Loss: 0.12741  Avg Loss: 0.13506  Avg mIoU:  77.17  
[Epoch: 234] [Batch: 0501/0580] Loss: 0.11048  Avg Loss: 0.13467  Avg mIoU:  77.10  
[Epoch: 234] [Batch: 0551/0580] Loss: 0.13838  Avg Loss: 0.13486  Avg mIoU:  77.04  

*** Training [@Epoch 234] Avg Loss: 0.13535  Avg mIoU:  76.99  ***

[Epoch: 234] [Batch: 0001/0050] Loss: 0.17398  Avg Loss: 0.17398  Avg mIoU:  55.14  

*** Validation [@Epoch 234] Avg Loss: 0.24800  Avg mIoU:  58.93  ***

[Epoch: 235] [Batch: 0001/0580] Loss: 0.09387  Avg Loss: 0.09387  Avg mIoU:  56.73  
[Epoch: 235] [Batch: 0051/0580] Loss: 0.10085  Avg Loss: 0.13407  Avg mIoU:  76.16  
[Epoch: 235] [Batch: 0101/0580] Loss: 0.11161  Avg Loss: 0.13842  Avg mIoU:  77.00  
[Epoch: 235] [Batch: 0151/0580] Loss: 0.10272  Avg Loss: 0.13710  Avg mIoU:  76.60  
[Epoch: 235] [Batch: 0201/0580] Loss: 0.19298  Avg Loss: 0.13554  Avg mIoU:  76.95  
[Epoch: 235] [Batch: 0251/0580] Loss: 0.11190  Avg Loss: 0.13464  Avg mIoU:  76.96  
[Epoch: 235] [Batch: 0301/0580] Loss: 0.14363  Avg Loss: 0.13541  Avg mIoU:  76.83  
[Epoch: 235] [Batch: 0351/0580] Loss: 0.10283  Avg Loss: 0.13594  Avg mIoU:  76.82  
[Epoch: 235] [Batch: 0401/0580] Loss: 0.10836  Avg Loss: 0.13544  Avg mIoU:  76.80  
[Epoch: 235] [Batch: 0451/0580] Loss: 0.09498  Avg Loss: 0.13566  Avg mIoU:  76.86  
[Epoch: 235] [Batch: 0501/0580] Loss: 0.13277  Avg Loss: 0.13632  Avg mIoU:  76.68  
[Epoch: 235] [Batch: 0551/0580] Loss: 0.13931  Avg Loss: 0.13594  Avg mIoU:  76.76  

*** Training [@Epoch 235] Avg Loss: 0.13633  Avg mIoU:  76.68  ***

[Epoch: 235] [Batch: 0001/0050] Loss: 0.15337  Avg Loss: 0.15337  Avg mIoU:  59.60  

*** Validation [@Epoch 235] Avg Loss: 0.24377  Avg mIoU:  60.91  ***

[Epoch: 236] [Batch: 0001/0580] Loss: 0.11295  Avg Loss: 0.11295  Avg mIoU:  51.95  
[Epoch: 236] [Batch: 0051/0580] Loss: 0.21416  Avg Loss: 0.12980  Avg mIoU:  76.96  
[Epoch: 236] [Batch: 0101/0580] Loss: 0.10783  Avg Loss: 0.12935  Avg mIoU:  77.64  
[Epoch: 236] [Batch: 0151/0580] Loss: 0.08680  Avg Loss: 0.13252  Avg mIoU:  77.28  
[Epoch: 236] [Batch: 0201/0580] Loss: 0.14500  Avg Loss: 0.13073  Avg mIoU:  77.75  
[Epoch: 236] [Batch: 0251/0580] Loss: 0.07511  Avg Loss: 0.12916  Avg mIoU:  77.77  
[Epoch: 236] [Batch: 0301/0580] Loss: 0.09714  Avg Loss: 0.13118  Avg mIoU:  77.55  
[Epoch: 236] [Batch: 0351/0580] Loss: 0.16016  Avg Loss: 0.13276  Avg mIoU:  77.34  
[Epoch: 236] [Batch: 0401/0580] Loss: 0.21560  Avg Loss: 0.13457  Avg mIoU:  76.95  
[Epoch: 236] [Batch: 0451/0580] Loss: 0.13415  Avg Loss: 0.13504  Avg mIoU:  76.78  
[Epoch: 236] [Batch: 0501/0580] Loss: 0.11524  Avg Loss: 0.13578  Avg mIoU:  76.75  
[Epoch: 236] [Batch: 0551/0580] Loss: 0.12072  Avg Loss: 0.13553  Avg mIoU:  76.87  

*** Training [@Epoch 236] Avg Loss: 0.13592  Avg mIoU:  76.75  ***

[Epoch: 236] [Batch: 0001/0050] Loss: 0.15363  Avg Loss: 0.15363  Avg mIoU:  57.84  

*** Validation [@Epoch 236] Avg Loss: 0.24599  Avg mIoU:  60.56  ***

[Epoch: 237] [Batch: 0001/0580] Loss: 0.12356  Avg Loss: 0.12356  Avg mIoU:  54.04  
[Epoch: 237] [Batch: 0051/0580] Loss: 0.11662  Avg Loss: 0.13652  Avg mIoU:  76.23  
[Epoch: 237] [Batch: 0101/0580] Loss: 0.14097  Avg Loss: 0.13583  Avg mIoU:  76.40  
[Epoch: 237] [Batch: 0151/0580] Loss: 0.12042  Avg Loss: 0.13771  Avg mIoU:  76.52  
[Epoch: 237] [Batch: 0201/0580] Loss: 0.10760  Avg Loss: 0.13711  Avg mIoU:  76.69  
[Epoch: 237] [Batch: 0251/0580] Loss: 0.10337  Avg Loss: 0.13697  Avg mIoU:  77.05  
[Epoch: 237] [Batch: 0301/0580] Loss: 0.12755  Avg Loss: 0.13717  Avg mIoU:  77.23  
[Epoch: 237] [Batch: 0351/0580] Loss: 0.11320  Avg Loss: 0.13691  Avg mIoU:  77.26  
[Epoch: 237] [Batch: 0401/0580] Loss: 0.12377  Avg Loss: 0.13670  Avg mIoU:  77.20  
[Epoch: 237] [Batch: 0451/0580] Loss: 0.19482  Avg Loss: 0.13737  Avg mIoU:  76.84  
[Epoch: 237] [Batch: 0501/0580] Loss: 0.17731  Avg Loss: 0.13669  Avg mIoU:  76.88  
[Epoch: 237] [Batch: 0551/0580] Loss: 0.09761  Avg Loss: 0.13638  Avg mIoU:  76.95  

*** Training [@Epoch 237] Avg Loss: 0.13591  Avg mIoU:  76.89  ***

[Epoch: 237] [Batch: 0001/0050] Loss: 0.14988  Avg Loss: 0.14988  Avg mIoU:  60.97  

*** Validation [@Epoch 237] Avg Loss: 0.24584  Avg mIoU:  61.07  ***

[Epoch: 238] [Batch: 0001/0580] Loss: 0.11690  Avg Loss: 0.11690  Avg mIoU:  44.70  
[Epoch: 238] [Batch: 0051/0580] Loss: 0.13366  Avg Loss: 0.13819  Avg mIoU:  76.14  
[Epoch: 238] [Batch: 0101/0580] Loss: 0.13390  Avg Loss: 0.14052  Avg mIoU:  75.76  
[Epoch: 238] [Batch: 0151/0580] Loss: 0.15519  Avg Loss: 0.13941  Avg mIoU:  76.18  
[Epoch: 238] [Batch: 0201/0580] Loss: 0.12950  Avg Loss: 0.13757  Avg mIoU:  76.65  
[Epoch: 238] [Batch: 0251/0580] Loss: 0.17929  Avg Loss: 0.13594  Avg mIoU:  76.62  
[Epoch: 238] [Batch: 0301/0580] Loss: 0.11645  Avg Loss: 0.13701  Avg mIoU:  76.60  
