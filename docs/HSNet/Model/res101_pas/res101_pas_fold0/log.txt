
:======== Few-shot Seg. with CHM ========
|             datapath: ../Datasets_HPN         
|            benchmark: pascal                  
|              logpath: res101_pas_fold0        
|                  bsz: 20                      
|                   lr: 0.001                   
|                niter: 2000                    
|              nworker: 8                       
|                 load:                         
|                 fold: 0                       
|             backbone: resnet101               
:================================================

Backbone # param.: 44654608
Learner # param.: 2587394
Total # param.: 47242002
Using 4 GPUs (DataParallel)
[Epoch: 00] [Batch: 0001/0570] Loss: 0.69035  Avg Loss: 0.69035  Avg mIoU:   6.90  
[Epoch: 00] [Batch: 0051/0570] Loss: 0.39144  Avg Loss: 0.54246  Avg mIoU:   0.59  
[Epoch: 00] [Batch: 0101/0570] Loss: 0.36976  Avg Loss: 0.48876  Avg mIoU:  14.81  
[Epoch: 00] [Batch: 0151/0570] Loss: 0.44077  Avg Loss: 0.46353  Avg mIoU:  22.08  
[Epoch: 00] [Batch: 0201/0570] Loss: 0.40936  Avg Loss: 0.44898  Avg mIoU:  25.10  
[Epoch: 00] [Batch: 0251/0570] Loss: 0.84807  Avg Loss: 0.44145  Avg mIoU:  26.74  
[Epoch: 00] [Batch: 0301/0570] Loss: 0.39388  Avg Loss: 0.43586  Avg mIoU:  28.51  
[Epoch: 00] [Batch: 0351/0570] Loss: 0.41465  Avg Loss: 0.43035  Avg mIoU:  29.99  
[Epoch: 00] [Batch: 0401/0570] Loss: 0.38244  Avg Loss: 0.42482  Avg mIoU:  31.36  
[Epoch: 00] [Batch: 0451/0570] Loss: 0.38003  Avg Loss: 0.41900  Avg mIoU:  32.38  
[Epoch: 00] [Batch: 0501/0570] Loss: 0.42096  Avg Loss: 0.41505  Avg mIoU:  33.48  
[Epoch: 00] [Batch: 0551/0570] Loss: 0.35935  Avg Loss: 0.41160  Avg mIoU:  34.34  

*** Training [@Epoch 00] Avg Loss: 0.41019  Avg mIoU:  34.67  ***

[Epoch: 00] [Batch: 0001/0050] Loss: 0.33624  Avg Loss: 0.33624  Avg mIoU:  47.58  

*** Validation [@Epoch 00] Avg Loss: 0.31432  Avg mIoU:  47.45  ***

Model saved @0 w/ val. mIoU: 47.45.

[Epoch: 01] [Batch: 0001/0570] Loss: 0.38281  Avg Loss: 0.38281  Avg mIoU:  27.23  
[Epoch: 01] [Batch: 0051/0570] Loss: 0.38449  Avg Loss: 0.36308  Avg mIoU:  43.64  
[Epoch: 01] [Batch: 0101/0570] Loss: 0.41841  Avg Loss: 0.36596  Avg mIoU:  44.13  
[Epoch: 01] [Batch: 0151/0570] Loss: 0.38158  Avg Loss: 0.36458  Avg mIoU:  44.61  
[Epoch: 01] [Batch: 0201/0570] Loss: 0.29324  Avg Loss: 0.36061  Avg mIoU:  44.82  
[Epoch: 01] [Batch: 0251/0570] Loss: 0.31648  Avg Loss: 0.35604  Avg mIoU:  45.53  
[Epoch: 01] [Batch: 0301/0570] Loss: 0.31410  Avg Loss: 0.35577  Avg mIoU:  46.17  
[Epoch: 01] [Batch: 0351/0570] Loss: 0.42635  Avg Loss: 0.35449  Avg mIoU:  45.94  
[Epoch: 01] [Batch: 0401/0570] Loss: 0.37693  Avg Loss: 0.35360  Avg mIoU:  46.22  
[Epoch: 01] [Batch: 0451/0570] Loss: 0.35534  Avg Loss: 0.35158  Avg mIoU:  46.46  
[Epoch: 01] [Batch: 0501/0570] Loss: 0.35275  Avg Loss: 0.35188  Avg mIoU:  46.46  
[Epoch: 01] [Batch: 0551/0570] Loss: 0.32518  Avg Loss: 0.35053  Avg mIoU:  46.89  

*** Training [@Epoch 01] Avg Loss: 0.35073  Avg mIoU:  46.97  ***

[Epoch: 01] [Batch: 0001/0050] Loss: 0.23124  Avg Loss: 0.23124  Avg mIoU:  53.39  

*** Validation [@Epoch 01] Avg Loss: 0.22780  Avg mIoU:  52.36  ***

Model saved @1 w/ val. mIoU: 52.36.

[Epoch: 02] [Batch: 0001/0570] Loss: 0.26028  Avg Loss: 0.26028  Avg mIoU:  25.08  
[Epoch: 02] [Batch: 0051/0570] Loss: 0.28209  Avg Loss: 0.31793  Avg mIoU:  52.30  
[Epoch: 02] [Batch: 0101/0570] Loss: 0.29942  Avg Loss: 0.32826  Avg mIoU:  51.62  
[Epoch: 02] [Batch: 0151/0570] Loss: 0.37636  Avg Loss: 0.33175  Avg mIoU:  51.05  
[Epoch: 02] [Batch: 0201/0570] Loss: 0.32905  Avg Loss: 0.33005  Avg mIoU:  51.01  
[Epoch: 02] [Batch: 0251/0570] Loss: 0.36640  Avg Loss: 0.32975  Avg mIoU:  51.32  
[Epoch: 02] [Batch: 0301/0570] Loss: 0.32736  Avg Loss: 0.33044  Avg mIoU:  50.91  
[Epoch: 02] [Batch: 0351/0570] Loss: 0.36424  Avg Loss: 0.33173  Avg mIoU:  51.12  
[Epoch: 02] [Batch: 0401/0570] Loss: 0.26562  Avg Loss: 0.33094  Avg mIoU:  50.89  
[Epoch: 02] [Batch: 0451/0570] Loss: 0.32031  Avg Loss: 0.32834  Avg mIoU:  51.09  
[Epoch: 02] [Batch: 0501/0570] Loss: 0.27853  Avg Loss: 0.32842  Avg mIoU:  51.08  
[Epoch: 02] [Batch: 0551/0570] Loss: 0.24302  Avg Loss: 0.32787  Avg mIoU:  50.97  

*** Training [@Epoch 02] Avg Loss: 0.32738  Avg mIoU:  51.15  ***

[Epoch: 02] [Batch: 0001/0050] Loss: 0.26660  Avg Loss: 0.26660  Avg mIoU:  53.09  

*** Validation [@Epoch 02] Avg Loss: 0.23724  Avg mIoU:  54.21  ***

Model saved @2 w/ val. mIoU: 54.21.

[Epoch: 03] [Batch: 0001/0570] Loss: 0.35502  Avg Loss: 0.35502  Avg mIoU:  19.91  
[Epoch: 03] [Batch: 0051/0570] Loss: 0.31718  Avg Loss: 0.32906  Avg mIoU:  52.15  
[Epoch: 03] [Batch: 0101/0570] Loss: 0.32189  Avg Loss: 0.32663  Avg mIoU:  52.11  
[Epoch: 03] [Batch: 0151/0570] Loss: 0.34473  Avg Loss: 0.32185  Avg mIoU:  52.22  
[Epoch: 03] [Batch: 0201/0570] Loss: 0.28197  Avg Loss: 0.31906  Avg mIoU:  52.70  
[Epoch: 03] [Batch: 0251/0570] Loss: 0.35143  Avg Loss: 0.31604  Avg mIoU:  52.67  
[Epoch: 03] [Batch: 0301/0570] Loss: 0.31404  Avg Loss: 0.31727  Avg mIoU:  52.57  
[Epoch: 03] [Batch: 0351/0570] Loss: 0.29094  Avg Loss: 0.31558  Avg mIoU:  52.86  
[Epoch: 03] [Batch: 0401/0570] Loss: 0.35506  Avg Loss: 0.31330  Avg mIoU:  52.98  
[Epoch: 03] [Batch: 0451/0570] Loss: 0.31427  Avg Loss: 0.31273  Avg mIoU:  53.06  
[Epoch: 03] [Batch: 0501/0570] Loss: 0.29544  Avg Loss: 0.31245  Avg mIoU:  53.08  
[Epoch: 03] [Batch: 0551/0570] Loss: 0.39964  Avg Loss: 0.31193  Avg mIoU:  53.25  

*** Training [@Epoch 03] Avg Loss: 0.31232  Avg mIoU:  53.20  ***

[Epoch: 03] [Batch: 0001/0050] Loss: 0.20593  Avg Loss: 0.20593  Avg mIoU:  56.62  

*** Validation [@Epoch 03] Avg Loss: 0.20884  Avg mIoU:  56.47  ***

Model saved @3 w/ val. mIoU: 56.47.

[Epoch: 04] [Batch: 0001/0570] Loss: 0.27169  Avg Loss: 0.27169  Avg mIoU:  40.04  
[Epoch: 04] [Batch: 0051/0570] Loss: 0.43579  Avg Loss: 0.29656  Avg mIoU:  55.53  
[Epoch: 04] [Batch: 0101/0570] Loss: 0.26712  Avg Loss: 0.29708  Avg mIoU:  55.31  
[Epoch: 04] [Batch: 0151/0570] Loss: 0.36496  Avg Loss: 0.29869  Avg mIoU:  55.08  
[Epoch: 04] [Batch: 0201/0570] Loss: 0.32657  Avg Loss: 0.30153  Avg mIoU:  55.24  
[Epoch: 04] [Batch: 0251/0570] Loss: 0.42565  Avg Loss: 0.30084  Avg mIoU:  55.50  
[Epoch: 04] [Batch: 0301/0570] Loss: 0.27106  Avg Loss: 0.29930  Avg mIoU:  55.48  
[Epoch: 04] [Batch: 0351/0570] Loss: 0.32574  Avg Loss: 0.30055  Avg mIoU:  55.53  
[Epoch: 04] [Batch: 0401/0570] Loss: 0.41878  Avg Loss: 0.30049  Avg mIoU:  55.36  
[Epoch: 04] [Batch: 0451/0570] Loss: 0.25206  Avg Loss: 0.30045  Avg mIoU:  55.08  
[Epoch: 04] [Batch: 0501/0570] Loss: 0.25567  Avg Loss: 0.30019  Avg mIoU:  55.11  
[Epoch: 04] [Batch: 0551/0570] Loss: 0.28526  Avg Loss: 0.29987  Avg mIoU:  55.03  

*** Training [@Epoch 04] Avg Loss: 0.29924  Avg mIoU:  55.09  ***

[Epoch: 04] [Batch: 0001/0050] Loss: 0.23612  Avg Loss: 0.23612  Avg mIoU:  52.40  

*** Validation [@Epoch 04] Avg Loss: 0.21664  Avg mIoU:  55.48  ***

[Epoch: 05] [Batch: 0001/0570] Loss: 0.21922  Avg Loss: 0.21922  Avg mIoU:  31.30  
[Epoch: 05] [Batch: 0051/0570] Loss: 0.28967  Avg Loss: 0.30930  Avg mIoU:  55.22  
[Epoch: 05] [Batch: 0101/0570] Loss: 0.28683  Avg Loss: 0.31160  Avg mIoU:  55.57  
[Epoch: 05] [Batch: 0151/0570] Loss: 0.27649  Avg Loss: 0.30418  Avg mIoU:  55.69  
[Epoch: 05] [Batch: 0201/0570] Loss: 0.33590  Avg Loss: 0.30277  Avg mIoU:  55.67  
[Epoch: 05] [Batch: 0251/0570] Loss: 0.29551  Avg Loss: 0.30138  Avg mIoU:  55.70  
[Epoch: 05] [Batch: 0301/0570] Loss: 0.26793  Avg Loss: 0.29784  Avg mIoU:  55.80  
[Epoch: 05] [Batch: 0351/0570] Loss: 0.28065  Avg Loss: 0.29719  Avg mIoU:  55.65  
[Epoch: 05] [Batch: 0401/0570] Loss: 0.24929  Avg Loss: 0.29401  Avg mIoU:  55.56  
[Epoch: 05] [Batch: 0451/0570] Loss: 0.37749  Avg Loss: 0.29391  Avg mIoU:  55.42  
[Epoch: 05] [Batch: 0501/0570] Loss: 0.34881  Avg Loss: 0.29358  Avg mIoU:  55.52  
[Epoch: 05] [Batch: 0551/0570] Loss: 0.37871  Avg Loss: 0.29284  Avg mIoU:  55.63  

*** Training [@Epoch 05] Avg Loss: 0.29325  Avg mIoU:  55.62  ***

[Epoch: 05] [Batch: 0001/0050] Loss: 0.20999  Avg Loss: 0.20999  Avg mIoU:  56.81  

*** Validation [@Epoch 05] Avg Loss: 0.20588  Avg mIoU:  58.05  ***

Model saved @5 w/ val. mIoU: 58.05.

[Epoch: 06] [Batch: 0001/0570] Loss: 0.27484  Avg Loss: 0.27484  Avg mIoU:  37.27  
[Epoch: 06] [Batch: 0051/0570] Loss: 0.38608  Avg Loss: 0.28858  Avg mIoU:  57.67  
[Epoch: 06] [Batch: 0101/0570] Loss: 0.30472  Avg Loss: 0.29214  Avg mIoU:  55.84  
[Epoch: 06] [Batch: 0151/0570] Loss: 0.29086  Avg Loss: 0.29281  Avg mIoU:  55.21  
[Epoch: 06] [Batch: 0201/0570] Loss: 0.29587  Avg Loss: 0.28686  Avg mIoU:  55.42  
[Epoch: 06] [Batch: 0251/0570] Loss: 0.27177  Avg Loss: 0.28520  Avg mIoU:  55.77  
[Epoch: 06] [Batch: 0301/0570] Loss: 0.30280  Avg Loss: 0.28601  Avg mIoU:  56.01  
[Epoch: 06] [Batch: 0351/0570] Loss: 0.28936  Avg Loss: 0.28537  Avg mIoU:  55.99  
[Epoch: 06] [Batch: 0401/0570] Loss: 0.29526  Avg Loss: 0.28613  Avg mIoU:  55.88  
[Epoch: 06] [Batch: 0451/0570] Loss: 0.29503  Avg Loss: 0.28532  Avg mIoU:  55.80  
[Epoch: 06] [Batch: 0501/0570] Loss: 0.26348  Avg Loss: 0.28661  Avg mIoU:  55.71  
[Epoch: 06] [Batch: 0551/0570] Loss: 0.27174  Avg Loss: 0.28769  Avg mIoU:  55.85  

*** Training [@Epoch 06] Avg Loss: 0.28702  Avg mIoU:  55.86  ***

[Epoch: 06] [Batch: 0001/0050] Loss: 0.21014  Avg Loss: 0.21014  Avg mIoU:  56.13  

*** Validation [@Epoch 06] Avg Loss: 0.19970  Avg mIoU:  57.34  ***

[Epoch: 07] [Batch: 0001/0570] Loss: 0.23072  Avg Loss: 0.23072  Avg mIoU:  36.38  
[Epoch: 07] [Batch: 0051/0570] Loss: 0.35353  Avg Loss: 0.27618  Avg mIoU:  56.02  
[Epoch: 07] [Batch: 0101/0570] Loss: 0.30821  Avg Loss: 0.27732  Avg mIoU:  56.94  
[Epoch: 07] [Batch: 0151/0570] Loss: 0.22780  Avg Loss: 0.28122  Avg mIoU:  56.86  
[Epoch: 07] [Batch: 0201/0570] Loss: 0.24959  Avg Loss: 0.27900  Avg mIoU:  56.88  
[Epoch: 07] [Batch: 0251/0570] Loss: 0.36821  Avg Loss: 0.27946  Avg mIoU:  56.86  
[Epoch: 07] [Batch: 0301/0570] Loss: 0.23211  Avg Loss: 0.28028  Avg mIoU:  57.01  
[Epoch: 07] [Batch: 0351/0570] Loss: 0.25531  Avg Loss: 0.27859  Avg mIoU:  57.31  
[Epoch: 07] [Batch: 0401/0570] Loss: 0.24185  Avg Loss: 0.27965  Avg mIoU:  57.32  
[Epoch: 07] [Batch: 0451/0570] Loss: 0.20400  Avg Loss: 0.27937  Avg mIoU:  57.43  
[Epoch: 07] [Batch: 0501/0570] Loss: 0.22974  Avg Loss: 0.27844  Avg mIoU:  57.28  
[Epoch: 07] [Batch: 0551/0570] Loss: 0.26700  Avg Loss: 0.27925  Avg mIoU:  57.27  

*** Training [@Epoch 07] Avg Loss: 0.27965  Avg mIoU:  57.31  ***

[Epoch: 07] [Batch: 0001/0050] Loss: 0.17769  Avg Loss: 0.17769  Avg mIoU:  54.87  

*** Validation [@Epoch 07] Avg Loss: 0.18010  Avg mIoU:  53.60  ***

[Epoch: 08] [Batch: 0001/0570] Loss: 0.27321  Avg Loss: 0.27321  Avg mIoU:  36.42  
[Epoch: 08] [Batch: 0051/0570] Loss: 0.32515  Avg Loss: 0.27667  Avg mIoU:  58.69  
[Epoch: 08] [Batch: 0101/0570] Loss: 0.35253  Avg Loss: 0.27277  Avg mIoU:  59.27  
[Epoch: 08] [Batch: 0151/0570] Loss: 0.29464  Avg Loss: 0.27408  Avg mIoU:  58.46  
[Epoch: 08] [Batch: 0201/0570] Loss: 0.21605  Avg Loss: 0.27627  Avg mIoU:  58.03  
[Epoch: 08] [Batch: 0251/0570] Loss: 0.25316  Avg Loss: 0.27574  Avg mIoU:  57.69  
[Epoch: 08] [Batch: 0301/0570] Loss: 0.38001  Avg Loss: 0.27627  Avg mIoU:  57.71  
[Epoch: 08] [Batch: 0351/0570] Loss: 0.18459  Avg Loss: 0.27491  Avg mIoU:  57.84  
[Epoch: 08] [Batch: 0401/0570] Loss: 0.18183  Avg Loss: 0.27352  Avg mIoU:  57.96  
[Epoch: 08] [Batch: 0451/0570] Loss: 0.21872  Avg Loss: 0.27323  Avg mIoU:  58.23  
[Epoch: 08] [Batch: 0501/0570] Loss: 0.22955  Avg Loss: 0.27285  Avg mIoU:  58.30  
[Epoch: 08] [Batch: 0551/0570] Loss: 0.21839  Avg Loss: 0.27274  Avg mIoU:  58.23  

*** Training [@Epoch 08] Avg Loss: 0.27291  Avg mIoU:  58.07  ***

[Epoch: 08] [Batch: 0001/0050] Loss: 0.21646  Avg Loss: 0.21646  Avg mIoU:  59.29  

*** Validation [@Epoch 08] Avg Loss: 0.20482  Avg mIoU:  58.40  ***

Model saved @8 w/ val. mIoU: 58.40.

[Epoch: 09] [Batch: 0001/0570] Loss: 0.23110  Avg Loss: 0.23110  Avg mIoU:  30.29  
[Epoch: 09] [Batch: 0051/0570] Loss: 0.22885  Avg Loss: 0.26090  Avg mIoU:  59.60  
[Epoch: 09] [Batch: 0101/0570] Loss: 0.21625  Avg Loss: 0.26284  Avg mIoU:  59.30  
[Epoch: 09] [Batch: 0151/0570] Loss: 0.27316  Avg Loss: 0.26713  Avg mIoU:  58.85  
[Epoch: 09] [Batch: 0201/0570] Loss: 0.36361  Avg Loss: 0.26801  Avg mIoU:  59.29  
[Epoch: 09] [Batch: 0251/0570] Loss: 0.28098  Avg Loss: 0.26539  Avg mIoU:  59.45  
[Epoch: 09] [Batch: 0301/0570] Loss: 0.32835  Avg Loss: 0.26590  Avg mIoU:  58.97  
[Epoch: 09] [Batch: 0351/0570] Loss: 0.38038  Avg Loss: 0.26771  Avg mIoU:  58.59  
[Epoch: 09] [Batch: 0401/0570] Loss: 0.27247  Avg Loss: 0.26783  Avg mIoU:  58.73  
[Epoch: 09] [Batch: 0451/0570] Loss: 0.24420  Avg Loss: 0.26888  Avg mIoU:  58.54  
[Epoch: 09] [Batch: 0501/0570] Loss: 0.20093  Avg Loss: 0.26852  Avg mIoU:  58.71  
[Epoch: 09] [Batch: 0551/0570] Loss: 0.30639  Avg Loss: 0.26912  Avg mIoU:  58.68  

*** Training [@Epoch 09] Avg Loss: 0.26872  Avg mIoU:  58.85  ***

[Epoch: 09] [Batch: 0001/0050] Loss: 0.19720  Avg Loss: 0.19720  Avg mIoU:  58.60  

*** Validation [@Epoch 09] Avg Loss: 0.18435  Avg mIoU:  60.28  ***

Model saved @9 w/ val. mIoU: 60.28.

[Epoch: 10] [Batch: 0001/0570] Loss: 0.30515  Avg Loss: 0.30515  Avg mIoU:  45.93  
[Epoch: 10] [Batch: 0051/0570] Loss: 0.28550  Avg Loss: 0.28726  Avg mIoU:  55.41  
[Epoch: 10] [Batch: 0101/0570] Loss: 0.26096  Avg Loss: 0.27272  Avg mIoU:  58.62  
[Epoch: 10] [Batch: 0151/0570] Loss: 0.23940  Avg Loss: 0.27261  Avg mIoU:  59.12  
[Epoch: 10] [Batch: 0201/0570] Loss: 0.20553  Avg Loss: 0.26919  Avg mIoU:  58.96  
[Epoch: 10] [Batch: 0251/0570] Loss: 0.21220  Avg Loss: 0.26734  Avg mIoU:  59.38  
[Epoch: 10] [Batch: 0301/0570] Loss: 0.22556  Avg Loss: 0.26692  Avg mIoU:  59.57  
[Epoch: 10] [Batch: 0351/0570] Loss: 0.24853  Avg Loss: 0.26596  Avg mIoU:  59.27  
[Epoch: 10] [Batch: 0401/0570] Loss: 0.30708  Avg Loss: 0.26561  Avg mIoU:  59.31  
[Epoch: 10] [Batch: 0451/0570] Loss: 0.19686  Avg Loss: 0.26521  Avg mIoU:  59.16  
[Epoch: 10] [Batch: 0501/0570] Loss: 0.28447  Avg Loss: 0.26507  Avg mIoU:  59.24  
[Epoch: 10] [Batch: 0551/0570] Loss: 0.25164  Avg Loss: 0.26406  Avg mIoU:  59.25  

*** Training [@Epoch 10] Avg Loss: 0.26406  Avg mIoU:  59.16  ***

[Epoch: 10] [Batch: 0001/0050] Loss: 0.19290  Avg Loss: 0.19290  Avg mIoU:  57.98  

*** Validation [@Epoch 10] Avg Loss: 0.18667  Avg mIoU:  58.41  ***

[Epoch: 11] [Batch: 0001/0570] Loss: 0.27170  Avg Loss: 0.27170  Avg mIoU:  25.85  
[Epoch: 11] [Batch: 0051/0570] Loss: 0.25034  Avg Loss: 0.26607  Avg mIoU:  60.69  
[Epoch: 11] [Batch: 0101/0570] Loss: 0.28616  Avg Loss: 0.26002  Avg mIoU:  59.64  
[Epoch: 11] [Batch: 0151/0570] Loss: 0.27307  Avg Loss: 0.25952  Avg mIoU:  59.53  
[Epoch: 11] [Batch: 0201/0570] Loss: 0.30402  Avg Loss: 0.26044  Avg mIoU:  59.82  
[Epoch: 11] [Batch: 0251/0570] Loss: 0.15668  Avg Loss: 0.25951  Avg mIoU:  59.98  
[Epoch: 11] [Batch: 0301/0570] Loss: 0.21232  Avg Loss: 0.25941  Avg mIoU:  60.06  
[Epoch: 11] [Batch: 0351/0570] Loss: 0.25986  Avg Loss: 0.25811  Avg mIoU:  60.37  
[Epoch: 11] [Batch: 0401/0570] Loss: 0.22667  Avg Loss: 0.25796  Avg mIoU:  60.23  
[Epoch: 11] [Batch: 0451/0570] Loss: 0.24262  Avg Loss: 0.25823  Avg mIoU:  60.04  
[Epoch: 11] [Batch: 0501/0570] Loss: 0.28085  Avg Loss: 0.26003  Avg mIoU:  59.87  
[Epoch: 11] [Batch: 0551/0570] Loss: 0.24926  Avg Loss: 0.25980  Avg mIoU:  59.76  

*** Training [@Epoch 11] Avg Loss: 0.25954  Avg mIoU:  59.82  ***

[Epoch: 11] [Batch: 0001/0050] Loss: 0.22158  Avg Loss: 0.22158  Avg mIoU:  57.90  

*** Validation [@Epoch 11] Avg Loss: 0.20899  Avg mIoU:  59.34  ***

[Epoch: 12] [Batch: 0001/0570] Loss: 0.24722  Avg Loss: 0.24722  Avg mIoU:  39.04  
[Epoch: 12] [Batch: 0051/0570] Loss: 0.32480  Avg Loss: 0.25823  Avg mIoU:  61.16  
[Epoch: 12] [Batch: 0101/0570] Loss: 0.24549  Avg Loss: 0.25934  Avg mIoU:  60.04  
[Epoch: 12] [Batch: 0151/0570] Loss: 0.28197  Avg Loss: 0.25714  Avg mIoU:  59.35  
[Epoch: 12] [Batch: 0201/0570] Loss: 0.19188  Avg Loss: 0.25355  Avg mIoU:  59.58  
[Epoch: 12] [Batch: 0251/0570] Loss: 0.23815  Avg Loss: 0.25542  Avg mIoU:  59.62  
[Epoch: 12] [Batch: 0301/0570] Loss: 0.21715  Avg Loss: 0.25653  Avg mIoU:  59.55  
[Epoch: 12] [Batch: 0351/0570] Loss: 0.40106  Avg Loss: 0.25721  Avg mIoU:  59.50  
[Epoch: 12] [Batch: 0401/0570] Loss: 0.27093  Avg Loss: 0.25772  Avg mIoU:  59.68  
[Epoch: 12] [Batch: 0451/0570] Loss: 0.18802  Avg Loss: 0.25755  Avg mIoU:  59.71  
[Epoch: 12] [Batch: 0501/0570] Loss: 0.37521  Avg Loss: 0.25685  Avg mIoU:  59.96  
[Epoch: 12] [Batch: 0551/0570] Loss: 0.20262  Avg Loss: 0.25626  Avg mIoU:  60.03  

*** Training [@Epoch 12] Avg Loss: 0.25632  Avg mIoU:  59.96  ***

[Epoch: 12] [Batch: 0001/0050] Loss: 0.18592  Avg Loss: 0.18592  Avg mIoU:  58.86  

*** Validation [@Epoch 12] Avg Loss: 0.18314  Avg mIoU:  60.22  ***

[Epoch: 13] [Batch: 0001/0570] Loss: 0.19257  Avg Loss: 0.19257  Avg mIoU:  39.38  
[Epoch: 13] [Batch: 0051/0570] Loss: 0.30575  Avg Loss: 0.25786  Avg mIoU:  60.93  
[Epoch: 13] [Batch: 0101/0570] Loss: 0.25234  Avg Loss: 0.25589  Avg mIoU:  61.22  
[Epoch: 13] [Batch: 0151/0570] Loss: 0.15589  Avg Loss: 0.25628  Avg mIoU:  60.68  
[Epoch: 13] [Batch: 0201/0570] Loss: 0.19985  Avg Loss: 0.25547  Avg mIoU:  60.57  
[Epoch: 13] [Batch: 0251/0570] Loss: 0.21398  Avg Loss: 0.25652  Avg mIoU:  60.42  
[Epoch: 13] [Batch: 0301/0570] Loss: 0.23133  Avg Loss: 0.25641  Avg mIoU:  60.34  
[Epoch: 13] [Batch: 0351/0570] Loss: 0.19977  Avg Loss: 0.25490  Avg mIoU:  60.59  
[Epoch: 13] [Batch: 0401/0570] Loss: 0.12043  Avg Loss: 0.25558  Avg mIoU:  60.57  
[Epoch: 13] [Batch: 0451/0570] Loss: 0.21834  Avg Loss: 0.25679  Avg mIoU:  60.53  
[Epoch: 13] [Batch: 0501/0570] Loss: 0.19847  Avg Loss: 0.25717  Avg mIoU:  60.54  
[Epoch: 13] [Batch: 0551/0570] Loss: 0.24790  Avg Loss: 0.25716  Avg mIoU:  60.63  

*** Training [@Epoch 13] Avg Loss: 0.25687  Avg mIoU:  60.65  ***

[Epoch: 13] [Batch: 0001/0050] Loss: 0.18146  Avg Loss: 0.18146  Avg mIoU:  61.02  

*** Validation [@Epoch 13] Avg Loss: 0.17557  Avg mIoU:  61.68  ***

Model saved @13 w/ val. mIoU: 61.68.

[Epoch: 14] [Batch: 0001/0570] Loss: 0.26331  Avg Loss: 0.26331  Avg mIoU:  27.35  
[Epoch: 14] [Batch: 0051/0570] Loss: 0.23971  Avg Loss: 0.25950  Avg mIoU:  61.24  
[Epoch: 14] [Batch: 0101/0570] Loss: 0.22551  Avg Loss: 0.26164  Avg mIoU:  60.87  
[Epoch: 14] [Batch: 0151/0570] Loss: 0.28552  Avg Loss: 0.25755  Avg mIoU:  60.52  
[Epoch: 14] [Batch: 0201/0570] Loss: 0.24229  Avg Loss: 0.25425  Avg mIoU:  60.25  
[Epoch: 14] [Batch: 0251/0570] Loss: 0.25244  Avg Loss: 0.25513  Avg mIoU:  60.20  
[Epoch: 14] [Batch: 0301/0570] Loss: 0.23353  Avg Loss: 0.25361  Avg mIoU:  60.37  
[Epoch: 14] [Batch: 0351/0570] Loss: 0.19689  Avg Loss: 0.25175  Avg mIoU:  60.66  
[Epoch: 14] [Batch: 0401/0570] Loss: 0.27728  Avg Loss: 0.25174  Avg mIoU:  60.69  
[Epoch: 14] [Batch: 0451/0570] Loss: 0.27797  Avg Loss: 0.25154  Avg mIoU:  60.65  
[Epoch: 14] [Batch: 0501/0570] Loss: 0.23332  Avg Loss: 0.25056  Avg mIoU:  60.78  
[Epoch: 14] [Batch: 0551/0570] Loss: 0.19996  Avg Loss: 0.24934  Avg mIoU:  60.86  

*** Training [@Epoch 14] Avg Loss: 0.24991  Avg mIoU:  60.70  ***

[Epoch: 14] [Batch: 0001/0050] Loss: 0.20883  Avg Loss: 0.20883  Avg mIoU:  58.25  

*** Validation [@Epoch 14] Avg Loss: 0.20157  Avg mIoU:  60.18  ***

[Epoch: 15] [Batch: 0001/0570] Loss: 0.17787  Avg Loss: 0.17787  Avg mIoU:  31.24  
[Epoch: 15] [Batch: 0051/0570] Loss: 0.19279  Avg Loss: 0.25629  Avg mIoU:  59.41  
[Epoch: 15] [Batch: 0101/0570] Loss: 0.24719  Avg Loss: 0.25310  Avg mIoU:  60.49  
[Epoch: 15] [Batch: 0151/0570] Loss: 0.28365  Avg Loss: 0.24730  Avg mIoU:  61.44  
[Epoch: 15] [Batch: 0201/0570] Loss: 0.20192  Avg Loss: 0.24919  Avg mIoU:  61.44  
[Epoch: 15] [Batch: 0251/0570] Loss: 0.22596  Avg Loss: 0.24598  Avg mIoU:  61.71  
[Epoch: 15] [Batch: 0301/0570] Loss: 0.22458  Avg Loss: 0.24505  Avg mIoU:  62.00  
[Epoch: 15] [Batch: 0351/0570] Loss: 0.34212  Avg Loss: 0.24486  Avg mIoU:  61.89  
[Epoch: 15] [Batch: 0401/0570] Loss: 0.31046  Avg Loss: 0.24623  Avg mIoU:  61.76  
[Epoch: 15] [Batch: 0451/0570] Loss: 0.17431  Avg Loss: 0.24530  Avg mIoU:  61.78  
[Epoch: 15] [Batch: 0501/0570] Loss: 0.22484  Avg Loss: 0.24475  Avg mIoU:  61.63  
[Epoch: 15] [Batch: 0551/0570] Loss: 0.19618  Avg Loss: 0.24509  Avg mIoU:  61.75  

*** Training [@Epoch 15] Avg Loss: 0.24560  Avg mIoU:  61.79  ***

[Epoch: 15] [Batch: 0001/0050] Loss: 0.20768  Avg Loss: 0.20768  Avg mIoU:  60.34  

*** Validation [@Epoch 15] Avg Loss: 0.20444  Avg mIoU:  61.06  ***

[Epoch: 16] [Batch: 0001/0570] Loss: 0.30242  Avg Loss: 0.30242  Avg mIoU:  37.21  
[Epoch: 16] [Batch: 0051/0570] Loss: 0.21190  Avg Loss: 0.25471  Avg mIoU:  60.97  
[Epoch: 16] [Batch: 0101/0570] Loss: 0.29329  Avg Loss: 0.24495  Avg mIoU:  61.69  
[Epoch: 16] [Batch: 0151/0570] Loss: 0.28309  Avg Loss: 0.24679  Avg mIoU:  61.70  
[Epoch: 16] [Batch: 0201/0570] Loss: 0.26611  Avg Loss: 0.24829  Avg mIoU:  61.37  
[Epoch: 16] [Batch: 0251/0570] Loss: 0.23182  Avg Loss: 0.24891  Avg mIoU:  61.34  
[Epoch: 16] [Batch: 0301/0570] Loss: 0.16444  Avg Loss: 0.24771  Avg mIoU:  61.41  
[Epoch: 16] [Batch: 0351/0570] Loss: 0.31963  Avg Loss: 0.24739  Avg mIoU:  61.25  
[Epoch: 16] [Batch: 0401/0570] Loss: 0.32820  Avg Loss: 0.24791  Avg mIoU:  61.23  
[Epoch: 16] [Batch: 0451/0570] Loss: 0.22932  Avg Loss: 0.24811  Avg mIoU:  61.36  
[Epoch: 16] [Batch: 0501/0570] Loss: 0.31290  Avg Loss: 0.24769  Avg mIoU:  61.41  
[Epoch: 16] [Batch: 0551/0570] Loss: 0.27826  Avg Loss: 0.24753  Avg mIoU:  61.41  

*** Training [@Epoch 16] Avg Loss: 0.24731  Avg mIoU:  61.38  ***

[Epoch: 16] [Batch: 0001/0050] Loss: 0.18505  Avg Loss: 0.18505  Avg mIoU:  61.12  

*** Validation [@Epoch 16] Avg Loss: 0.18014  Avg mIoU:  59.98  ***

[Epoch: 17] [Batch: 0001/0570] Loss: 0.27484  Avg Loss: 0.27484  Avg mIoU:  31.50  
[Epoch: 17] [Batch: 0051/0570] Loss: 0.23742  Avg Loss: 0.25729  Avg mIoU:  61.06  
[Epoch: 17] [Batch: 0101/0570] Loss: 0.21890  Avg Loss: 0.24961  Avg mIoU:  61.26  
[Epoch: 17] [Batch: 0151/0570] Loss: 0.26381  Avg Loss: 0.24721  Avg mIoU:  61.89  
[Epoch: 17] [Batch: 0201/0570] Loss: 0.19548  Avg Loss: 0.24651  Avg mIoU:  61.41  
[Epoch: 17] [Batch: 0251/0570] Loss: 0.32804  Avg Loss: 0.24992  Avg mIoU:  61.15  
[Epoch: 17] [Batch: 0301/0570] Loss: 0.20391  Avg Loss: 0.24893  Avg mIoU:  61.26  
[Epoch: 17] [Batch: 0351/0570] Loss: 0.32251  Avg Loss: 0.24658  Avg mIoU:  61.42  
[Epoch: 17] [Batch: 0401/0570] Loss: 0.26596  Avg Loss: 0.24477  Avg mIoU:  61.52  
[Epoch: 17] [Batch: 0451/0570] Loss: 0.21824  Avg Loss: 0.24368  Avg mIoU:  61.89  
[Epoch: 17] [Batch: 0501/0570] Loss: 0.17618  Avg Loss: 0.24281  Avg mIoU:  62.12  
[Epoch: 17] [Batch: 0551/0570] Loss: 0.23725  Avg Loss: 0.24277  Avg mIoU:  62.11  

*** Training [@Epoch 17] Avg Loss: 0.24257  Avg mIoU:  62.14  ***

[Epoch: 17] [Batch: 0001/0050] Loss: 0.16960  Avg Loss: 0.16960  Avg mIoU:  61.24  

*** Validation [@Epoch 17] Avg Loss: 0.16990  Avg mIoU:  61.37  ***

[Epoch: 18] [Batch: 0001/0570] Loss: 0.24749  Avg Loss: 0.24749  Avg mIoU:  30.41  
[Epoch: 18] [Batch: 0051/0570] Loss: 0.29590  Avg Loss: 0.24464  Avg mIoU:  61.76  
[Epoch: 18] [Batch: 0101/0570] Loss: 0.25785  Avg Loss: 0.24344  Avg mIoU:  61.00  
[Epoch: 18] [Batch: 0151/0570] Loss: 0.23920  Avg Loss: 0.24440  Avg mIoU:  61.59  
[Epoch: 18] [Batch: 0201/0570] Loss: 0.16052  Avg Loss: 0.24528  Avg mIoU:  61.70  
[Epoch: 18] [Batch: 0251/0570] Loss: 0.19940  Avg Loss: 0.24542  Avg mIoU:  61.91  
[Epoch: 18] [Batch: 0301/0570] Loss: 0.18158  Avg Loss: 0.24274  Avg mIoU:  62.14  
[Epoch: 18] [Batch: 0351/0570] Loss: 0.29779  Avg Loss: 0.24323  Avg mIoU:  62.06  
[Epoch: 18] [Batch: 0401/0570] Loss: 0.26234  Avg Loss: 0.24113  Avg mIoU:  62.18  
[Epoch: 18] [Batch: 0451/0570] Loss: 0.34277  Avg Loss: 0.24156  Avg mIoU:  62.27  
[Epoch: 18] [Batch: 0501/0570] Loss: 0.22996  Avg Loss: 0.24129  Avg mIoU:  62.22  
[Epoch: 18] [Batch: 0551/0570] Loss: 0.26775  Avg Loss: 0.24141  Avg mIoU:  62.14  

*** Training [@Epoch 18] Avg Loss: 0.24094  Avg mIoU:  62.16  ***

[Epoch: 18] [Batch: 0001/0050] Loss: 0.16934  Avg Loss: 0.16934  Avg mIoU:  64.33  

*** Validation [@Epoch 18] Avg Loss: 0.17672  Avg mIoU:  61.32  ***

[Epoch: 19] [Batch: 0001/0570] Loss: 0.21976  Avg Loss: 0.21976  Avg mIoU:  41.19  
[Epoch: 19] [Batch: 0051/0570] Loss: 0.24263  Avg Loss: 0.24332  Avg mIoU:  60.87  
[Epoch: 19] [Batch: 0101/0570] Loss: 0.19573  Avg Loss: 0.23881  Avg mIoU:  62.37  
[Epoch: 19] [Batch: 0151/0570] Loss: 0.25417  Avg Loss: 0.23745  Avg mIoU:  63.03  
[Epoch: 19] [Batch: 0201/0570] Loss: 0.21889  Avg Loss: 0.24042  Avg mIoU:  62.45  
[Epoch: 19] [Batch: 0251/0570] Loss: 0.15371  Avg Loss: 0.24091  Avg mIoU:  62.46  
[Epoch: 19] [Batch: 0301/0570] Loss: 0.21717  Avg Loss: 0.24093  Avg mIoU:  62.32  
[Epoch: 19] [Batch: 0351/0570] Loss: 0.29005  Avg Loss: 0.24223  Avg mIoU:  62.32  
[Epoch: 19] [Batch: 0401/0570] Loss: 0.32605  Avg Loss: 0.24188  Avg mIoU:  62.34  
[Epoch: 19] [Batch: 0451/0570] Loss: 0.19339  Avg Loss: 0.24104  Avg mIoU:  62.47  
[Epoch: 19] [Batch: 0501/0570] Loss: 0.16237  Avg Loss: 0.24123  Avg mIoU:  62.30  
[Epoch: 19] [Batch: 0551/0570] Loss: 0.29849  Avg Loss: 0.24000  Avg mIoU:  62.37  

*** Training [@Epoch 19] Avg Loss: 0.23972  Avg mIoU:  62.40  ***

[Epoch: 19] [Batch: 0001/0050] Loss: 0.16644  Avg Loss: 0.16644  Avg mIoU:  57.70  

*** Validation [@Epoch 19] Avg Loss: 0.16828  Avg mIoU:  58.62  ***

[Epoch: 20] [Batch: 0001/0570] Loss: 0.19998  Avg Loss: 0.19998  Avg mIoU:  32.35  
[Epoch: 20] [Batch: 0051/0570] Loss: 0.20458  Avg Loss: 0.24115  Avg mIoU:  62.11  
[Epoch: 20] [Batch: 0101/0570] Loss: 0.30426  Avg Loss: 0.23980  Avg mIoU:  62.03  
[Epoch: 20] [Batch: 0151/0570] Loss: 0.26068  Avg Loss: 0.23863  Avg mIoU:  62.32  
[Epoch: 20] [Batch: 0201/0570] Loss: 0.26620  Avg Loss: 0.23855  Avg mIoU:  62.71  
[Epoch: 20] [Batch: 0251/0570] Loss: 0.23518  Avg Loss: 0.23691  Avg mIoU:  62.66  
[Epoch: 20] [Batch: 0301/0570] Loss: 0.25321  Avg Loss: 0.23699  Avg mIoU:  62.66  
[Epoch: 20] [Batch: 0351/0570] Loss: 0.20386  Avg Loss: 0.23596  Avg mIoU:  63.09  
[Epoch: 20] [Batch: 0401/0570] Loss: 0.23535  Avg Loss: 0.23548  Avg mIoU:  63.01  
[Epoch: 20] [Batch: 0451/0570] Loss: 0.33728  Avg Loss: 0.23509  Avg mIoU:  63.10  
[Epoch: 20] [Batch: 0501/0570] Loss: 0.25741  Avg Loss: 0.23545  Avg mIoU:  62.69  
[Epoch: 20] [Batch: 0551/0570] Loss: 0.26172  Avg Loss: 0.23558  Avg mIoU:  62.85  

*** Training [@Epoch 20] Avg Loss: 0.23587  Avg mIoU:  62.84  ***

[Epoch: 20] [Batch: 0001/0050] Loss: 0.22554  Avg Loss: 0.22554  Avg mIoU:  58.95  

*** Validation [@Epoch 20] Avg Loss: 0.21156  Avg mIoU:  60.28  ***

[Epoch: 21] [Batch: 0001/0570] Loss: 0.21519  Avg Loss: 0.21519  Avg mIoU:  35.86  
[Epoch: 21] [Batch: 0051/0570] Loss: 0.30875  Avg Loss: 0.23586  Avg mIoU:  62.16  
[Epoch: 21] [Batch: 0101/0570] Loss: 0.14916  Avg Loss: 0.23794  Avg mIoU:  62.49  
[Epoch: 21] [Batch: 0151/0570] Loss: 0.31654  Avg Loss: 0.24079  Avg mIoU:  62.76  
[Epoch: 21] [Batch: 0201/0570] Loss: 0.18430  Avg Loss: 0.23909  Avg mIoU:  62.76  
[Epoch: 21] [Batch: 0251/0570] Loss: 0.24922  Avg Loss: 0.23670  Avg mIoU:  63.20  
[Epoch: 21] [Batch: 0301/0570] Loss: 0.18335  Avg Loss: 0.23395  Avg mIoU:  63.20  
[Epoch: 21] [Batch: 0351/0570] Loss: 0.25594  Avg Loss: 0.23373  Avg mIoU:  63.16  
[Epoch: 21] [Batch: 0401/0570] Loss: 0.24579  Avg Loss: 0.23352  Avg mIoU:  63.12  
[Epoch: 21] [Batch: 0451/0570] Loss: 0.20507  Avg Loss: 0.23317  Avg mIoU:  63.34  
[Epoch: 21] [Batch: 0501/0570] Loss: 0.29568  Avg Loss: 0.23442  Avg mIoU:  63.17  
[Epoch: 21] [Batch: 0551/0570] Loss: 0.17071  Avg Loss: 0.23418  Avg mIoU:  63.01  

*** Training [@Epoch 21] Avg Loss: 0.23430  Avg mIoU:  63.07  ***

[Epoch: 21] [Batch: 0001/0050] Loss: 0.17851  Avg Loss: 0.17851  Avg mIoU:  58.93  

*** Validation [@Epoch 21] Avg Loss: 0.17318  Avg mIoU:  60.08  ***

[Epoch: 22] [Batch: 0001/0570] Loss: 0.15719  Avg Loss: 0.15719  Avg mIoU:  47.26  
[Epoch: 22] [Batch: 0051/0570] Loss: 0.21252  Avg Loss: 0.21508  Avg mIoU:  63.62  
[Epoch: 22] [Batch: 0101/0570] Loss: 0.26679  Avg Loss: 0.22253  Avg mIoU:  63.85  
[Epoch: 22] [Batch: 0151/0570] Loss: 0.22692  Avg Loss: 0.22430  Avg mIoU:  63.86  
[Epoch: 22] [Batch: 0201/0570] Loss: 0.23572  Avg Loss: 0.22737  Avg mIoU:  63.45  
[Epoch: 22] [Batch: 0251/0570] Loss: 0.26448  Avg Loss: 0.22943  Avg mIoU:  63.43  
[Epoch: 22] [Batch: 0301/0570] Loss: 0.23245  Avg Loss: 0.22849  Avg mIoU:  63.76  
[Epoch: 22] [Batch: 0351/0570] Loss: 0.26661  Avg Loss: 0.23022  Avg mIoU:  63.86  
[Epoch: 22] [Batch: 0401/0570] Loss: 0.18795  Avg Loss: 0.23135  Avg mIoU:  64.01  
[Epoch: 22] [Batch: 0451/0570] Loss: 0.17325  Avg Loss: 0.23178  Avg mIoU:  63.89  
[Epoch: 22] [Batch: 0501/0570] Loss: 0.29371  Avg Loss: 0.23260  Avg mIoU:  63.69  
[Epoch: 22] [Batch: 0551/0570] Loss: 0.24035  Avg Loss: 0.23268  Avg mIoU:  63.82  

*** Training [@Epoch 22] Avg Loss: 0.23226  Avg mIoU:  63.91  ***

[Epoch: 22] [Batch: 0001/0050] Loss: 0.17213  Avg Loss: 0.17213  Avg mIoU:  60.98  

*** Validation [@Epoch 22] Avg Loss: 0.16969  Avg mIoU:  62.18  ***

Model saved @22 w/ val. mIoU: 62.18.

[Epoch: 23] [Batch: 0001/0570] Loss: 0.20929  Avg Loss: 0.20929  Avg mIoU:  33.21  
[Epoch: 23] [Batch: 0051/0570] Loss: 0.26295  Avg Loss: 0.23199  Avg mIoU:  61.54  
[Epoch: 23] [Batch: 0101/0570] Loss: 0.25804  Avg Loss: 0.22919  Avg mIoU:  63.61  
[Epoch: 23] [Batch: 0151/0570] Loss: 0.19143  Avg Loss: 0.22957  Avg mIoU:  63.44  
[Epoch: 23] [Batch: 0201/0570] Loss: 0.26631  Avg Loss: 0.22971  Avg mIoU:  63.72  
[Epoch: 23] [Batch: 0251/0570] Loss: 0.20337  Avg Loss: 0.22823  Avg mIoU:  64.02  
[Epoch: 23] [Batch: 0301/0570] Loss: 0.22165  Avg Loss: 0.22886  Avg mIoU:  64.15  
[Epoch: 23] [Batch: 0351/0570] Loss: 0.21331  Avg Loss: 0.23120  Avg mIoU:  63.70  
[Epoch: 23] [Batch: 0401/0570] Loss: 0.17308  Avg Loss: 0.23092  Avg mIoU:  63.65  
[Epoch: 23] [Batch: 0451/0570] Loss: 0.33746  Avg Loss: 0.23131  Avg mIoU:  63.55  
[Epoch: 23] [Batch: 0501/0570] Loss: 0.18840  Avg Loss: 0.23226  Avg mIoU:  63.49  
[Epoch: 23] [Batch: 0551/0570] Loss: 0.28087  Avg Loss: 0.23228  Avg mIoU:  63.31  

*** Training [@Epoch 23] Avg Loss: 0.23175  Avg mIoU:  63.28  ***

[Epoch: 23] [Batch: 0001/0050] Loss: 0.18371  Avg Loss: 0.18371  Avg mIoU:  60.07  

*** Validation [@Epoch 23] Avg Loss: 0.17605  Avg mIoU:  62.36  ***

Model saved @23 w/ val. mIoU: 62.36.

[Epoch: 24] [Batch: 0001/0570] Loss: 0.17298  Avg Loss: 0.17298  Avg mIoU:  52.53  
[Epoch: 24] [Batch: 0051/0570] Loss: 0.22603  Avg Loss: 0.22278  Avg mIoU:  65.73  
[Epoch: 24] [Batch: 0101/0570] Loss: 0.17629  Avg Loss: 0.22522  Avg mIoU:  64.17  
[Epoch: 24] [Batch: 0151/0570] Loss: 0.23421  Avg Loss: 0.22817  Avg mIoU:  64.18  
[Epoch: 24] [Batch: 0201/0570] Loss: 0.13815  Avg Loss: 0.22931  Avg mIoU:  63.79  
[Epoch: 24] [Batch: 0251/0570] Loss: 0.19702  Avg Loss: 0.22800  Avg mIoU:  63.65  
[Epoch: 24] [Batch: 0301/0570] Loss: 0.14195  Avg Loss: 0.22991  Avg mIoU:  63.63  
[Epoch: 24] [Batch: 0351/0570] Loss: 0.20577  Avg Loss: 0.23087  Avg mIoU:  63.61  
[Epoch: 24] [Batch: 0401/0570] Loss: 0.17110  Avg Loss: 0.22941  Avg mIoU:  63.67  
[Epoch: 24] [Batch: 0451/0570] Loss: 0.21764  Avg Loss: 0.22935  Avg mIoU:  63.77  
[Epoch: 24] [Batch: 0501/0570] Loss: 0.22238  Avg Loss: 0.22792  Avg mIoU:  63.93  
[Epoch: 24] [Batch: 0551/0570] Loss: 0.27552  Avg Loss: 0.22820  Avg mIoU:  63.87  

*** Training [@Epoch 24] Avg Loss: 0.22844  Avg mIoU:  63.85  ***

[Epoch: 24] [Batch: 0001/0050] Loss: 0.22922  Avg Loss: 0.22922  Avg mIoU:  58.42  

*** Validation [@Epoch 24] Avg Loss: 0.22841  Avg mIoU:  59.77  ***

[Epoch: 25] [Batch: 0001/0570] Loss: 0.17677  Avg Loss: 0.17677  Avg mIoU:  36.35  
[Epoch: 25] [Batch: 0051/0570] Loss: 0.17822  Avg Loss: 0.24515  Avg mIoU:  62.08  
[Epoch: 25] [Batch: 0101/0570] Loss: 0.21572  Avg Loss: 0.23012  Avg mIoU:  64.15  
[Epoch: 25] [Batch: 0151/0570] Loss: 0.26855  Avg Loss: 0.23169  Avg mIoU:  63.39  
[Epoch: 25] [Batch: 0201/0570] Loss: 0.20222  Avg Loss: 0.23118  Avg mIoU:  63.37  
[Epoch: 25] [Batch: 0251/0570] Loss: 0.33255  Avg Loss: 0.23012  Avg mIoU:  63.50  
[Epoch: 25] [Batch: 0301/0570] Loss: 0.27787  Avg Loss: 0.23018  Avg mIoU:  63.75  
[Epoch: 25] [Batch: 0351/0570] Loss: 0.17005  Avg Loss: 0.23070  Avg mIoU:  63.96  
[Epoch: 25] [Batch: 0401/0570] Loss: 0.26320  Avg Loss: 0.22953  Avg mIoU:  64.10  
[Epoch: 25] [Batch: 0451/0570] Loss: 0.19942  Avg Loss: 0.22994  Avg mIoU:  63.85  
[Epoch: 25] [Batch: 0501/0570] Loss: 0.20339  Avg Loss: 0.22905  Avg mIoU:  63.93  
[Epoch: 25] [Batch: 0551/0570] Loss: 0.29504  Avg Loss: 0.22902  Avg mIoU:  63.86  

*** Training [@Epoch 25] Avg Loss: 0.22897  Avg mIoU:  63.81  ***

[Epoch: 25] [Batch: 0001/0050] Loss: 0.17292  Avg Loss: 0.17292  Avg mIoU:  61.78  

*** Validation [@Epoch 25] Avg Loss: 0.16723  Avg mIoU:  62.99  ***

Model saved @25 w/ val. mIoU: 62.99.

[Epoch: 26] [Batch: 0001/0570] Loss: 0.23408  Avg Loss: 0.23408  Avg mIoU:  34.95  
[Epoch: 26] [Batch: 0051/0570] Loss: 0.21530  Avg Loss: 0.23192  Avg mIoU:  64.85  
[Epoch: 26] [Batch: 0101/0570] Loss: 0.19670  Avg Loss: 0.22932  Avg mIoU:  65.30  
[Epoch: 26] [Batch: 0151/0570] Loss: 0.17973  Avg Loss: 0.23149  Avg mIoU:  64.58  
[Epoch: 26] [Batch: 0201/0570] Loss: 0.20915  Avg Loss: 0.23160  Avg mIoU:  64.26  
[Epoch: 26] [Batch: 0251/0570] Loss: 0.24619  Avg Loss: 0.22891  Avg mIoU:  64.23  
[Epoch: 26] [Batch: 0301/0570] Loss: 0.17090  Avg Loss: 0.22802  Avg mIoU:  64.48  
[Epoch: 26] [Batch: 0351/0570] Loss: 0.21844  Avg Loss: 0.22635  Avg mIoU:  64.49  
[Epoch: 26] [Batch: 0401/0570] Loss: 0.21520  Avg Loss: 0.22632  Avg mIoU:  64.45  
[Epoch: 26] [Batch: 0451/0570] Loss: 0.18268  Avg Loss: 0.22700  Avg mIoU:  64.11  
[Epoch: 26] [Batch: 0501/0570] Loss: 0.26453  Avg Loss: 0.22726  Avg mIoU:  64.10  
[Epoch: 26] [Batch: 0551/0570] Loss: 0.26045  Avg Loss: 0.22806  Avg mIoU:  64.18  

*** Training [@Epoch 26] Avg Loss: 0.22831  Avg mIoU:  64.11  ***

[Epoch: 26] [Batch: 0001/0050] Loss: 0.17528  Avg Loss: 0.17528  Avg mIoU:  60.67  

*** Validation [@Epoch 26] Avg Loss: 0.17649  Avg mIoU:  62.67  ***

[Epoch: 27] [Batch: 0001/0570] Loss: 0.22295  Avg Loss: 0.22295  Avg mIoU:  30.71  
[Epoch: 27] [Batch: 0051/0570] Loss: 0.18716  Avg Loss: 0.21476  Avg mIoU:  63.39  
[Epoch: 27] [Batch: 0101/0570] Loss: 0.19496  Avg Loss: 0.21612  Avg mIoU:  64.83  
[Epoch: 27] [Batch: 0151/0570] Loss: 0.26847  Avg Loss: 0.22035  Avg mIoU:  64.59  
[Epoch: 27] [Batch: 0201/0570] Loss: 0.26782  Avg Loss: 0.22258  Avg mIoU:  64.52  
[Epoch: 27] [Batch: 0251/0570] Loss: 0.29023  Avg Loss: 0.22579  Avg mIoU:  64.39  
[Epoch: 27] [Batch: 0301/0570] Loss: 0.20258  Avg Loss: 0.22654  Avg mIoU:  64.51  
[Epoch: 27] [Batch: 0351/0570] Loss: 0.21044  Avg Loss: 0.22612  Avg mIoU:  64.50  
[Epoch: 27] [Batch: 0401/0570] Loss: 0.21476  Avg Loss: 0.22604  Avg mIoU:  64.26  
[Epoch: 27] [Batch: 0451/0570] Loss: 0.17109  Avg Loss: 0.22605  Avg mIoU:  64.35  
[Epoch: 27] [Batch: 0501/0570] Loss: 0.33040  Avg Loss: 0.22546  Avg mIoU:  64.32  
[Epoch: 27] [Batch: 0551/0570] Loss: 0.26249  Avg Loss: 0.22750  Avg mIoU:  64.00  

*** Training [@Epoch 27] Avg Loss: 0.22731  Avg mIoU:  63.95  ***

[Epoch: 27] [Batch: 0001/0050] Loss: 0.15457  Avg Loss: 0.15457  Avg mIoU:  64.46  

*** Validation [@Epoch 27] Avg Loss: 0.16595  Avg mIoU:  62.22  ***

[Epoch: 28] [Batch: 0001/0570] Loss: 0.19799  Avg Loss: 0.19799  Avg mIoU:  45.83  
[Epoch: 28] [Batch: 0051/0570] Loss: 0.18748  Avg Loss: 0.22237  Avg mIoU:  64.86  
[Epoch: 28] [Batch: 0101/0570] Loss: 0.15197  Avg Loss: 0.22491  Avg mIoU:  64.97  
[Epoch: 28] [Batch: 0151/0570] Loss: 0.28020  Avg Loss: 0.22392  Avg mIoU:  64.58  
[Epoch: 28] [Batch: 0201/0570] Loss: 0.15249  Avg Loss: 0.22961  Avg mIoU:  64.08  
[Epoch: 28] [Batch: 0251/0570] Loss: 0.28655  Avg Loss: 0.22776  Avg mIoU:  64.20  
[Epoch: 28] [Batch: 0301/0570] Loss: 0.23584  Avg Loss: 0.22777  Avg mIoU:  64.33  
[Epoch: 28] [Batch: 0351/0570] Loss: 0.20617  Avg Loss: 0.22681  Avg mIoU:  64.50  
[Epoch: 28] [Batch: 0401/0570] Loss: 0.25123  Avg Loss: 0.22709  Avg mIoU:  64.24  
[Epoch: 28] [Batch: 0451/0570] Loss: 0.21250  Avg Loss: 0.22608  Avg mIoU:  64.23  
[Epoch: 28] [Batch: 0501/0570] Loss: 0.22051  Avg Loss: 0.22566  Avg mIoU:  64.22  
[Epoch: 28] [Batch: 0551/0570] Loss: 0.15852  Avg Loss: 0.22592  Avg mIoU:  64.14  

*** Training [@Epoch 28] Avg Loss: 0.22637  Avg mIoU:  64.11  ***

[Epoch: 28] [Batch: 0001/0050] Loss: 0.18765  Avg Loss: 0.18765  Avg mIoU:  59.01  

*** Validation [@Epoch 28] Avg Loss: 0.17977  Avg mIoU:  61.22  ***

[Epoch: 29] [Batch: 0001/0570] Loss: 0.23827  Avg Loss: 0.23827  Avg mIoU:  46.90  
[Epoch: 29] [Batch: 0051/0570] Loss: 0.14340  Avg Loss: 0.21693  Avg mIoU:  64.65  
[Epoch: 29] [Batch: 0101/0570] Loss: 0.20696  Avg Loss: 0.22464  Avg mIoU:  63.53  
[Epoch: 29] [Batch: 0151/0570] Loss: 0.24351  Avg Loss: 0.22673  Avg mIoU:  63.47  
[Epoch: 29] [Batch: 0201/0570] Loss: 0.13399  Avg Loss: 0.22349  Avg mIoU:  64.23  
[Epoch: 29] [Batch: 0251/0570] Loss: 0.21960  Avg Loss: 0.22366  Avg mIoU:  64.39  
[Epoch: 29] [Batch: 0301/0570] Loss: 0.27120  Avg Loss: 0.22496  Avg mIoU:  64.45  
[Epoch: 29] [Batch: 0351/0570] Loss: 0.19822  Avg Loss: 0.22405  Avg mIoU:  64.64  
[Epoch: 29] [Batch: 0401/0570] Loss: 0.19633  Avg Loss: 0.22468  Avg mIoU:  64.42  
[Epoch: 29] [Batch: 0451/0570] Loss: 0.21298  Avg Loss: 0.22468  Avg mIoU:  64.32  
[Epoch: 29] [Batch: 0501/0570] Loss: 0.30315  Avg Loss: 0.22533  Avg mIoU:  64.28  
[Epoch: 29] [Batch: 0551/0570] Loss: 0.30103  Avg Loss: 0.22517  Avg mIoU:  64.05  

*** Training [@Epoch 29] Avg Loss: 0.22507  Avg mIoU:  64.16  ***

[Epoch: 29] [Batch: 0001/0050] Loss: 0.18157  Avg Loss: 0.18157  Avg mIoU:  60.19  

*** Validation [@Epoch 29] Avg Loss: 0.17817  Avg mIoU:  61.59  ***

[Epoch: 30] [Batch: 0001/0570] Loss: 0.20063  Avg Loss: 0.20063  Avg mIoU:  41.86  
[Epoch: 30] [Batch: 0051/0570] Loss: 0.21986  Avg Loss: 0.22453  Avg mIoU:  63.35  
[Epoch: 30] [Batch: 0101/0570] Loss: 0.22527  Avg Loss: 0.21690  Avg mIoU:  64.86  
[Epoch: 30] [Batch: 0151/0570] Loss: 0.23952  Avg Loss: 0.22200  Avg mIoU:  64.25  
[Epoch: 30] [Batch: 0201/0570] Loss: 0.25358  Avg Loss: 0.22404  Avg mIoU:  64.19  
[Epoch: 30] [Batch: 0251/0570] Loss: 0.22191  Avg Loss: 0.22317  Avg mIoU:  64.41  
[Epoch: 30] [Batch: 0301/0570] Loss: 0.25925  Avg Loss: 0.22091  Avg mIoU:  64.79  
[Epoch: 30] [Batch: 0351/0570] Loss: 0.20884  Avg Loss: 0.22217  Avg mIoU:  64.85  
[Epoch: 30] [Batch: 0401/0570] Loss: 0.21133  Avg Loss: 0.22261  Avg mIoU:  64.71  
[Epoch: 30] [Batch: 0451/0570] Loss: 0.13498  Avg Loss: 0.22197  Avg mIoU:  64.81  
[Epoch: 30] [Batch: 0501/0570] Loss: 0.24594  Avg Loss: 0.22243  Avg mIoU:  64.82  
[Epoch: 30] [Batch: 0551/0570] Loss: 0.16382  Avg Loss: 0.22250  Avg mIoU:  64.90  

*** Training [@Epoch 30] Avg Loss: 0.22285  Avg mIoU:  64.87  ***

[Epoch: 30] [Batch: 0001/0050] Loss: 0.20796  Avg Loss: 0.20796  Avg mIoU:  59.64  

*** Validation [@Epoch 30] Avg Loss: 0.18163  Avg mIoU:  62.21  ***

[Epoch: 31] [Batch: 0001/0570] Loss: 0.30550  Avg Loss: 0.30550  Avg mIoU:  22.09  
[Epoch: 31] [Batch: 0051/0570] Loss: 0.20572  Avg Loss: 0.21881  Avg mIoU:  64.20  
[Epoch: 31] [Batch: 0101/0570] Loss: 0.15903  Avg Loss: 0.22278  Avg mIoU:  64.82  
[Epoch: 31] [Batch: 0151/0570] Loss: 0.27485  Avg Loss: 0.22704  Avg mIoU:  64.17  
[Epoch: 31] [Batch: 0201/0570] Loss: 0.27606  Avg Loss: 0.22254  Avg mIoU:  64.43  
[Epoch: 31] [Batch: 0251/0570] Loss: 0.23659  Avg Loss: 0.22188  Avg mIoU:  63.97  
[Epoch: 31] [Batch: 0301/0570] Loss: 0.23412  Avg Loss: 0.22144  Avg mIoU:  64.44  
[Epoch: 31] [Batch: 0351/0570] Loss: 0.21978  Avg Loss: 0.22185  Avg mIoU:  64.66  
[Epoch: 31] [Batch: 0401/0570] Loss: 0.15588  Avg Loss: 0.22228  Avg mIoU:  64.62  
[Epoch: 31] [Batch: 0451/0570] Loss: 0.25432  Avg Loss: 0.22295  Avg mIoU:  64.57  
[Epoch: 31] [Batch: 0501/0570] Loss: 0.18111  Avg Loss: 0.22159  Avg mIoU:  64.70  
[Epoch: 31] [Batch: 0551/0570] Loss: 0.19599  Avg Loss: 0.22156  Avg mIoU:  64.69  

*** Training [@Epoch 31] Avg Loss: 0.22179  Avg mIoU:  64.75  ***

[Epoch: 31] [Batch: 0001/0050] Loss: 0.15939  Avg Loss: 0.15939  Avg mIoU:  61.83  

*** Validation [@Epoch 31] Avg Loss: 0.16683  Avg mIoU:  62.94  ***

[Epoch: 32] [Batch: 0001/0570] Loss: 0.23002  Avg Loss: 0.23002  Avg mIoU:  32.19  
[Epoch: 32] [Batch: 0051/0570] Loss: 0.20704  Avg Loss: 0.20051  Avg mIoU:  64.98  
[Epoch: 32] [Batch: 0101/0570] Loss: 0.18902  Avg Loss: 0.21158  Avg mIoU:  64.83  
[Epoch: 32] [Batch: 0151/0570] Loss: 0.22248  Avg Loss: 0.21765  Avg mIoU:  64.24  
[Epoch: 32] [Batch: 0201/0570] Loss: 0.17787  Avg Loss: 0.21806  Avg mIoU:  64.23  
[Epoch: 32] [Batch: 0251/0570] Loss: 0.26870  Avg Loss: 0.22037  Avg mIoU:  64.25  
[Epoch: 32] [Batch: 0301/0570] Loss: 0.20683  Avg Loss: 0.22051  Avg mIoU:  64.48  
[Epoch: 32] [Batch: 0351/0570] Loss: 0.20228  Avg Loss: 0.22012  Avg mIoU:  64.54  
[Epoch: 32] [Batch: 0401/0570] Loss: 0.23939  Avg Loss: 0.21919  Avg mIoU:  64.59  
[Epoch: 32] [Batch: 0451/0570] Loss: 0.19125  Avg Loss: 0.21837  Avg mIoU:  64.57  
[Epoch: 32] [Batch: 0501/0570] Loss: 0.18377  Avg Loss: 0.21967  Avg mIoU:  64.60  
[Epoch: 32] [Batch: 0551/0570] Loss: 0.28711  Avg Loss: 0.21970  Avg mIoU:  64.72  

*** Training [@Epoch 32] Avg Loss: 0.21960  Avg mIoU:  64.81  ***

[Epoch: 32] [Batch: 0001/0050] Loss: 0.14572  Avg Loss: 0.14572  Avg mIoU:  62.47  

*** Validation [@Epoch 32] Avg Loss: 0.15643  Avg mIoU:  61.73  ***

[Epoch: 33] [Batch: 0001/0570] Loss: 0.31610  Avg Loss: 0.31610  Avg mIoU:  39.62  
[Epoch: 33] [Batch: 0051/0570] Loss: 0.18488  Avg Loss: 0.22574  Avg mIoU:  66.73  
[Epoch: 33] [Batch: 0101/0570] Loss: 0.22465  Avg Loss: 0.22447  Avg mIoU:  65.96  
[Epoch: 33] [Batch: 0151/0570] Loss: 0.24310  Avg Loss: 0.22543  Avg mIoU:  65.78  
[Epoch: 33] [Batch: 0201/0570] Loss: 0.21387  Avg Loss: 0.22327  Avg mIoU:  65.74  
[Epoch: 33] [Batch: 0251/0570] Loss: 0.36939  Avg Loss: 0.22124  Avg mIoU:  65.39  
[Epoch: 33] [Batch: 0301/0570] Loss: 0.16125  Avg Loss: 0.22081  Avg mIoU:  65.17  
[Epoch: 33] [Batch: 0351/0570] Loss: 0.21244  Avg Loss: 0.21951  Avg mIoU:  65.18  
[Epoch: 33] [Batch: 0401/0570] Loss: 0.17327  Avg Loss: 0.21897  Avg mIoU:  65.23  
[Epoch: 33] [Batch: 0451/0570] Loss: 0.24191  Avg Loss: 0.21960  Avg mIoU:  65.21  
[Epoch: 33] [Batch: 0501/0570] Loss: 0.24307  Avg Loss: 0.21911  Avg mIoU:  65.28  
[Epoch: 33] [Batch: 0551/0570] Loss: 0.32714  Avg Loss: 0.21841  Avg mIoU:  65.46  

*** Training [@Epoch 33] Avg Loss: 0.21784  Avg mIoU:  65.43  ***

[Epoch: 33] [Batch: 0001/0050] Loss: 0.16153  Avg Loss: 0.16153  Avg mIoU:  64.00  

*** Validation [@Epoch 33] Avg Loss: 0.17319  Avg mIoU:  63.76  ***

Model saved @33 w/ val. mIoU: 63.76.

[Epoch: 34] [Batch: 0001/0570] Loss: 0.21582  Avg Loss: 0.21582  Avg mIoU:  32.93  
[Epoch: 34] [Batch: 0051/0570] Loss: 0.23129  Avg Loss: 0.22223  Avg mIoU:  64.61  
[Epoch: 34] [Batch: 0101/0570] Loss: 0.28100  Avg Loss: 0.21936  Avg mIoU:  64.83  
[Epoch: 34] [Batch: 0151/0570] Loss: 0.18475  Avg Loss: 0.22179  Avg mIoU:  64.60  
[Epoch: 34] [Batch: 0201/0570] Loss: 0.21998  Avg Loss: 0.22212  Avg mIoU:  64.69  
[Epoch: 34] [Batch: 0251/0570] Loss: 0.12504  Avg Loss: 0.21988  Avg mIoU:  64.55  
[Epoch: 34] [Batch: 0301/0570] Loss: 0.23235  Avg Loss: 0.21813  Avg mIoU:  64.75  
[Epoch: 34] [Batch: 0351/0570] Loss: 0.32056  Avg Loss: 0.21952  Avg mIoU:  64.60  
[Epoch: 34] [Batch: 0401/0570] Loss: 0.16845  Avg Loss: 0.21991  Avg mIoU:  64.54  
[Epoch: 34] [Batch: 0451/0570] Loss: 0.16139  Avg Loss: 0.21854  Avg mIoU:  64.69  
[Epoch: 34] [Batch: 0501/0570] Loss: 0.24983  Avg Loss: 0.21828  Avg mIoU:  64.75  
[Epoch: 34] [Batch: 0551/0570] Loss: 0.20198  Avg Loss: 0.21894  Avg mIoU:  64.67  

*** Training [@Epoch 34] Avg Loss: 0.21856  Avg mIoU:  64.75  ***

[Epoch: 34] [Batch: 0001/0050] Loss: 0.14903  Avg Loss: 0.14903  Avg mIoU:  63.31  

*** Validation [@Epoch 34] Avg Loss: 0.15400  Avg mIoU:  62.96  ***

[Epoch: 35] [Batch: 0001/0570] Loss: 0.16870  Avg Loss: 0.16870  Avg mIoU:  47.74  
[Epoch: 35] [Batch: 0051/0570] Loss: 0.17215  Avg Loss: 0.21056  Avg mIoU:  67.33  
[Epoch: 35] [Batch: 0101/0570] Loss: 0.17721  Avg Loss: 0.21687  Avg mIoU:  66.25  
[Epoch: 35] [Batch: 0151/0570] Loss: 0.24437  Avg Loss: 0.22006  Avg mIoU:  66.02  
[Epoch: 35] [Batch: 0201/0570] Loss: 0.27704  Avg Loss: 0.21908  Avg mIoU:  65.57  
[Epoch: 35] [Batch: 0251/0570] Loss: 0.21291  Avg Loss: 0.21792  Avg mIoU:  65.55  
[Epoch: 35] [Batch: 0301/0570] Loss: 0.16267  Avg Loss: 0.21609  Avg mIoU:  65.41  
[Epoch: 35] [Batch: 0351/0570] Loss: 0.23248  Avg Loss: 0.21756  Avg mIoU:  65.37  
[Epoch: 35] [Batch: 0401/0570] Loss: 0.15030  Avg Loss: 0.21588  Avg mIoU:  65.51  
[Epoch: 35] [Batch: 0451/0570] Loss: 0.19492  Avg Loss: 0.21666  Avg mIoU:  65.49  
[Epoch: 35] [Batch: 0501/0570] Loss: 0.25164  Avg Loss: 0.21568  Avg mIoU:  65.78  
[Epoch: 35] [Batch: 0551/0570] Loss: 0.18546  Avg Loss: 0.21640  Avg mIoU:  65.73  

*** Training [@Epoch 35] Avg Loss: 0.21615  Avg mIoU:  65.78  ***

[Epoch: 35] [Batch: 0001/0050] Loss: 0.15190  Avg Loss: 0.15190  Avg mIoU:  62.72  

*** Validation [@Epoch 35] Avg Loss: 0.15766  Avg mIoU:  61.58  ***

[Epoch: 36] [Batch: 0001/0570] Loss: 0.14718  Avg Loss: 0.14718  Avg mIoU:  54.78  
[Epoch: 36] [Batch: 0051/0570] Loss: 0.23676  Avg Loss: 0.21529  Avg mIoU:  65.74  
[Epoch: 36] [Batch: 0101/0570] Loss: 0.17494  Avg Loss: 0.21768  Avg mIoU:  64.80  
[Epoch: 36] [Batch: 0151/0570] Loss: 0.16095  Avg Loss: 0.21735  Avg mIoU:  65.03  
[Epoch: 36] [Batch: 0201/0570] Loss: 0.17878  Avg Loss: 0.21647  Avg mIoU:  65.10  
[Epoch: 36] [Batch: 0251/0570] Loss: 0.20460  Avg Loss: 0.21659  Avg mIoU:  65.40  
[Epoch: 36] [Batch: 0301/0570] Loss: 0.25536  Avg Loss: 0.21765  Avg mIoU:  65.40  
[Epoch: 36] [Batch: 0351/0570] Loss: 0.30613  Avg Loss: 0.21666  Avg mIoU:  65.52  
[Epoch: 36] [Batch: 0401/0570] Loss: 0.13622  Avg Loss: 0.21674  Avg mIoU:  65.24  
[Epoch: 36] [Batch: 0451/0570] Loss: 0.24689  Avg Loss: 0.21574  Avg mIoU:  65.41  
[Epoch: 36] [Batch: 0501/0570] Loss: 0.22534  Avg Loss: 0.21537  Avg mIoU:  65.35  
[Epoch: 36] [Batch: 0551/0570] Loss: 0.18575  Avg Loss: 0.21569  Avg mIoU:  65.52  

*** Training [@Epoch 36] Avg Loss: 0.21550  Avg mIoU:  65.50  ***

[Epoch: 36] [Batch: 0001/0050] Loss: 0.15812  Avg Loss: 0.15812  Avg mIoU:  64.12  

*** Validation [@Epoch 36] Avg Loss: 0.16337  Avg mIoU:  63.11  ***

[Epoch: 37] [Batch: 0001/0570] Loss: 0.21370  Avg Loss: 0.21370  Avg mIoU:  31.97  
[Epoch: 37] [Batch: 0051/0570] Loss: 0.19462  Avg Loss: 0.22507  Avg mIoU:  64.43  
[Epoch: 37] [Batch: 0101/0570] Loss: 0.16709  Avg Loss: 0.21702  Avg mIoU:  65.82  
[Epoch: 37] [Batch: 0151/0570] Loss: 0.18655  Avg Loss: 0.21539  Avg mIoU:  66.38  
[Epoch: 37] [Batch: 0201/0570] Loss: 0.22776  Avg Loss: 0.21839  Avg mIoU:  66.21  
[Epoch: 37] [Batch: 0251/0570] Loss: 0.17332  Avg Loss: 0.21761  Avg mIoU:  66.12  
[Epoch: 37] [Batch: 0301/0570] Loss: 0.19530  Avg Loss: 0.21851  Avg mIoU:  65.97  
[Epoch: 37] [Batch: 0351/0570] Loss: 0.25292  Avg Loss: 0.21848  Avg mIoU:  65.73  
[Epoch: 37] [Batch: 0401/0570] Loss: 0.21775  Avg Loss: 0.21861  Avg mIoU:  65.85  
[Epoch: 37] [Batch: 0451/0570] Loss: 0.19808  Avg Loss: 0.21746  Avg mIoU:  65.73  
[Epoch: 37] [Batch: 0501/0570] Loss: 0.27219  Avg Loss: 0.21674  Avg mIoU:  65.71  
[Epoch: 37] [Batch: 0551/0570] Loss: 0.20131  Avg Loss: 0.21617  Avg mIoU:  65.59  

*** Training [@Epoch 37] Avg Loss: 0.21629  Avg mIoU:  65.63  ***

[Epoch: 37] [Batch: 0001/0050] Loss: 0.20603  Avg Loss: 0.20603  Avg mIoU:  61.65  

*** Validation [@Epoch 37] Avg Loss: 0.19754  Avg mIoU:  61.60  ***

[Epoch: 38] [Batch: 0001/0570] Loss: 0.27588  Avg Loss: 0.27588  Avg mIoU:  19.21  
[Epoch: 38] [Batch: 0051/0570] Loss: 0.18824  Avg Loss: 0.21671  Avg mIoU:  65.89  
[Epoch: 38] [Batch: 0101/0570] Loss: 0.14276  Avg Loss: 0.21120  Avg mIoU:  65.01  
[Epoch: 38] [Batch: 0151/0570] Loss: 0.22675  Avg Loss: 0.21203  Avg mIoU:  64.62  
[Epoch: 38] [Batch: 0201/0570] Loss: 0.14815  Avg Loss: 0.21330  Avg mIoU:  64.45  
[Epoch: 38] [Batch: 0251/0570] Loss: 0.25512  Avg Loss: 0.21298  Avg mIoU:  64.69  
[Epoch: 38] [Batch: 0301/0570] Loss: 0.27600  Avg Loss: 0.21608  Avg mIoU:  64.74  
[Epoch: 38] [Batch: 0351/0570] Loss: 0.19737  Avg Loss: 0.21583  Avg mIoU:  65.02  
[Epoch: 38] [Batch: 0401/0570] Loss: 0.19202  Avg Loss: 0.21590  Avg mIoU:  64.99  
[Epoch: 38] [Batch: 0451/0570] Loss: 0.21486  Avg Loss: 0.21504  Avg mIoU:  65.06  
[Epoch: 38] [Batch: 0501/0570] Loss: 0.24120  Avg Loss: 0.21447  Avg mIoU:  65.49  
[Epoch: 38] [Batch: 0551/0570] Loss: 0.22765  Avg Loss: 0.21450  Avg mIoU:  65.49  

*** Training [@Epoch 38] Avg Loss: 0.21405  Avg mIoU:  65.53  ***

[Epoch: 38] [Batch: 0001/0050] Loss: 0.16406  Avg Loss: 0.16406  Avg mIoU:  61.27  

*** Validation [@Epoch 38] Avg Loss: 0.16468  Avg mIoU:  63.62  ***

[Epoch: 39] [Batch: 0001/0570] Loss: 0.28226  Avg Loss: 0.28226  Avg mIoU:  35.86  
[Epoch: 39] [Batch: 0051/0570] Loss: 0.26666  Avg Loss: 0.20928  Avg mIoU:  67.32  
[Epoch: 39] [Batch: 0101/0570] Loss: 0.19899  Avg Loss: 0.20639  Avg mIoU:  66.45  
[Epoch: 39] [Batch: 0151/0570] Loss: 0.15014  Avg Loss: 0.20969  Avg mIoU:  66.34  
[Epoch: 39] [Batch: 0201/0570] Loss: 0.18194  Avg Loss: 0.20944  Avg mIoU:  66.58  
[Epoch: 39] [Batch: 0251/0570] Loss: 0.19824  Avg Loss: 0.20867  Avg mIoU:  66.25  
[Epoch: 39] [Batch: 0301/0570] Loss: 0.34051  Avg Loss: 0.21084  Avg mIoU:  66.07  
[Epoch: 39] [Batch: 0351/0570] Loss: 0.22718  Avg Loss: 0.21130  Avg mIoU:  66.04  
[Epoch: 39] [Batch: 0401/0570] Loss: 0.20696  Avg Loss: 0.21209  Avg mIoU:  65.85  
[Epoch: 39] [Batch: 0451/0570] Loss: 0.29881  Avg Loss: 0.21236  Avg mIoU:  65.95  
[Epoch: 39] [Batch: 0501/0570] Loss: 0.23763  Avg Loss: 0.21224  Avg mIoU:  65.90  
[Epoch: 39] [Batch: 0551/0570] Loss: 0.25394  Avg Loss: 0.21274  Avg mIoU:  65.95  

*** Training [@Epoch 39] Avg Loss: 0.21288  Avg mIoU:  65.99  ***

[Epoch: 39] [Batch: 0001/0050] Loss: 0.15871  Avg Loss: 0.15871  Avg mIoU:  63.04  

*** Validation [@Epoch 39] Avg Loss: 0.17268  Avg mIoU:  63.36  ***

[Epoch: 40] [Batch: 0001/0570] Loss: 0.16090  Avg Loss: 0.16090  Avg mIoU:  46.00  
[Epoch: 40] [Batch: 0051/0570] Loss: 0.27022  Avg Loss: 0.20139  Avg mIoU:  66.60  
[Epoch: 40] [Batch: 0101/0570] Loss: 0.25654  Avg Loss: 0.20483  Avg mIoU:  67.39  
[Epoch: 40] [Batch: 0151/0570] Loss: 0.16013  Avg Loss: 0.20576  Avg mIoU:  67.01  
[Epoch: 40] [Batch: 0201/0570] Loss: 0.28151  Avg Loss: 0.20772  Avg mIoU:  66.65  
[Epoch: 40] [Batch: 0251/0570] Loss: 0.22120  Avg Loss: 0.21003  Avg mIoU:  66.92  
[Epoch: 40] [Batch: 0301/0570] Loss: 0.21040  Avg Loss: 0.21085  Avg mIoU:  66.68  
[Epoch: 40] [Batch: 0351/0570] Loss: 0.25952  Avg Loss: 0.21279  Avg mIoU:  66.47  
[Epoch: 40] [Batch: 0401/0570] Loss: 0.24007  Avg Loss: 0.21219  Avg mIoU:  66.32  
[Epoch: 40] [Batch: 0451/0570] Loss: 0.12314  Avg Loss: 0.21235  Avg mIoU:  66.21  
[Epoch: 40] [Batch: 0501/0570] Loss: 0.19939  Avg Loss: 0.21207  Avg mIoU:  66.41  
[Epoch: 40] [Batch: 0551/0570] Loss: 0.16393  Avg Loss: 0.21115  Avg mIoU:  66.22  

*** Training [@Epoch 40] Avg Loss: 0.21109  Avg mIoU:  66.07  ***

[Epoch: 40] [Batch: 0001/0050] Loss: 0.14991  Avg Loss: 0.14991  Avg mIoU:  61.60  

*** Validation [@Epoch 40] Avg Loss: 0.15712  Avg mIoU:  61.95  ***

[Epoch: 41] [Batch: 0001/0570] Loss: 0.24242  Avg Loss: 0.24242  Avg mIoU:  33.32  
[Epoch: 41] [Batch: 0051/0570] Loss: 0.25084  Avg Loss: 0.20793  Avg mIoU:  67.05  
[Epoch: 41] [Batch: 0101/0570] Loss: 0.20743  Avg Loss: 0.20763  Avg mIoU:  67.54  
[Epoch: 41] [Batch: 0151/0570] Loss: 0.26926  Avg Loss: 0.21137  Avg mIoU:  67.01  
[Epoch: 41] [Batch: 0201/0570] Loss: 0.23887  Avg Loss: 0.21181  Avg mIoU:  66.39  
[Epoch: 41] [Batch: 0251/0570] Loss: 0.20732  Avg Loss: 0.20954  Avg mIoU:  66.44  
[Epoch: 41] [Batch: 0301/0570] Loss: 0.24068  Avg Loss: 0.21146  Avg mIoU:  66.22  
[Epoch: 41] [Batch: 0351/0570] Loss: 0.21871  Avg Loss: 0.21154  Avg mIoU:  66.18  
[Epoch: 41] [Batch: 0401/0570] Loss: 0.26166  Avg Loss: 0.21188  Avg mIoU:  66.11  
[Epoch: 41] [Batch: 0451/0570] Loss: 0.25167  Avg Loss: 0.21312  Avg mIoU:  65.97  
[Epoch: 41] [Batch: 0501/0570] Loss: 0.26535  Avg Loss: 0.21462  Avg mIoU:  65.61  
[Epoch: 41] [Batch: 0551/0570] Loss: 0.11192  Avg Loss: 0.21369  Avg mIoU:  65.69  

*** Training [@Epoch 41] Avg Loss: 0.21312  Avg mIoU:  65.80  ***

[Epoch: 41] [Batch: 0001/0050] Loss: 0.17265  Avg Loss: 0.17265  Avg mIoU:  60.41  

*** Validation [@Epoch 41] Avg Loss: 0.17102  Avg mIoU:  62.75  ***

[Epoch: 42] [Batch: 0001/0570] Loss: 0.21205  Avg Loss: 0.21205  Avg mIoU:  35.67  
[Epoch: 42] [Batch: 0051/0570] Loss: 0.18151  Avg Loss: 0.21454  Avg mIoU:  66.66  
[Epoch: 42] [Batch: 0101/0570] Loss: 0.18077  Avg Loss: 0.20703  Avg mIoU:  67.10  
[Epoch: 42] [Batch: 0151/0570] Loss: 0.22138  Avg Loss: 0.20840  Avg mIoU:  66.82  
[Epoch: 42] [Batch: 0201/0570] Loss: 0.19280  Avg Loss: 0.20920  Avg mIoU:  66.45  
[Epoch: 42] [Batch: 0251/0570] Loss: 0.19879  Avg Loss: 0.20792  Avg mIoU:  66.41  
[Epoch: 42] [Batch: 0301/0570] Loss: 0.22332  Avg Loss: 0.20953  Avg mIoU:  66.33  
[Epoch: 42] [Batch: 0351/0570] Loss: 0.29415  Avg Loss: 0.21029  Avg mIoU:  66.23  
[Epoch: 42] [Batch: 0401/0570] Loss: 0.20415  Avg Loss: 0.20993  Avg mIoU:  66.17  
[Epoch: 42] [Batch: 0451/0570] Loss: 0.27730  Avg Loss: 0.21040  Avg mIoU:  66.15  
[Epoch: 42] [Batch: 0501/0570] Loss: 0.17206  Avg Loss: 0.21036  Avg mIoU:  66.30  
[Epoch: 42] [Batch: 0551/0570] Loss: 0.19599  Avg Loss: 0.21128  Avg mIoU:  66.16  

*** Training [@Epoch 42] Avg Loss: 0.21110  Avg mIoU:  66.15  ***

[Epoch: 42] [Batch: 0001/0050] Loss: 0.16679  Avg Loss: 0.16679  Avg mIoU:  61.01  

*** Validation [@Epoch 42] Avg Loss: 0.16243  Avg mIoU:  63.39  ***

[Epoch: 43] [Batch: 0001/0570] Loss: 0.24116  Avg Loss: 0.24116  Avg mIoU:  31.49  
[Epoch: 43] [Batch: 0051/0570] Loss: 0.10818  Avg Loss: 0.20888  Avg mIoU:  66.07  
[Epoch: 43] [Batch: 0101/0570] Loss: 0.23639  Avg Loss: 0.20864  Avg mIoU:  66.31  
[Epoch: 43] [Batch: 0151/0570] Loss: 0.23910  Avg Loss: 0.20688  Avg mIoU:  67.19  
[Epoch: 43] [Batch: 0201/0570] Loss: 0.19162  Avg Loss: 0.20950  Avg mIoU:  66.95  
[Epoch: 43] [Batch: 0251/0570] Loss: 0.24498  Avg Loss: 0.21208  Avg mIoU:  66.50  
[Epoch: 43] [Batch: 0301/0570] Loss: 0.26937  Avg Loss: 0.21154  Avg mIoU:  66.86  
[Epoch: 43] [Batch: 0351/0570] Loss: 0.25980  Avg Loss: 0.21197  Avg mIoU:  66.61  
[Epoch: 43] [Batch: 0401/0570] Loss: 0.18060  Avg Loss: 0.21286  Avg mIoU:  66.52  
[Epoch: 43] [Batch: 0451/0570] Loss: 0.15238  Avg Loss: 0.21268  Avg mIoU:  66.46  
[Epoch: 43] [Batch: 0501/0570] Loss: 0.17764  Avg Loss: 0.21330  Avg mIoU:  66.19  
[Epoch: 43] [Batch: 0551/0570] Loss: 0.13232  Avg Loss: 0.21307  Avg mIoU:  66.11  

*** Training [@Epoch 43] Avg Loss: 0.21320  Avg mIoU:  66.03  ***

[Epoch: 43] [Batch: 0001/0050] Loss: 0.18733  Avg Loss: 0.18733  Avg mIoU:  62.80  

*** Validation [@Epoch 43] Avg Loss: 0.19292  Avg mIoU:  62.93  ***

[Epoch: 44] [Batch: 0001/0570] Loss: 0.17362  Avg Loss: 0.17362  Avg mIoU:  45.01  
[Epoch: 44] [Batch: 0051/0570] Loss: 0.19618  Avg Loss: 0.19990  Avg mIoU:  68.02  
[Epoch: 44] [Batch: 0101/0570] Loss: 0.20426  Avg Loss: 0.20287  Avg mIoU:  67.32  
[Epoch: 44] [Batch: 0151/0570] Loss: 0.19344  Avg Loss: 0.20419  Avg mIoU:  67.69  
[Epoch: 44] [Batch: 0201/0570] Loss: 0.17579  Avg Loss: 0.20457  Avg mIoU:  67.51  
[Epoch: 44] [Batch: 0251/0570] Loss: 0.18924  Avg Loss: 0.20651  Avg mIoU:  67.64  
[Epoch: 44] [Batch: 0301/0570] Loss: 0.19487  Avg Loss: 0.20636  Avg mIoU:  67.51  
[Epoch: 44] [Batch: 0351/0570] Loss: 0.23755  Avg Loss: 0.20732  Avg mIoU:  67.57  
[Epoch: 44] [Batch: 0401/0570] Loss: 0.20591  Avg Loss: 0.20762  Avg mIoU:  67.20  
[Epoch: 44] [Batch: 0451/0570] Loss: 0.27351  Avg Loss: 0.20842  Avg mIoU:  67.26  
[Epoch: 44] [Batch: 0501/0570] Loss: 0.24151  Avg Loss: 0.20737  Avg mIoU:  67.31  
[Epoch: 44] [Batch: 0551/0570] Loss: 0.19921  Avg Loss: 0.20756  Avg mIoU:  67.31  

*** Training [@Epoch 44] Avg Loss: 0.20760  Avg mIoU:  67.21  ***

[Epoch: 44] [Batch: 0001/0050] Loss: 0.15158  Avg Loss: 0.15158  Avg mIoU:  63.93  

*** Validation [@Epoch 44] Avg Loss: 0.15613  Avg mIoU:  63.85  ***

Model saved @44 w/ val. mIoU: 63.85.

[Epoch: 45] [Batch: 0001/0570] Loss: 0.19624  Avg Loss: 0.19624  Avg mIoU:  55.93  
[Epoch: 45] [Batch: 0051/0570] Loss: 0.29032  Avg Loss: 0.21089  Avg mIoU:  65.57  
[Epoch: 45] [Batch: 0101/0570] Loss: 0.23233  Avg Loss: 0.21011  Avg mIoU:  66.65  
[Epoch: 45] [Batch: 0151/0570] Loss: 0.27401  Avg Loss: 0.20689  Avg mIoU:  66.52  
[Epoch: 45] [Batch: 0201/0570] Loss: 0.22793  Avg Loss: 0.20830  Avg mIoU:  65.93  
[Epoch: 45] [Batch: 0251/0570] Loss: 0.28782  Avg Loss: 0.20831  Avg mIoU:  66.19  
[Epoch: 45] [Batch: 0301/0570] Loss: 0.20509  Avg Loss: 0.20792  Avg mIoU:  66.36  
[Epoch: 45] [Batch: 0351/0570] Loss: 0.21992  Avg Loss: 0.20761  Avg mIoU:  66.16  
[Epoch: 45] [Batch: 0401/0570] Loss: 0.20283  Avg Loss: 0.20707  Avg mIoU:  66.63  
[Epoch: 45] [Batch: 0451/0570] Loss: 0.19491  Avg Loss: 0.20656  Avg mIoU:  66.62  
[Epoch: 45] [Batch: 0501/0570] Loss: 0.28674  Avg Loss: 0.20812  Avg mIoU:  66.41  
[Epoch: 45] [Batch: 0551/0570] Loss: 0.14137  Avg Loss: 0.20760  Avg mIoU:  66.59  

*** Training [@Epoch 45] Avg Loss: 0.20807  Avg mIoU:  66.51  ***

[Epoch: 45] [Batch: 0001/0050] Loss: 0.18833  Avg Loss: 0.18833  Avg mIoU:  63.58  

*** Validation [@Epoch 45] Avg Loss: 0.18377  Avg mIoU:  64.35  ***

Model saved @45 w/ val. mIoU: 64.35.

[Epoch: 46] [Batch: 0001/0570] Loss: 0.15803  Avg Loss: 0.15803  Avg mIoU:  42.63  
[Epoch: 46] [Batch: 0051/0570] Loss: 0.16688  Avg Loss: 0.20306  Avg mIoU:  70.01  
[Epoch: 46] [Batch: 0101/0570] Loss: 0.14851  Avg Loss: 0.20615  Avg mIoU:  68.02  
[Epoch: 46] [Batch: 0151/0570] Loss: 0.21988  Avg Loss: 0.20750  Avg mIoU:  67.97  
[Epoch: 46] [Batch: 0201/0570] Loss: 0.20326  Avg Loss: 0.20772  Avg mIoU:  67.44  
[Epoch: 46] [Batch: 0251/0570] Loss: 0.18561  Avg Loss: 0.21067  Avg mIoU:  67.31  
[Epoch: 46] [Batch: 0301/0570] Loss: 0.19531  Avg Loss: 0.20883  Avg mIoU:  67.22  
[Epoch: 46] [Batch: 0351/0570] Loss: 0.19222  Avg Loss: 0.20726  Avg mIoU:  66.99  
[Epoch: 46] [Batch: 0401/0570] Loss: 0.20076  Avg Loss: 0.20752  Avg mIoU:  67.36  
[Epoch: 46] [Batch: 0451/0570] Loss: 0.18153  Avg Loss: 0.20679  Avg mIoU:  67.31  
[Epoch: 46] [Batch: 0501/0570] Loss: 0.25460  Avg Loss: 0.20728  Avg mIoU:  67.36  
[Epoch: 46] [Batch: 0551/0570] Loss: 0.21111  Avg Loss: 0.20651  Avg mIoU:  67.36  

*** Training [@Epoch 46] Avg Loss: 0.20629  Avg mIoU:  67.40  ***

[Epoch: 46] [Batch: 0001/0050] Loss: 0.16500  Avg Loss: 0.16500  Avg mIoU:  64.69  

*** Validation [@Epoch 46] Avg Loss: 0.16951  Avg mIoU:  64.56  ***

Model saved @46 w/ val. mIoU: 64.56.

[Epoch: 47] [Batch: 0001/0570] Loss: 0.16550  Avg Loss: 0.16550  Avg mIoU:  36.50  
[Epoch: 47] [Batch: 0051/0570] Loss: 0.23751  Avg Loss: 0.21127  Avg mIoU:  66.48  
[Epoch: 47] [Batch: 0101/0570] Loss: 0.18077  Avg Loss: 0.20717  Avg mIoU:  67.04  
[Epoch: 47] [Batch: 0151/0570] Loss: 0.17676  Avg Loss: 0.20780  Avg mIoU:  67.13  
[Epoch: 47] [Batch: 0201/0570] Loss: 0.17996  Avg Loss: 0.20809  Avg mIoU:  67.02  
[Epoch: 47] [Batch: 0251/0570] Loss: 0.16702  Avg Loss: 0.20891  Avg mIoU:  66.90  
[Epoch: 47] [Batch: 0301/0570] Loss: 0.19240  Avg Loss: 0.20887  Avg mIoU:  66.68  
[Epoch: 47] [Batch: 0351/0570] Loss: 0.20662  Avg Loss: 0.20874  Avg mIoU:  66.70  
[Epoch: 47] [Batch: 0401/0570] Loss: 0.16430  Avg Loss: 0.20793  Avg mIoU:  66.69  
[Epoch: 47] [Batch: 0451/0570] Loss: 0.19828  Avg Loss: 0.20863  Avg mIoU:  66.54  
[Epoch: 47] [Batch: 0501/0570] Loss: 0.20266  Avg Loss: 0.20776  Avg mIoU:  66.55  
[Epoch: 47] [Batch: 0551/0570] Loss: 0.30058  Avg Loss: 0.20781  Avg mIoU:  66.47  

*** Training [@Epoch 47] Avg Loss: 0.20758  Avg mIoU:  66.48  ***

[Epoch: 47] [Batch: 0001/0050] Loss: 0.16773  Avg Loss: 0.16773  Avg mIoU:  63.30  

*** Validation [@Epoch 47] Avg Loss: 0.16992  Avg mIoU:  64.35  ***

[Epoch: 48] [Batch: 0001/0570] Loss: 0.15991  Avg Loss: 0.15991  Avg mIoU:  39.86  
[Epoch: 48] [Batch: 0051/0570] Loss: 0.20005  Avg Loss: 0.20712  Avg mIoU:  68.19  
[Epoch: 48] [Batch: 0101/0570] Loss: 0.27705  Avg Loss: 0.21312  Avg mIoU:  66.77  
[Epoch: 48] [Batch: 0151/0570] Loss: 0.16962  Avg Loss: 0.20875  Avg mIoU:  67.91  
[Epoch: 48] [Batch: 0201/0570] Loss: 0.24060  Avg Loss: 0.20785  Avg mIoU:  67.64  
[Epoch: 48] [Batch: 0251/0570] Loss: 0.12620  Avg Loss: 0.20622  Avg mIoU:  67.69  
[Epoch: 48] [Batch: 0301/0570] Loss: 0.23394  Avg Loss: 0.20678  Avg mIoU:  67.47  
[Epoch: 48] [Batch: 0351/0570] Loss: 0.16865  Avg Loss: 0.20587  Avg mIoU:  67.67  
[Epoch: 48] [Batch: 0401/0570] Loss: 0.26857  Avg Loss: 0.20749  Avg mIoU:  67.50  
[Epoch: 48] [Batch: 0451/0570] Loss: 0.27709  Avg Loss: 0.20653  Avg mIoU:  67.60  
[Epoch: 48] [Batch: 0501/0570] Loss: 0.23398  Avg Loss: 0.20576  Avg mIoU:  67.34  
[Epoch: 48] [Batch: 0551/0570] Loss: 0.17181  Avg Loss: 0.20622  Avg mIoU:  67.25  

*** Training [@Epoch 48] Avg Loss: 0.20613  Avg mIoU:  67.18  ***

[Epoch: 48] [Batch: 0001/0050] Loss: 0.15951  Avg Loss: 0.15951  Avg mIoU:  62.44  

*** Validation [@Epoch 48] Avg Loss: 0.16709  Avg mIoU:  64.34  ***

[Epoch: 49] [Batch: 0001/0570] Loss: 0.19298  Avg Loss: 0.19298  Avg mIoU:  22.64  
[Epoch: 49] [Batch: 0051/0570] Loss: 0.22149  Avg Loss: 0.20947  Avg mIoU:  70.22  
[Epoch: 49] [Batch: 0101/0570] Loss: 0.20484  Avg Loss: 0.21010  Avg mIoU:  67.88  
[Epoch: 49] [Batch: 0151/0570] Loss: 0.27082  Avg Loss: 0.20738  Avg mIoU:  67.89  
[Epoch: 49] [Batch: 0201/0570] Loss: 0.22957  Avg Loss: 0.20574  Avg mIoU:  67.24  
[Epoch: 49] [Batch: 0251/0570] Loss: 0.18226  Avg Loss: 0.20701  Avg mIoU:  67.10  
[Epoch: 49] [Batch: 0301/0570] Loss: 0.22659  Avg Loss: 0.20742  Avg mIoU:  67.16  
[Epoch: 49] [Batch: 0351/0570] Loss: 0.18270  Avg Loss: 0.20646  Avg mIoU:  67.37  
[Epoch: 49] [Batch: 0401/0570] Loss: 0.27610  Avg Loss: 0.20696  Avg mIoU:  67.22  
[Epoch: 49] [Batch: 0451/0570] Loss: 0.18788  Avg Loss: 0.20597  Avg mIoU:  67.08  
[Epoch: 49] [Batch: 0501/0570] Loss: 0.22657  Avg Loss: 0.20636  Avg mIoU:  67.04  
[Epoch: 49] [Batch: 0551/0570] Loss: 0.17376  Avg Loss: 0.20604  Avg mIoU:  67.15  

*** Training [@Epoch 49] Avg Loss: 0.20675  Avg mIoU:  67.09  ***

[Epoch: 49] [Batch: 0001/0050] Loss: 0.15441  Avg Loss: 0.15441  Avg mIoU:  63.20  

*** Validation [@Epoch 49] Avg Loss: 0.15861  Avg mIoU:  63.87  ***

[Epoch: 50] [Batch: 0001/0570] Loss: 0.23586  Avg Loss: 0.23586  Avg mIoU:  44.34  
[Epoch: 50] [Batch: 0051/0570] Loss: 0.14275  Avg Loss: 0.21415  Avg mIoU:  65.40  
[Epoch: 50] [Batch: 0101/0570] Loss: 0.20144  Avg Loss: 0.20673  Avg mIoU:  65.56  
[Epoch: 50] [Batch: 0151/0570] Loss: 0.20462  Avg Loss: 0.20482  Avg mIoU:  65.50  
[Epoch: 50] [Batch: 0201/0570] Loss: 0.28040  Avg Loss: 0.20417  Avg mIoU:  66.55  
[Epoch: 50] [Batch: 0251/0570] Loss: 0.23351  Avg Loss: 0.20271  Avg mIoU:  66.79  
[Epoch: 50] [Batch: 0301/0570] Loss: 0.31261  Avg Loss: 0.20233  Avg mIoU:  67.11  
[Epoch: 50] [Batch: 0351/0570] Loss: 0.18291  Avg Loss: 0.20438  Avg mIoU:  66.85  
[Epoch: 50] [Batch: 0401/0570] Loss: 0.23239  Avg Loss: 0.20376  Avg mIoU:  66.98  
[Epoch: 50] [Batch: 0451/0570] Loss: 0.18490  Avg Loss: 0.20419  Avg mIoU:  67.07  
[Epoch: 50] [Batch: 0501/0570] Loss: 0.17882  Avg Loss: 0.20552  Avg mIoU:  67.06  
[Epoch: 50] [Batch: 0551/0570] Loss: 0.34380  Avg Loss: 0.20491  Avg mIoU:  67.26  

*** Training [@Epoch 50] Avg Loss: 0.20530  Avg mIoU:  67.20  ***

[Epoch: 50] [Batch: 0001/0050] Loss: 0.14562  Avg Loss: 0.14562  Avg mIoU:  63.30  

*** Validation [@Epoch 50] Avg Loss: 0.15643  Avg mIoU:  63.41  ***

[Epoch: 51] [Batch: 0001/0570] Loss: 0.18677  Avg Loss: 0.18677  Avg mIoU:  45.78  
[Epoch: 51] [Batch: 0051/0570] Loss: 0.18465  Avg Loss: 0.20039  Avg mIoU:  68.40  
[Epoch: 51] [Batch: 0101/0570] Loss: 0.16484  Avg Loss: 0.20217  Avg mIoU:  67.44  
[Epoch: 51] [Batch: 0151/0570] Loss: 0.23078  Avg Loss: 0.20092  Avg mIoU:  67.67  
[Epoch: 51] [Batch: 0201/0570] Loss: 0.21264  Avg Loss: 0.20327  Avg mIoU:  67.32  
[Epoch: 51] [Batch: 0251/0570] Loss: 0.18720  Avg Loss: 0.20421  Avg mIoU:  66.84  
[Epoch: 51] [Batch: 0301/0570] Loss: 0.20729  Avg Loss: 0.20281  Avg mIoU:  66.84  
[Epoch: 51] [Batch: 0351/0570] Loss: 0.17350  Avg Loss: 0.20412  Avg mIoU:  66.81  
[Epoch: 51] [Batch: 0401/0570] Loss: 0.20652  Avg Loss: 0.20461  Avg mIoU:  66.69  
[Epoch: 51] [Batch: 0451/0570] Loss: 0.19648  Avg Loss: 0.20597  Avg mIoU:  66.83  
[Epoch: 51] [Batch: 0501/0570] Loss: 0.22010  Avg Loss: 0.20702  Avg mIoU:  66.84  
[Epoch: 51] [Batch: 0551/0570] Loss: 0.14924  Avg Loss: 0.20577  Avg mIoU:  66.87  

*** Training [@Epoch 51] Avg Loss: 0.20557  Avg mIoU:  66.91  ***

[Epoch: 51] [Batch: 0001/0050] Loss: 0.15196  Avg Loss: 0.15196  Avg mIoU:  63.46  

*** Validation [@Epoch 51] Avg Loss: 0.15996  Avg mIoU:  64.50  ***

[Epoch: 52] [Batch: 0001/0570] Loss: 0.16040  Avg Loss: 0.16040  Avg mIoU:  48.07  
[Epoch: 52] [Batch: 0051/0570] Loss: 0.18946  Avg Loss: 0.19530  Avg mIoU:  68.45  
[Epoch: 52] [Batch: 0101/0570] Loss: 0.12941  Avg Loss: 0.20062  Avg mIoU:  66.94  
[Epoch: 52] [Batch: 0151/0570] Loss: 0.17615  Avg Loss: 0.20406  Avg mIoU:  67.27  
[Epoch: 52] [Batch: 0201/0570] Loss: 0.18839  Avg Loss: 0.20522  Avg mIoU:  67.29  
[Epoch: 52] [Batch: 0251/0570] Loss: 0.13363  Avg Loss: 0.20361  Avg mIoU:  67.32  
[Epoch: 52] [Batch: 0301/0570] Loss: 0.16289  Avg Loss: 0.20320  Avg mIoU:  67.14  
[Epoch: 52] [Batch: 0351/0570] Loss: 0.20340  Avg Loss: 0.20332  Avg mIoU:  67.21  
[Epoch: 52] [Batch: 0401/0570] Loss: 0.24106  Avg Loss: 0.20272  Avg mIoU:  67.04  
[Epoch: 52] [Batch: 0451/0570] Loss: 0.18955  Avg Loss: 0.20364  Avg mIoU:  66.82  
[Epoch: 52] [Batch: 0501/0570] Loss: 0.19758  Avg Loss: 0.20223  Avg mIoU:  66.82  
[Epoch: 52] [Batch: 0551/0570] Loss: 0.17591  Avg Loss: 0.20265  Avg mIoU:  67.09  

*** Training [@Epoch 52] Avg Loss: 0.20324  Avg mIoU:  66.97  ***

[Epoch: 52] [Batch: 0001/0050] Loss: 0.17069  Avg Loss: 0.17069  Avg mIoU:  64.58  

*** Validation [@Epoch 52] Avg Loss: 0.16898  Avg mIoU:  64.35  ***

[Epoch: 53] [Batch: 0001/0570] Loss: 0.23364  Avg Loss: 0.23364  Avg mIoU:  27.45  
[Epoch: 53] [Batch: 0051/0570] Loss: 0.28290  Avg Loss: 0.20667  Avg mIoU:  68.23  
[Epoch: 53] [Batch: 0101/0570] Loss: 0.17012  Avg Loss: 0.20766  Avg mIoU:  68.07  
[Epoch: 53] [Batch: 0151/0570] Loss: 0.19622  Avg Loss: 0.20438  Avg mIoU:  68.16  
[Epoch: 53] [Batch: 0201/0570] Loss: 0.14422  Avg Loss: 0.20575  Avg mIoU:  67.62  
[Epoch: 53] [Batch: 0251/0570] Loss: 0.24515  Avg Loss: 0.20492  Avg mIoU:  67.58  
[Epoch: 53] [Batch: 0301/0570] Loss: 0.19357  Avg Loss: 0.20491  Avg mIoU:  67.74  
[Epoch: 53] [Batch: 0351/0570] Loss: 0.10271  Avg Loss: 0.20283  Avg mIoU:  67.83  
[Epoch: 53] [Batch: 0401/0570] Loss: 0.18605  Avg Loss: 0.20290  Avg mIoU:  67.54  
[Epoch: 53] [Batch: 0451/0570] Loss: 0.17163  Avg Loss: 0.20228  Avg mIoU:  67.57  
[Epoch: 53] [Batch: 0501/0570] Loss: 0.26412  Avg Loss: 0.20258  Avg mIoU:  67.47  
[Epoch: 53] [Batch: 0551/0570] Loss: 0.22641  Avg Loss: 0.20199  Avg mIoU:  67.52  

*** Training [@Epoch 53] Avg Loss: 0.20233  Avg mIoU:  67.56  ***

[Epoch: 53] [Batch: 0001/0050] Loss: 0.16388  Avg Loss: 0.16388  Avg mIoU:  64.21  

*** Validation [@Epoch 53] Avg Loss: 0.16376  Avg mIoU:  62.31  ***

[Epoch: 54] [Batch: 0001/0570] Loss: 0.17321  Avg Loss: 0.17321  Avg mIoU:  44.02  
[Epoch: 54] [Batch: 0051/0570] Loss: 0.24137  Avg Loss: 0.20107  Avg mIoU:  67.23  
[Epoch: 54] [Batch: 0101/0570] Loss: 0.24848  Avg Loss: 0.20599  Avg mIoU:  67.15  
[Epoch: 54] [Batch: 0151/0570] Loss: 0.20481  Avg Loss: 0.20799  Avg mIoU:  67.01  
[Epoch: 54] [Batch: 0201/0570] Loss: 0.20083  Avg Loss: 0.20726  Avg mIoU:  66.48  
[Epoch: 54] [Batch: 0251/0570] Loss: 0.16778  Avg Loss: 0.20671  Avg mIoU:  66.62  
[Epoch: 54] [Batch: 0301/0570] Loss: 0.17120  Avg Loss: 0.20642  Avg mIoU:  66.77  
[Epoch: 54] [Batch: 0351/0570] Loss: 0.20710  Avg Loss: 0.20509  Avg mIoU:  66.89  
[Epoch: 54] [Batch: 0401/0570] Loss: 0.19683  Avg Loss: 0.20549  Avg mIoU:  66.63  
[Epoch: 54] [Batch: 0451/0570] Loss: 0.23960  Avg Loss: 0.20613  Avg mIoU:  66.84  
[Epoch: 54] [Batch: 0501/0570] Loss: 0.18503  Avg Loss: 0.20655  Avg mIoU:  66.71  
[Epoch: 54] [Batch: 0551/0570] Loss: 0.17009  Avg Loss: 0.20597  Avg mIoU:  66.78  

*** Training [@Epoch 54] Avg Loss: 0.20550  Avg mIoU:  66.90  ***

[Epoch: 54] [Batch: 0001/0050] Loss: 0.19958  Avg Loss: 0.19958  Avg mIoU:  61.62  

*** Validation [@Epoch 54] Avg Loss: 0.19524  Avg mIoU:  62.81  ***

[Epoch: 55] [Batch: 0001/0570] Loss: 0.20957  Avg Loss: 0.20957  Avg mIoU:  37.52  
[Epoch: 55] [Batch: 0051/0570] Loss: 0.14893  Avg Loss: 0.19032  Avg mIoU:  68.28  
[Epoch: 55] [Batch: 0101/0570] Loss: 0.20972  Avg Loss: 0.19858  Avg mIoU:  68.47  
[Epoch: 55] [Batch: 0151/0570] Loss: 0.18029  Avg Loss: 0.19997  Avg mIoU:  68.22  
[Epoch: 55] [Batch: 0201/0570] Loss: 0.19878  Avg Loss: 0.20121  Avg mIoU:  68.09  
[Epoch: 55] [Batch: 0251/0570] Loss: 0.15141  Avg Loss: 0.20172  Avg mIoU:  68.04  
[Epoch: 55] [Batch: 0301/0570] Loss: 0.20234  Avg Loss: 0.20140  Avg mIoU:  67.98  
[Epoch: 55] [Batch: 0351/0570] Loss: 0.22107  Avg Loss: 0.20172  Avg mIoU:  67.84  
[Epoch: 55] [Batch: 0401/0570] Loss: 0.26059  Avg Loss: 0.20206  Avg mIoU:  67.81  
[Epoch: 55] [Batch: 0451/0570] Loss: 0.21211  Avg Loss: 0.20069  Avg mIoU:  67.93  
[Epoch: 55] [Batch: 0501/0570] Loss: 0.20779  Avg Loss: 0.20112  Avg mIoU:  67.86  
[Epoch: 55] [Batch: 0551/0570] Loss: 0.15169  Avg Loss: 0.20038  Avg mIoU:  67.73  

*** Training [@Epoch 55] Avg Loss: 0.19961  Avg mIoU:  67.78  ***

[Epoch: 55] [Batch: 0001/0050] Loss: 0.13626  Avg Loss: 0.13626  Avg mIoU:  65.59  

*** Validation [@Epoch 55] Avg Loss: 0.15232  Avg mIoU:  65.22  ***

Model saved @55 w/ val. mIoU: 65.22.

[Epoch: 56] [Batch: 0001/0570] Loss: 0.17171  Avg Loss: 0.17171  Avg mIoU:  30.01  
[Epoch: 56] [Batch: 0051/0570] Loss: 0.19718  Avg Loss: 0.19807  Avg mIoU:  67.91  
[Epoch: 56] [Batch: 0101/0570] Loss: 0.20374  Avg Loss: 0.19568  Avg mIoU:  68.21  
[Epoch: 56] [Batch: 0151/0570] Loss: 0.27026  Avg Loss: 0.20099  Avg mIoU:  67.33  
[Epoch: 56] [Batch: 0201/0570] Loss: 0.18749  Avg Loss: 0.20213  Avg mIoU:  66.94  
[Epoch: 56] [Batch: 0251/0570] Loss: 0.23153  Avg Loss: 0.20071  Avg mIoU:  66.84  
[Epoch: 56] [Batch: 0301/0570] Loss: 0.18696  Avg Loss: 0.19980  Avg mIoU:  66.78  
[Epoch: 56] [Batch: 0351/0570] Loss: 0.28532  Avg Loss: 0.20006  Avg mIoU:  66.80  
[Epoch: 56] [Batch: 0401/0570] Loss: 0.22998  Avg Loss: 0.20164  Avg mIoU:  66.82  
[Epoch: 56] [Batch: 0451/0570] Loss: 0.18893  Avg Loss: 0.20088  Avg mIoU:  66.87  
[Epoch: 56] [Batch: 0501/0570] Loss: 0.22154  Avg Loss: 0.20157  Avg mIoU:  66.90  
[Epoch: 56] [Batch: 0551/0570] Loss: 0.21182  Avg Loss: 0.20095  Avg mIoU:  67.09  

*** Training [@Epoch 56] Avg Loss: 0.20079  Avg mIoU:  67.14  ***

[Epoch: 56] [Batch: 0001/0050] Loss: 0.17617  Avg Loss: 0.17617  Avg mIoU:  64.63  

*** Validation [@Epoch 56] Avg Loss: 0.17556  Avg mIoU:  64.10  ***

[Epoch: 57] [Batch: 0001/0570] Loss: 0.18003  Avg Loss: 0.18003  Avg mIoU:  44.12  
[Epoch: 57] [Batch: 0051/0570] Loss: 0.18201  Avg Loss: 0.21522  Avg mIoU:  66.45  
[Epoch: 57] [Batch: 0101/0570] Loss: 0.15214  Avg Loss: 0.20786  Avg mIoU:  67.11  
[Epoch: 57] [Batch: 0151/0570] Loss: 0.20075  Avg Loss: 0.20364  Avg mIoU:  67.61  
[Epoch: 57] [Batch: 0201/0570] Loss: 0.17380  Avg Loss: 0.20218  Avg mIoU:  68.24  
[Epoch: 57] [Batch: 0251/0570] Loss: 0.18492  Avg Loss: 0.20228  Avg mIoU:  68.28  
[Epoch: 57] [Batch: 0301/0570] Loss: 0.21245  Avg Loss: 0.20094  Avg mIoU:  67.96  
[Epoch: 57] [Batch: 0351/0570] Loss: 0.24391  Avg Loss: 0.20053  Avg mIoU:  67.94  
[Epoch: 57] [Batch: 0401/0570] Loss: 0.14814  Avg Loss: 0.20121  Avg mIoU:  67.56  
[Epoch: 57] [Batch: 0451/0570] Loss: 0.18012  Avg Loss: 0.20208  Avg mIoU:  67.44  
[Epoch: 57] [Batch: 0501/0570] Loss: 0.20029  Avg Loss: 0.20150  Avg mIoU:  67.44  
[Epoch: 57] [Batch: 0551/0570] Loss: 0.19691  Avg Loss: 0.20109  Avg mIoU:  67.35  

*** Training [@Epoch 57] Avg Loss: 0.20086  Avg mIoU:  67.37  ***

[Epoch: 57] [Batch: 0001/0050] Loss: 0.15404  Avg Loss: 0.15404  Avg mIoU:  64.39  

*** Validation [@Epoch 57] Avg Loss: 0.16648  Avg mIoU:  64.38  ***

[Epoch: 58] [Batch: 0001/0570] Loss: 0.26155  Avg Loss: 0.26155  Avg mIoU:  39.36  
[Epoch: 58] [Batch: 0051/0570] Loss: 0.16902  Avg Loss: 0.20903  Avg mIoU:  67.52  
[Epoch: 58] [Batch: 0101/0570] Loss: 0.22923  Avg Loss: 0.20062  Avg mIoU:  68.40  
[Epoch: 58] [Batch: 0151/0570] Loss: 0.19025  Avg Loss: 0.19912  Avg mIoU:  68.30  
[Epoch: 58] [Batch: 0201/0570] Loss: 0.23560  Avg Loss: 0.19762  Avg mIoU:  67.96  
[Epoch: 58] [Batch: 0251/0570] Loss: 0.15001  Avg Loss: 0.19812  Avg mIoU:  67.73  
[Epoch: 58] [Batch: 0301/0570] Loss: 0.17186  Avg Loss: 0.19730  Avg mIoU:  67.65  
[Epoch: 58] [Batch: 0351/0570] Loss: 0.30118  Avg Loss: 0.19733  Avg mIoU:  67.62  
[Epoch: 58] [Batch: 0401/0570] Loss: 0.22429  Avg Loss: 0.19872  Avg mIoU:  67.67  
[Epoch: 58] [Batch: 0451/0570] Loss: 0.17059  Avg Loss: 0.19858  Avg mIoU:  67.66  
[Epoch: 58] [Batch: 0501/0570] Loss: 0.25292  Avg Loss: 0.19904  Avg mIoU:  67.51  
[Epoch: 58] [Batch: 0551/0570] Loss: 0.23855  Avg Loss: 0.19943  Avg mIoU:  67.52  

*** Training [@Epoch 58] Avg Loss: 0.20011  Avg mIoU:  67.50  ***

[Epoch: 58] [Batch: 0001/0050] Loss: 0.13913  Avg Loss: 0.13913  Avg mIoU:  62.45  

*** Validation [@Epoch 58] Avg Loss: 0.15121  Avg mIoU:  63.72  ***

[Epoch: 59] [Batch: 0001/0570] Loss: 0.16936  Avg Loss: 0.16936  Avg mIoU:  29.37  
[Epoch: 59] [Batch: 0051/0570] Loss: 0.26673  Avg Loss: 0.21104  Avg mIoU:  64.16  
[Epoch: 59] [Batch: 0101/0570] Loss: 0.17218  Avg Loss: 0.20578  Avg mIoU:  65.87  
[Epoch: 59] [Batch: 0151/0570] Loss: 0.15138  Avg Loss: 0.20371  Avg mIoU:  66.46  
[Epoch: 59] [Batch: 0201/0570] Loss: 0.18339  Avg Loss: 0.20455  Avg mIoU:  66.41  
[Epoch: 59] [Batch: 0251/0570] Loss: 0.19252  Avg Loss: 0.20267  Avg mIoU:  66.63  
[Epoch: 59] [Batch: 0301/0570] Loss: 0.18350  Avg Loss: 0.20207  Avg mIoU:  66.93  
[Epoch: 59] [Batch: 0351/0570] Loss: 0.19666  Avg Loss: 0.20309  Avg mIoU:  67.32  
[Epoch: 59] [Batch: 0401/0570] Loss: 0.21058  Avg Loss: 0.20215  Avg mIoU:  67.24  
[Epoch: 59] [Batch: 0451/0570] Loss: 0.15371  Avg Loss: 0.20222  Avg mIoU:  67.17  
[Epoch: 59] [Batch: 0501/0570] Loss: 0.21272  Avg Loss: 0.20158  Avg mIoU:  67.37  
[Epoch: 59] [Batch: 0551/0570] Loss: 0.17738  Avg Loss: 0.20146  Avg mIoU:  67.58  

*** Training [@Epoch 59] Avg Loss: 0.20158  Avg mIoU:  67.60  ***

[Epoch: 59] [Batch: 0001/0050] Loss: 0.17384  Avg Loss: 0.17384  Avg mIoU:  60.43  

*** Validation [@Epoch 59] Avg Loss: 0.17150  Avg mIoU:  64.18  ***

[Epoch: 60] [Batch: 0001/0570] Loss: 0.12552  Avg Loss: 0.12552  Avg mIoU:  47.59  
[Epoch: 60] [Batch: 0051/0570] Loss: 0.14903  Avg Loss: 0.20397  Avg mIoU:  68.06  
[Epoch: 60] [Batch: 0101/0570] Loss: 0.14903  Avg Loss: 0.19964  Avg mIoU:  68.54  
[Epoch: 60] [Batch: 0151/0570] Loss: 0.18297  Avg Loss: 0.19800  Avg mIoU:  68.36  
[Epoch: 60] [Batch: 0201/0570] Loss: 0.17569  Avg Loss: 0.19895  Avg mIoU:  68.53  
[Epoch: 60] [Batch: 0251/0570] Loss: 0.16246  Avg Loss: 0.19773  Avg mIoU:  68.28  
[Epoch: 60] [Batch: 0301/0570] Loss: 0.14345  Avg Loss: 0.19775  Avg mIoU:  68.11  
[Epoch: 60] [Batch: 0351/0570] Loss: 0.17066  Avg Loss: 0.19751  Avg mIoU:  68.09  
[Epoch: 60] [Batch: 0401/0570] Loss: 0.23764  Avg Loss: 0.19762  Avg mIoU:  67.95  
[Epoch: 60] [Batch: 0451/0570] Loss: 0.23846  Avg Loss: 0.19800  Avg mIoU:  67.78  
[Epoch: 60] [Batch: 0501/0570] Loss: 0.26553  Avg Loss: 0.19885  Avg mIoU:  67.69  
[Epoch: 60] [Batch: 0551/0570] Loss: 0.21172  Avg Loss: 0.19850  Avg mIoU:  68.00  

*** Training [@Epoch 60] Avg Loss: 0.19853  Avg mIoU:  67.94  ***

[Epoch: 60] [Batch: 0001/0050] Loss: 0.13954  Avg Loss: 0.13954  Avg mIoU:  64.38  

*** Validation [@Epoch 60] Avg Loss: 0.15149  Avg mIoU:  63.95  ***

[Epoch: 61] [Batch: 0001/0570] Loss: 0.21812  Avg Loss: 0.21812  Avg mIoU:  40.25  
[Epoch: 61] [Batch: 0051/0570] Loss: 0.19559  Avg Loss: 0.20578  Avg mIoU:  68.47  
[Epoch: 61] [Batch: 0101/0570] Loss: 0.24657  Avg Loss: 0.19902  Avg mIoU:  68.48  
[Epoch: 61] [Batch: 0151/0570] Loss: 0.27511  Avg Loss: 0.20063  Avg mIoU:  68.87  
[Epoch: 61] [Batch: 0201/0570] Loss: 0.19128  Avg Loss: 0.20206  Avg mIoU:  68.58  
[Epoch: 61] [Batch: 0251/0570] Loss: 0.21039  Avg Loss: 0.20075  Avg mIoU:  68.63  
[Epoch: 61] [Batch: 0301/0570] Loss: 0.21890  Avg Loss: 0.20164  Avg mIoU:  68.68  
[Epoch: 61] [Batch: 0351/0570] Loss: 0.17875  Avg Loss: 0.20077  Avg mIoU:  68.61  
[Epoch: 61] [Batch: 0401/0570] Loss: 0.21889  Avg Loss: 0.20042  Avg mIoU:  68.45  
[Epoch: 61] [Batch: 0451/0570] Loss: 0.15356  Avg Loss: 0.19988  Avg mIoU:  68.37  
[Epoch: 61] [Batch: 0501/0570] Loss: 0.14128  Avg Loss: 0.19858  Avg mIoU:  68.26  
[Epoch: 61] [Batch: 0551/0570] Loss: 0.15034  Avg Loss: 0.19773  Avg mIoU:  68.30  

*** Training [@Epoch 61] Avg Loss: 0.19796  Avg mIoU:  68.25  ***

[Epoch: 61] [Batch: 0001/0050] Loss: 0.15507  Avg Loss: 0.15507  Avg mIoU:  62.30  

*** Validation [@Epoch 61] Avg Loss: 0.16582  Avg mIoU:  65.02  ***

[Epoch: 62] [Batch: 0001/0570] Loss: 0.19364  Avg Loss: 0.19364  Avg mIoU:  53.37  
[Epoch: 62] [Batch: 0051/0570] Loss: 0.18073  Avg Loss: 0.21480  Avg mIoU:  66.16  
[Epoch: 62] [Batch: 0101/0570] Loss: 0.17547  Avg Loss: 0.20432  Avg mIoU:  66.93  
[Epoch: 62] [Batch: 0151/0570] Loss: 0.17307  Avg Loss: 0.20238  Avg mIoU:  67.55  
[Epoch: 62] [Batch: 0201/0570] Loss: 0.11513  Avg Loss: 0.20076  Avg mIoU:  67.74  
[Epoch: 62] [Batch: 0251/0570] Loss: 0.16541  Avg Loss: 0.19937  Avg mIoU:  67.90  
[Epoch: 62] [Batch: 0301/0570] Loss: 0.21973  Avg Loss: 0.19873  Avg mIoU:  68.16  
[Epoch: 62] [Batch: 0351/0570] Loss: 0.21609  Avg Loss: 0.20079  Avg mIoU:  68.10  
[Epoch: 62] [Batch: 0401/0570] Loss: 0.12319  Avg Loss: 0.20020  Avg mIoU:  68.02  
[Epoch: 62] [Batch: 0451/0570] Loss: 0.17745  Avg Loss: 0.20056  Avg mIoU:  67.76  
[Epoch: 62] [Batch: 0501/0570] Loss: 0.14390  Avg Loss: 0.20027  Avg mIoU:  67.97  
[Epoch: 62] [Batch: 0551/0570] Loss: 0.14597  Avg Loss: 0.19972  Avg mIoU:  67.76  

*** Training [@Epoch 62] Avg Loss: 0.19940  Avg mIoU:  67.92  ***

[Epoch: 62] [Batch: 0001/0050] Loss: 0.15833  Avg Loss: 0.15833  Avg mIoU:  61.28  

*** Validation [@Epoch 62] Avg Loss: 0.17232  Avg mIoU:  63.07  ***

[Epoch: 63] [Batch: 0001/0570] Loss: 0.17944  Avg Loss: 0.17944  Avg mIoU:  38.41  
[Epoch: 63] [Batch: 0051/0570] Loss: 0.17401  Avg Loss: 0.19762  Avg mIoU:  68.43  
[Epoch: 63] [Batch: 0101/0570] Loss: 0.14828  Avg Loss: 0.19634  Avg mIoU:  69.12  
[Epoch: 63] [Batch: 0151/0570] Loss: 0.13716  Avg Loss: 0.19532  Avg mIoU:  69.30  
[Epoch: 63] [Batch: 0201/0570] Loss: 0.13534  Avg Loss: 0.19520  Avg mIoU:  69.11  
[Epoch: 63] [Batch: 0251/0570] Loss: 0.14494  Avg Loss: 0.19762  Avg mIoU:  68.46  
[Epoch: 63] [Batch: 0301/0570] Loss: 0.16688  Avg Loss: 0.19735  Avg mIoU:  68.30  
[Epoch: 63] [Batch: 0351/0570] Loss: 0.17511  Avg Loss: 0.19638  Avg mIoU:  68.49  
[Epoch: 63] [Batch: 0401/0570] Loss: 0.18560  Avg Loss: 0.19545  Avg mIoU:  68.41  
[Epoch: 63] [Batch: 0451/0570] Loss: 0.16599  Avg Loss: 0.19481  Avg mIoU:  68.46  
[Epoch: 63] [Batch: 0501/0570] Loss: 0.17516  Avg Loss: 0.19478  Avg mIoU:  68.38  
[Epoch: 63] [Batch: 0551/0570] Loss: 0.26360  Avg Loss: 0.19670  Avg mIoU:  68.24  

*** Training [@Epoch 63] Avg Loss: 0.19696  Avg mIoU:  68.29  ***

[Epoch: 63] [Batch: 0001/0050] Loss: 0.13884  Avg Loss: 0.13884  Avg mIoU:  64.07  

*** Validation [@Epoch 63] Avg Loss: 0.15868  Avg mIoU:  64.39  ***

[Epoch: 64] [Batch: 0001/0570] Loss: 0.13777  Avg Loss: 0.13777  Avg mIoU:  49.19  
[Epoch: 64] [Batch: 0051/0570] Loss: 0.20157  Avg Loss: 0.19728  Avg mIoU:  68.38  
[Epoch: 64] [Batch: 0101/0570] Loss: 0.15060  Avg Loss: 0.20652  Avg mIoU:  68.50  
[Epoch: 64] [Batch: 0151/0570] Loss: 0.17164  Avg Loss: 0.20182  Avg mIoU:  68.77  
[Epoch: 64] [Batch: 0201/0570] Loss: 0.32304  Avg Loss: 0.20072  Avg mIoU:  68.57  
[Epoch: 64] [Batch: 0251/0570] Loss: 0.15525  Avg Loss: 0.19860  Avg mIoU:  68.55  
[Epoch: 64] [Batch: 0301/0570] Loss: 0.20950  Avg Loss: 0.19818  Avg mIoU:  68.55  
[Epoch: 64] [Batch: 0351/0570] Loss: 0.24149  Avg Loss: 0.19746  Avg mIoU:  68.36  
[Epoch: 64] [Batch: 0401/0570] Loss: 0.17124  Avg Loss: 0.19712  Avg mIoU:  68.40  
[Epoch: 64] [Batch: 0451/0570] Loss: 0.21150  Avg Loss: 0.19819  Avg mIoU:  68.30  
[Epoch: 64] [Batch: 0501/0570] Loss: 0.26510  Avg Loss: 0.19886  Avg mIoU:  68.28  
[Epoch: 64] [Batch: 0551/0570] Loss: 0.13261  Avg Loss: 0.19846  Avg mIoU:  68.09  

*** Training [@Epoch 64] Avg Loss: 0.19797  Avg mIoU:  68.17  ***

[Epoch: 64] [Batch: 0001/0050] Loss: 0.14410  Avg Loss: 0.14410  Avg mIoU:  64.22  

*** Validation [@Epoch 64] Avg Loss: 0.15528  Avg mIoU:  64.75  ***

[Epoch: 65] [Batch: 0001/0570] Loss: 0.19039  Avg Loss: 0.19039  Avg mIoU:  32.97  
[Epoch: 65] [Batch: 0051/0570] Loss: 0.20942  Avg Loss: 0.19734  Avg mIoU:  67.54  
[Epoch: 65] [Batch: 0101/0570] Loss: 0.15601  Avg Loss: 0.19433  Avg mIoU:  67.90  
[Epoch: 65] [Batch: 0151/0570] Loss: 0.14352  Avg Loss: 0.19768  Avg mIoU:  67.65  
[Epoch: 65] [Batch: 0201/0570] Loss: 0.21443  Avg Loss: 0.19680  Avg mIoU:  67.80  
[Epoch: 65] [Batch: 0251/0570] Loss: 0.20162  Avg Loss: 0.19546  Avg mIoU:  67.92  
[Epoch: 65] [Batch: 0301/0570] Loss: 0.18375  Avg Loss: 0.19593  Avg mIoU:  67.89  
[Epoch: 65] [Batch: 0351/0570] Loss: 0.21734  Avg Loss: 0.19613  Avg mIoU:  67.64  
[Epoch: 65] [Batch: 0401/0570] Loss: 0.33022  Avg Loss: 0.19538  Avg mIoU:  67.91  
[Epoch: 65] [Batch: 0451/0570] Loss: 0.20152  Avg Loss: 0.19578  Avg mIoU:  68.19  
[Epoch: 65] [Batch: 0501/0570] Loss: 0.18777  Avg Loss: 0.19632  Avg mIoU:  68.18  
[Epoch: 65] [Batch: 0551/0570] Loss: 0.16836  Avg Loss: 0.19647  Avg mIoU:  67.99  

*** Training [@Epoch 65] Avg Loss: 0.19614  Avg mIoU:  68.14  ***

[Epoch: 65] [Batch: 0001/0050] Loss: 0.14763  Avg Loss: 0.14763  Avg mIoU:  64.33  

*** Validation [@Epoch 65] Avg Loss: 0.17185  Avg mIoU:  64.00  ***

[Epoch: 66] [Batch: 0001/0570] Loss: 0.18986  Avg Loss: 0.18986  Avg mIoU:  41.49  
[Epoch: 66] [Batch: 0051/0570] Loss: 0.17963  Avg Loss: 0.18616  Avg mIoU:  70.40  
[Epoch: 66] [Batch: 0101/0570] Loss: 0.21033  Avg Loss: 0.19136  Avg mIoU:  69.48  
[Epoch: 66] [Batch: 0151/0570] Loss: 0.23348  Avg Loss: 0.19306  Avg mIoU:  69.02  
[Epoch: 66] [Batch: 0201/0570] Loss: 0.25496  Avg Loss: 0.19457  Avg mIoU:  68.37  
[Epoch: 66] [Batch: 0251/0570] Loss: 0.17764  Avg Loss: 0.19426  Avg mIoU:  68.35  
[Epoch: 66] [Batch: 0301/0570] Loss: 0.15164  Avg Loss: 0.19522  Avg mIoU:  68.33  
[Epoch: 66] [Batch: 0351/0570] Loss: 0.19150  Avg Loss: 0.19555  Avg mIoU:  68.18  
[Epoch: 66] [Batch: 0401/0570] Loss: 0.22855  Avg Loss: 0.19798  Avg mIoU:  67.96  
[Epoch: 66] [Batch: 0451/0570] Loss: 0.17691  Avg Loss: 0.19719  Avg mIoU:  68.04  
[Epoch: 66] [Batch: 0501/0570] Loss: 0.24797  Avg Loss: 0.19660  Avg mIoU:  68.18  
[Epoch: 66] [Batch: 0551/0570] Loss: 0.18345  Avg Loss: 0.19874  Avg mIoU:  68.06  

*** Training [@Epoch 66] Avg Loss: 0.19855  Avg mIoU:  68.08  ***

[Epoch: 66] [Batch: 0001/0050] Loss: 0.13463  Avg Loss: 0.13463  Avg mIoU:  64.14  

*** Validation [@Epoch 66] Avg Loss: 0.16147  Avg mIoU:  63.27  ***

[Epoch: 67] [Batch: 0001/0570] Loss: 0.14992  Avg Loss: 0.14992  Avg mIoU:  43.01  
[Epoch: 67] [Batch: 0051/0570] Loss: 0.16987  Avg Loss: 0.20221  Avg mIoU:  68.81  
[Epoch: 67] [Batch: 0101/0570] Loss: 0.21865  Avg Loss: 0.20025  Avg mIoU:  68.74  
[Epoch: 67] [Batch: 0151/0570] Loss: 0.26476  Avg Loss: 0.20026  Avg mIoU:  67.73  
[Epoch: 67] [Batch: 0201/0570] Loss: 0.18882  Avg Loss: 0.19931  Avg mIoU:  68.04  
[Epoch: 67] [Batch: 0251/0570] Loss: 0.26463  Avg Loss: 0.19859  Avg mIoU:  67.66  
[Epoch: 67] [Batch: 0301/0570] Loss: 0.19844  Avg Loss: 0.20083  Avg mIoU:  67.69  
[Epoch: 67] [Batch: 0351/0570] Loss: 0.15149  Avg Loss: 0.20008  Avg mIoU:  67.88  
[Epoch: 67] [Batch: 0401/0570] Loss: 0.16996  Avg Loss: 0.19947  Avg mIoU:  68.01  
[Epoch: 67] [Batch: 0451/0570] Loss: 0.15343  Avg Loss: 0.19872  Avg mIoU:  67.96  
[Epoch: 67] [Batch: 0501/0570] Loss: 0.18078  Avg Loss: 0.19771  Avg mIoU:  68.02  
[Epoch: 67] [Batch: 0551/0570] Loss: 0.18520  Avg Loss: 0.19656  Avg mIoU:  68.19  

*** Training [@Epoch 67] Avg Loss: 0.19723  Avg mIoU:  68.11  ***

[Epoch: 67] [Batch: 0001/0050] Loss: 0.12553  Avg Loss: 0.12553  Avg mIoU:  64.45  

*** Validation [@Epoch 67] Avg Loss: 0.14761  Avg mIoU:  62.86  ***

[Epoch: 68] [Batch: 0001/0570] Loss: 0.20745  Avg Loss: 0.20745  Avg mIoU:  31.11  
[Epoch: 68] [Batch: 0051/0570] Loss: 0.33795  Avg Loss: 0.19919  Avg mIoU:  68.43  
[Epoch: 68] [Batch: 0101/0570] Loss: 0.19990  Avg Loss: 0.19654  Avg mIoU:  68.01  
[Epoch: 68] [Batch: 0151/0570] Loss: 0.27437  Avg Loss: 0.19404  Avg mIoU:  68.18  
[Epoch: 68] [Batch: 0201/0570] Loss: 0.24195  Avg Loss: 0.19394  Avg mIoU:  68.58  
[Epoch: 68] [Batch: 0251/0570] Loss: 0.13502  Avg Loss: 0.19448  Avg mIoU:  68.53  
[Epoch: 68] [Batch: 0301/0570] Loss: 0.15145  Avg Loss: 0.19629  Avg mIoU:  68.53  
[Epoch: 68] [Batch: 0351/0570] Loss: 0.14085  Avg Loss: 0.19608  Avg mIoU:  68.71  
[Epoch: 68] [Batch: 0401/0570] Loss: 0.14301  Avg Loss: 0.19581  Avg mIoU:  68.76  
[Epoch: 68] [Batch: 0451/0570] Loss: 0.18391  Avg Loss: 0.19628  Avg mIoU:  68.45  
[Epoch: 68] [Batch: 0501/0570] Loss: 0.20006  Avg Loss: 0.19723  Avg mIoU:  68.45  
[Epoch: 68] [Batch: 0551/0570] Loss: 0.15163  Avg Loss: 0.19612  Avg mIoU:  68.44  

*** Training [@Epoch 68] Avg Loss: 0.19588  Avg mIoU:  68.44  ***

[Epoch: 68] [Batch: 0001/0050] Loss: 0.14445  Avg Loss: 0.14445  Avg mIoU:  60.93  

*** Validation [@Epoch 68] Avg Loss: 0.15538  Avg mIoU:  62.75  ***

[Epoch: 69] [Batch: 0001/0570] Loss: 0.11631  Avg Loss: 0.11631  Avg mIoU:  30.33  
[Epoch: 69] [Batch: 0051/0570] Loss: 0.21298  Avg Loss: 0.18051  Avg mIoU:  68.52  
[Epoch: 69] [Batch: 0101/0570] Loss: 0.18791  Avg Loss: 0.18027  Avg mIoU:  69.41  
[Epoch: 69] [Batch: 0151/0570] Loss: 0.22515  Avg Loss: 0.18546  Avg mIoU:  69.25  
[Epoch: 69] [Batch: 0201/0570] Loss: 0.22694  Avg Loss: 0.18897  Avg mIoU:  69.07  
[Epoch: 69] [Batch: 0251/0570] Loss: 0.24444  Avg Loss: 0.19066  Avg mIoU:  68.80  
[Epoch: 69] [Batch: 0301/0570] Loss: 0.21297  Avg Loss: 0.19071  Avg mIoU:  68.68  
[Epoch: 69] [Batch: 0351/0570] Loss: 0.19333  Avg Loss: 0.19333  Avg mIoU:  68.53  
[Epoch: 69] [Batch: 0401/0570] Loss: 0.20343  Avg Loss: 0.19350  Avg mIoU:  68.49  
[Epoch: 69] [Batch: 0451/0570] Loss: 0.18371  Avg Loss: 0.19370  Avg mIoU:  68.39  
[Epoch: 69] [Batch: 0501/0570] Loss: 0.17350  Avg Loss: 0.19405  Avg mIoU:  68.47  
[Epoch: 69] [Batch: 0551/0570] Loss: 0.11981  Avg Loss: 0.19381  Avg mIoU:  68.57  

*** Training [@Epoch 69] Avg Loss: 0.19380  Avg mIoU:  68.65  ***

[Epoch: 69] [Batch: 0001/0050] Loss: 0.13612  Avg Loss: 0.13612  Avg mIoU:  61.09  

*** Validation [@Epoch 69] Avg Loss: 0.14813  Avg mIoU:  62.44  ***

[Epoch: 70] [Batch: 0001/0570] Loss: 0.17384  Avg Loss: 0.17384  Avg mIoU:  42.51  
[Epoch: 70] [Batch: 0051/0570] Loss: 0.17059  Avg Loss: 0.20781  Avg mIoU:  67.30  
[Epoch: 70] [Batch: 0101/0570] Loss: 0.20792  Avg Loss: 0.20570  Avg mIoU:  67.22  
[Epoch: 70] [Batch: 0151/0570] Loss: 0.22744  Avg Loss: 0.19872  Avg mIoU:  67.46  
[Epoch: 70] [Batch: 0201/0570] Loss: 0.25446  Avg Loss: 0.19958  Avg mIoU:  67.47  
[Epoch: 70] [Batch: 0251/0570] Loss: 0.17563  Avg Loss: 0.19715  Avg mIoU:  67.95  
[Epoch: 70] [Batch: 0301/0570] Loss: 0.24205  Avg Loss: 0.19689  Avg mIoU:  67.72  
[Epoch: 70] [Batch: 0351/0570] Loss: 0.17577  Avg Loss: 0.19807  Avg mIoU:  67.77  
[Epoch: 70] [Batch: 0401/0570] Loss: 0.24883  Avg Loss: 0.19657  Avg mIoU:  67.92  
[Epoch: 70] [Batch: 0451/0570] Loss: 0.18925  Avg Loss: 0.19526  Avg mIoU:  67.88  
[Epoch: 70] [Batch: 0501/0570] Loss: 0.13033  Avg Loss: 0.19568  Avg mIoU:  68.04  
[Epoch: 70] [Batch: 0551/0570] Loss: 0.18603  Avg Loss: 0.19647  Avg mIoU:  68.02  

*** Training [@Epoch 70] Avg Loss: 0.19683  Avg mIoU:  67.99  ***

[Epoch: 70] [Batch: 0001/0050] Loss: 0.18478  Avg Loss: 0.18478  Avg mIoU:  62.77  

*** Validation [@Epoch 70] Avg Loss: 0.17051  Avg mIoU:  64.11  ***

[Epoch: 71] [Batch: 0001/0570] Loss: 0.18919  Avg Loss: 0.18919  Avg mIoU:  55.96  
[Epoch: 71] [Batch: 0051/0570] Loss: 0.31744  Avg Loss: 0.19166  Avg mIoU:  69.72  
[Epoch: 71] [Batch: 0101/0570] Loss: 0.21932  Avg Loss: 0.19197  Avg mIoU:  70.02  
[Epoch: 71] [Batch: 0151/0570] Loss: 0.24630  Avg Loss: 0.19508  Avg mIoU:  69.40  
[Epoch: 71] [Batch: 0201/0570] Loss: 0.23653  Avg Loss: 0.19690  Avg mIoU:  69.07  
[Epoch: 71] [Batch: 0251/0570] Loss: 0.18831  Avg Loss: 0.19775  Avg mIoU:  68.79  
[Epoch: 71] [Batch: 0301/0570] Loss: 0.15596  Avg Loss: 0.19571  Avg mIoU:  69.01  
[Epoch: 71] [Batch: 0351/0570] Loss: 0.17828  Avg Loss: 0.19562  Avg mIoU:  68.86  
[Epoch: 71] [Batch: 0401/0570] Loss: 0.26271  Avg Loss: 0.19588  Avg mIoU:  68.74  
[Epoch: 71] [Batch: 0451/0570] Loss: 0.21002  Avg Loss: 0.19507  Avg mIoU:  68.61  
[Epoch: 71] [Batch: 0501/0570] Loss: 0.22004  Avg Loss: 0.19558  Avg mIoU:  68.55  
[Epoch: 71] [Batch: 0551/0570] Loss: 0.18801  Avg Loss: 0.19578  Avg mIoU:  68.46  

*** Training [@Epoch 71] Avg Loss: 0.19533  Avg mIoU:  68.50  ***

[Epoch: 71] [Batch: 0001/0050] Loss: 0.14282  Avg Loss: 0.14282  Avg mIoU:  63.39  

*** Validation [@Epoch 71] Avg Loss: 0.15846  Avg mIoU:  63.93  ***

[Epoch: 72] [Batch: 0001/0570] Loss: 0.28200  Avg Loss: 0.28200  Avg mIoU:  51.50  
[Epoch: 72] [Batch: 0051/0570] Loss: 0.11721  Avg Loss: 0.19658  Avg mIoU:  69.61  
[Epoch: 72] [Batch: 0101/0570] Loss: 0.17977  Avg Loss: 0.19210  Avg mIoU:  69.74  
[Epoch: 72] [Batch: 0151/0570] Loss: 0.15648  Avg Loss: 0.19052  Avg mIoU:  69.73  
[Epoch: 72] [Batch: 0201/0570] Loss: 0.22909  Avg Loss: 0.19156  Avg mIoU:  69.24  
[Epoch: 72] [Batch: 0251/0570] Loss: 0.14270  Avg Loss: 0.19103  Avg mIoU:  69.34  
[Epoch: 72] [Batch: 0301/0570] Loss: 0.25430  Avg Loss: 0.19037  Avg mIoU:  69.19  
[Epoch: 72] [Batch: 0351/0570] Loss: 0.22728  Avg Loss: 0.19049  Avg mIoU:  69.01  
[Epoch: 72] [Batch: 0401/0570] Loss: 0.27168  Avg Loss: 0.19087  Avg mIoU:  68.99  
[Epoch: 72] [Batch: 0451/0570] Loss: 0.17288  Avg Loss: 0.19062  Avg mIoU:  69.00  
[Epoch: 72] [Batch: 0501/0570] Loss: 0.19495  Avg Loss: 0.19158  Avg mIoU:  68.81  
[Epoch: 72] [Batch: 0551/0570] Loss: 0.22704  Avg Loss: 0.19243  Avg mIoU:  68.77  

*** Training [@Epoch 72] Avg Loss: 0.19270  Avg mIoU:  68.70  ***

[Epoch: 72] [Batch: 0001/0050] Loss: 0.14924  Avg Loss: 0.14924  Avg mIoU:  64.25  

*** Validation [@Epoch 72] Avg Loss: 0.15637  Avg mIoU:  63.91  ***

[Epoch: 73] [Batch: 0001/0570] Loss: 0.22796  Avg Loss: 0.22796  Avg mIoU:  36.67  
[Epoch: 73] [Batch: 0051/0570] Loss: 0.14427  Avg Loss: 0.18853  Avg mIoU:  70.20  
[Epoch: 73] [Batch: 0101/0570] Loss: 0.16039  Avg Loss: 0.19244  Avg mIoU:  69.58  
[Epoch: 73] [Batch: 0151/0570] Loss: 0.22349  Avg Loss: 0.18836  Avg mIoU:  69.28  
[Epoch: 73] [Batch: 0201/0570] Loss: 0.19827  Avg Loss: 0.18737  Avg mIoU:  69.33  
[Epoch: 73] [Batch: 0251/0570] Loss: 0.14998  Avg Loss: 0.18886  Avg mIoU:  69.29  
[Epoch: 73] [Batch: 0301/0570] Loss: 0.19004  Avg Loss: 0.19102  Avg mIoU:  69.28  
[Epoch: 73] [Batch: 0351/0570] Loss: 0.15420  Avg Loss: 0.19104  Avg mIoU:  69.38  
[Epoch: 73] [Batch: 0401/0570] Loss: 0.19827  Avg Loss: 0.19105  Avg mIoU:  69.06  
[Epoch: 73] [Batch: 0451/0570] Loss: 0.25200  Avg Loss: 0.19160  Avg mIoU:  68.96  
[Epoch: 73] [Batch: 0501/0570] Loss: 0.21391  Avg Loss: 0.19195  Avg mIoU:  69.08  
[Epoch: 73] [Batch: 0551/0570] Loss: 0.19196  Avg Loss: 0.19310  Avg mIoU:  69.00  

*** Training [@Epoch 73] Avg Loss: 0.19323  Avg mIoU:  68.94  ***

[Epoch: 73] [Batch: 0001/0050] Loss: 0.16191  Avg Loss: 0.16191  Avg mIoU:  63.08  

*** Validation [@Epoch 73] Avg Loss: 0.16434  Avg mIoU:  64.30  ***

[Epoch: 74] [Batch: 0001/0570] Loss: 0.17007  Avg Loss: 0.17007  Avg mIoU:  36.34  
[Epoch: 74] [Batch: 0051/0570] Loss: 0.17084  Avg Loss: 0.19374  Avg mIoU:  69.98  
[Epoch: 74] [Batch: 0101/0570] Loss: 0.26953  Avg Loss: 0.19286  Avg mIoU:  68.59  
[Epoch: 74] [Batch: 0151/0570] Loss: 0.22219  Avg Loss: 0.19763  Avg mIoU:  68.66  
[Epoch: 74] [Batch: 0201/0570] Loss: 0.18359  Avg Loss: 0.19511  Avg mIoU:  68.76  
[Epoch: 74] [Batch: 0251/0570] Loss: 0.28863  Avg Loss: 0.19557  Avg mIoU:  68.68  
[Epoch: 74] [Batch: 0301/0570] Loss: 0.18046  Avg Loss: 0.19495  Avg mIoU:  68.59  
[Epoch: 74] [Batch: 0351/0570] Loss: 0.19970  Avg Loss: 0.19428  Avg mIoU:  68.68  
[Epoch: 74] [Batch: 0401/0570] Loss: 0.19485  Avg Loss: 0.19368  Avg mIoU:  68.65  
[Epoch: 74] [Batch: 0451/0570] Loss: 0.18518  Avg Loss: 0.19498  Avg mIoU:  68.59  
[Epoch: 74] [Batch: 0501/0570] Loss: 0.24151  Avg Loss: 0.19454  Avg mIoU:  68.55  
[Epoch: 74] [Batch: 0551/0570] Loss: 0.26791  Avg Loss: 0.19461  Avg mIoU:  68.61  

*** Training [@Epoch 74] Avg Loss: 0.19438  Avg mIoU:  68.64  ***

[Epoch: 74] [Batch: 0001/0050] Loss: 0.15157  Avg Loss: 0.15157  Avg mIoU:  64.65  

*** Validation [@Epoch 74] Avg Loss: 0.15842  Avg mIoU:  65.33  ***

Model saved @74 w/ val. mIoU: 65.33.

[Epoch: 75] [Batch: 0001/0570] Loss: 0.23112  Avg Loss: 0.23112  Avg mIoU:  44.60  
[Epoch: 75] [Batch: 0051/0570] Loss: 0.16714  Avg Loss: 0.19491  Avg mIoU:  67.29  
[Epoch: 75] [Batch: 0101/0570] Loss: 0.21512  Avg Loss: 0.19808  Avg mIoU:  66.27  
[Epoch: 75] [Batch: 0151/0570] Loss: 0.13159  Avg Loss: 0.19383  Avg mIoU:  67.22  
[Epoch: 75] [Batch: 0201/0570] Loss: 0.17169  Avg Loss: 0.19482  Avg mIoU:  67.75  
[Epoch: 75] [Batch: 0251/0570] Loss: 0.24919  Avg Loss: 0.19210  Avg mIoU:  68.56  
[Epoch: 75] [Batch: 0301/0570] Loss: 0.17592  Avg Loss: 0.19241  Avg mIoU:  68.96  
[Epoch: 75] [Batch: 0351/0570] Loss: 0.23685  Avg Loss: 0.19247  Avg mIoU:  68.78  
[Epoch: 75] [Batch: 0401/0570] Loss: 0.15845  Avg Loss: 0.19206  Avg mIoU:  68.57  
[Epoch: 75] [Batch: 0451/0570] Loss: 0.24174  Avg Loss: 0.19216  Avg mIoU:  68.48  
[Epoch: 75] [Batch: 0501/0570] Loss: 0.14410  Avg Loss: 0.19224  Avg mIoU:  68.48  
[Epoch: 75] [Batch: 0551/0570] Loss: 0.25067  Avg Loss: 0.19254  Avg mIoU:  68.75  

*** Training [@Epoch 75] Avg Loss: 0.19256  Avg mIoU:  68.75  ***

[Epoch: 75] [Batch: 0001/0050] Loss: 0.13317  Avg Loss: 0.13317  Avg mIoU:  62.74  

*** Validation [@Epoch 75] Avg Loss: 0.15428  Avg mIoU:  64.32  ***

[Epoch: 76] [Batch: 0001/0570] Loss: 0.17806  Avg Loss: 0.17806  Avg mIoU:  33.49  
[Epoch: 76] [Batch: 0051/0570] Loss: 0.15850  Avg Loss: 0.20515  Avg mIoU:  69.91  
[Epoch: 76] [Batch: 0101/0570] Loss: 0.15643  Avg Loss: 0.20135  Avg mIoU:  69.29  
[Epoch: 76] [Batch: 0151/0570] Loss: 0.13259  Avg Loss: 0.19927  Avg mIoU:  68.92  
[Epoch: 76] [Batch: 0201/0570] Loss: 0.14118  Avg Loss: 0.19431  Avg mIoU:  69.11  
[Epoch: 76] [Batch: 0251/0570] Loss: 0.12292  Avg Loss: 0.19196  Avg mIoU:  68.96  
[Epoch: 76] [Batch: 0301/0570] Loss: 0.23093  Avg Loss: 0.19170  Avg mIoU:  69.05  
[Epoch: 76] [Batch: 0351/0570] Loss: 0.17789  Avg Loss: 0.19241  Avg mIoU:  68.80  
[Epoch: 76] [Batch: 0401/0570] Loss: 0.15843  Avg Loss: 0.19345  Avg mIoU:  68.88  
[Epoch: 76] [Batch: 0451/0570] Loss: 0.24282  Avg Loss: 0.19286  Avg mIoU:  68.79  
[Epoch: 76] [Batch: 0501/0570] Loss: 0.19473  Avg Loss: 0.19148  Avg mIoU:  68.88  
[Epoch: 76] [Batch: 0551/0570] Loss: 0.20671  Avg Loss: 0.19189  Avg mIoU:  68.78  

*** Training [@Epoch 76] Avg Loss: 0.19157  Avg mIoU:  68.85  ***

[Epoch: 76] [Batch: 0001/0050] Loss: 0.16354  Avg Loss: 0.16354  Avg mIoU:  60.81  

*** Validation [@Epoch 76] Avg Loss: 0.16691  Avg mIoU:  64.27  ***

[Epoch: 77] [Batch: 0001/0570] Loss: 0.13158  Avg Loss: 0.13158  Avg mIoU:  47.26  
[Epoch: 77] [Batch: 0051/0570] Loss: 0.15002  Avg Loss: 0.19833  Avg mIoU:  68.60  
[Epoch: 77] [Batch: 0101/0570] Loss: 0.22997  Avg Loss: 0.19236  Avg mIoU:  69.11  
[Epoch: 77] [Batch: 0151/0570] Loss: 0.16637  Avg Loss: 0.18923  Avg mIoU:  68.92  
[Epoch: 77] [Batch: 0201/0570] Loss: 0.28408  Avg Loss: 0.19050  Avg mIoU:  68.76  
[Epoch: 77] [Batch: 0251/0570] Loss: 0.13200  Avg Loss: 0.19128  Avg mIoU:  68.55  
[Epoch: 77] [Batch: 0301/0570] Loss: 0.12003  Avg Loss: 0.19084  Avg mIoU:  68.47  
[Epoch: 77] [Batch: 0351/0570] Loss: 0.26637  Avg Loss: 0.19147  Avg mIoU:  68.53  
[Epoch: 77] [Batch: 0401/0570] Loss: 0.22266  Avg Loss: 0.19281  Avg mIoU:  68.39  
[Epoch: 77] [Batch: 0451/0570] Loss: 0.20174  Avg Loss: 0.19332  Avg mIoU:  68.70  
[Epoch: 77] [Batch: 0501/0570] Loss: 0.15194  Avg Loss: 0.19179  Avg mIoU:  68.87  
[Epoch: 77] [Batch: 0551/0570] Loss: 0.26459  Avg Loss: 0.19177  Avg mIoU:  68.91  

*** Training [@Epoch 77] Avg Loss: 0.19185  Avg mIoU:  68.97  ***

[Epoch: 77] [Batch: 0001/0050] Loss: 0.14954  Avg Loss: 0.14954  Avg mIoU:  65.31  

*** Validation [@Epoch 77] Avg Loss: 0.17845  Avg mIoU:  63.27  ***

[Epoch: 78] [Batch: 0001/0570] Loss: 0.16745  Avg Loss: 0.16745  Avg mIoU:  37.40  
[Epoch: 78] [Batch: 0051/0570] Loss: 0.21812  Avg Loss: 0.19586  Avg mIoU:  67.64  
[Epoch: 78] [Batch: 0101/0570] Loss: 0.15688  Avg Loss: 0.18801  Avg mIoU:  69.18  
[Epoch: 78] [Batch: 0151/0570] Loss: 0.16798  Avg Loss: 0.18878  Avg mIoU:  69.11  
[Epoch: 78] [Batch: 0201/0570] Loss: 0.23181  Avg Loss: 0.18891  Avg mIoU:  69.36  
[Epoch: 78] [Batch: 0251/0570] Loss: 0.13281  Avg Loss: 0.18901  Avg mIoU:  69.37  
[Epoch: 78] [Batch: 0301/0570] Loss: 0.14708  Avg Loss: 0.18991  Avg mIoU:  69.03  
[Epoch: 78] [Batch: 0351/0570] Loss: 0.20760  Avg Loss: 0.19192  Avg mIoU:  68.79  
[Epoch: 78] [Batch: 0401/0570] Loss: 0.20243  Avg Loss: 0.18939  Avg mIoU:  69.04  
[Epoch: 78] [Batch: 0451/0570] Loss: 0.21711  Avg Loss: 0.18962  Avg mIoU:  69.15  
[Epoch: 78] [Batch: 0501/0570] Loss: 0.18757  Avg Loss: 0.19067  Avg mIoU:  69.16  
[Epoch: 78] [Batch: 0551/0570] Loss: 0.19885  Avg Loss: 0.19143  Avg mIoU:  69.06  

*** Training [@Epoch 78] Avg Loss: 0.19169  Avg mIoU:  68.97  ***

[Epoch: 78] [Batch: 0001/0050] Loss: 0.13895  Avg Loss: 0.13895  Avg mIoU:  63.06  

*** Validation [@Epoch 78] Avg Loss: 0.15926  Avg mIoU:  63.57  ***

[Epoch: 79] [Batch: 0001/0570] Loss: 0.08453  Avg Loss: 0.08453  Avg mIoU:  47.06  
[Epoch: 79] [Batch: 0051/0570] Loss: 0.18155  Avg Loss: 0.18753  Avg mIoU:  68.37  
[Epoch: 79] [Batch: 0101/0570] Loss: 0.19577  Avg Loss: 0.18669  Avg mIoU:  69.33  
[Epoch: 79] [Batch: 0151/0570] Loss: 0.13408  Avg Loss: 0.18969  Avg mIoU:  69.06  
[Epoch: 79] [Batch: 0201/0570] Loss: 0.25090  Avg Loss: 0.18997  Avg mIoU:  68.87  
[Epoch: 79] [Batch: 0251/0570] Loss: 0.18816  Avg Loss: 0.19173  Avg mIoU:  69.47  
[Epoch: 79] [Batch: 0301/0570] Loss: 0.19028  Avg Loss: 0.19039  Avg mIoU:  69.47  
[Epoch: 79] [Batch: 0351/0570] Loss: 0.24437  Avg Loss: 0.19073  Avg mIoU:  69.23  
[Epoch: 79] [Batch: 0401/0570] Loss: 0.28120  Avg Loss: 0.19058  Avg mIoU:  69.26  
[Epoch: 79] [Batch: 0451/0570] Loss: 0.22342  Avg Loss: 0.19085  Avg mIoU:  69.01  
[Epoch: 79] [Batch: 0501/0570] Loss: 0.18466  Avg Loss: 0.19163  Avg mIoU:  68.86  
[Epoch: 79] [Batch: 0551/0570] Loss: 0.18359  Avg Loss: 0.19171  Avg mIoU:  69.06  

*** Training [@Epoch 79] Avg Loss: 0.19183  Avg mIoU:  69.07  ***

[Epoch: 79] [Batch: 0001/0050] Loss: 0.13530  Avg Loss: 0.13530  Avg mIoU:  63.47  

*** Validation [@Epoch 79] Avg Loss: 0.15348  Avg mIoU:  64.35  ***

[Epoch: 80] [Batch: 0001/0570] Loss: 0.17884  Avg Loss: 0.17884  Avg mIoU:  46.60  
[Epoch: 80] [Batch: 0051/0570] Loss: 0.19435  Avg Loss: 0.19622  Avg mIoU:  67.74  
[Epoch: 80] [Batch: 0101/0570] Loss: 0.14145  Avg Loss: 0.19152  Avg mIoU:  68.61  
[Epoch: 80] [Batch: 0151/0570] Loss: 0.13606  Avg Loss: 0.18598  Avg mIoU:  69.07  
[Epoch: 80] [Batch: 0201/0570] Loss: 0.24508  Avg Loss: 0.19082  Avg mIoU:  68.90  
[Epoch: 80] [Batch: 0251/0570] Loss: 0.23122  Avg Loss: 0.19100  Avg mIoU:  69.06  
[Epoch: 80] [Batch: 0301/0570] Loss: 0.31704  Avg Loss: 0.19026  Avg mIoU:  69.00  
[Epoch: 80] [Batch: 0351/0570] Loss: 0.21669  Avg Loss: 0.18912  Avg mIoU:  69.00  
[Epoch: 80] [Batch: 0401/0570] Loss: 0.13055  Avg Loss: 0.18934  Avg mIoU:  68.98  
[Epoch: 80] [Batch: 0451/0570] Loss: 0.16116  Avg Loss: 0.18894  Avg mIoU:  68.87  
[Epoch: 80] [Batch: 0501/0570] Loss: 0.13901  Avg Loss: 0.18897  Avg mIoU:  68.99  
[Epoch: 80] [Batch: 0551/0570] Loss: 0.15286  Avg Loss: 0.18931  Avg mIoU:  69.08  

*** Training [@Epoch 80] Avg Loss: 0.18996  Avg mIoU:  68.96  ***

[Epoch: 80] [Batch: 0001/0050] Loss: 0.13886  Avg Loss: 0.13886  Avg mIoU:  65.39  

*** Validation [@Epoch 80] Avg Loss: 0.15978  Avg mIoU:  63.04  ***

[Epoch: 81] [Batch: 0001/0570] Loss: 0.19877  Avg Loss: 0.19877  Avg mIoU:  30.36  
[Epoch: 81] [Batch: 0051/0570] Loss: 0.18573  Avg Loss: 0.19560  Avg mIoU:  68.15  
[Epoch: 81] [Batch: 0101/0570] Loss: 0.18630  Avg Loss: 0.19689  Avg mIoU:  68.07  
[Epoch: 81] [Batch: 0151/0570] Loss: 0.20608  Avg Loss: 0.19358  Avg mIoU:  68.25  
[Epoch: 81] [Batch: 0201/0570] Loss: 0.12523  Avg Loss: 0.19068  Avg mIoU:  68.76  
[Epoch: 81] [Batch: 0251/0570] Loss: 0.15279  Avg Loss: 0.18894  Avg mIoU:  69.14  
[Epoch: 81] [Batch: 0301/0570] Loss: 0.20690  Avg Loss: 0.19047  Avg mIoU:  68.90  
[Epoch: 81] [Batch: 0351/0570] Loss: 0.14731  Avg Loss: 0.18986  Avg mIoU:  69.06  
[Epoch: 81] [Batch: 0401/0570] Loss: 0.11806  Avg Loss: 0.18882  Avg mIoU:  69.20  
[Epoch: 81] [Batch: 0451/0570] Loss: 0.25752  Avg Loss: 0.18813  Avg mIoU:  69.32  
[Epoch: 81] [Batch: 0501/0570] Loss: 0.23881  Avg Loss: 0.18916  Avg mIoU:  69.15  
[Epoch: 81] [Batch: 0551/0570] Loss: 0.27044  Avg Loss: 0.18981  Avg mIoU:  69.27  

*** Training [@Epoch 81] Avg Loss: 0.19000  Avg mIoU:  69.20  ***

[Epoch: 81] [Batch: 0001/0050] Loss: 0.17453  Avg Loss: 0.17453  Avg mIoU:  64.25  

*** Validation [@Epoch 81] Avg Loss: 0.18470  Avg mIoU:  64.74  ***

[Epoch: 82] [Batch: 0001/0570] Loss: 0.14392  Avg Loss: 0.14392  Avg mIoU:  35.59  
[Epoch: 82] [Batch: 0051/0570] Loss: 0.16045  Avg Loss: 0.20221  Avg mIoU:  69.38  
[Epoch: 82] [Batch: 0101/0570] Loss: 0.23219  Avg Loss: 0.19367  Avg mIoU:  69.10  
[Epoch: 82] [Batch: 0151/0570] Loss: 0.27174  Avg Loss: 0.18774  Avg mIoU:  69.99  
[Epoch: 82] [Batch: 0201/0570] Loss: 0.13974  Avg Loss: 0.18720  Avg mIoU:  69.71  
[Epoch: 82] [Batch: 0251/0570] Loss: 0.14084  Avg Loss: 0.18697  Avg mIoU:  69.52  
[Epoch: 82] [Batch: 0301/0570] Loss: 0.15822  Avg Loss: 0.18754  Avg mIoU:  69.40  
[Epoch: 82] [Batch: 0351/0570] Loss: 0.30386  Avg Loss: 0.18693  Avg mIoU:  69.21  
[Epoch: 82] [Batch: 0401/0570] Loss: 0.14087  Avg Loss: 0.18772  Avg mIoU:  69.17  
[Epoch: 82] [Batch: 0451/0570] Loss: 0.33392  Avg Loss: 0.18855  Avg mIoU:  69.17  
[Epoch: 82] [Batch: 0501/0570] Loss: 0.17121  Avg Loss: 0.18895  Avg mIoU:  68.92  
[Epoch: 82] [Batch: 0551/0570] Loss: 0.13694  Avg Loss: 0.18924  Avg mIoU:  69.12  

*** Training [@Epoch 82] Avg Loss: 0.18921  Avg mIoU:  69.13  ***

[Epoch: 82] [Batch: 0001/0050] Loss: 0.14061  Avg Loss: 0.14061  Avg mIoU:  65.58  

*** Validation [@Epoch 82] Avg Loss: 0.16097  Avg mIoU:  64.69  ***

[Epoch: 83] [Batch: 0001/0570] Loss: 0.14530  Avg Loss: 0.14530  Avg mIoU:  53.20  
[Epoch: 83] [Batch: 0051/0570] Loss: 0.24539  Avg Loss: 0.18866  Avg mIoU:  71.01  
[Epoch: 83] [Batch: 0101/0570] Loss: 0.12687  Avg Loss: 0.18512  Avg mIoU:  69.94  
[Epoch: 83] [Batch: 0151/0570] Loss: 0.18249  Avg Loss: 0.18459  Avg mIoU:  70.17  
[Epoch: 83] [Batch: 0201/0570] Loss: 0.18661  Avg Loss: 0.18734  Avg mIoU:  69.69  
[Epoch: 83] [Batch: 0251/0570] Loss: 0.20105  Avg Loss: 0.18803  Avg mIoU:  69.69  
[Epoch: 83] [Batch: 0301/0570] Loss: 0.15918  Avg Loss: 0.18885  Avg mIoU:  69.78  
[Epoch: 83] [Batch: 0351/0570] Loss: 0.20169  Avg Loss: 0.18836  Avg mIoU:  69.71  
[Epoch: 83] [Batch: 0401/0570] Loss: 0.17020  Avg Loss: 0.18824  Avg mIoU:  69.56  
[Epoch: 83] [Batch: 0451/0570] Loss: 0.22275  Avg Loss: 0.18842  Avg mIoU:  69.62  
[Epoch: 83] [Batch: 0501/0570] Loss: 0.24874  Avg Loss: 0.18811  Avg mIoU:  69.73  
[Epoch: 83] [Batch: 0551/0570] Loss: 0.16750  Avg Loss: 0.18778  Avg mIoU:  69.92  

*** Training [@Epoch 83] Avg Loss: 0.18755  Avg mIoU:  69.89  ***

[Epoch: 83] [Batch: 0001/0050] Loss: 0.13620  Avg Loss: 0.13620  Avg mIoU:  63.77  

*** Validation [@Epoch 83] Avg Loss: 0.16751  Avg mIoU:  63.99  ***

[Epoch: 84] [Batch: 0001/0570] Loss: 0.18375  Avg Loss: 0.18375  Avg mIoU:  30.21  
[Epoch: 84] [Batch: 0051/0570] Loss: 0.15456  Avg Loss: 0.18424  Avg mIoU:  67.37  
[Epoch: 84] [Batch: 0101/0570] Loss: 0.18189  Avg Loss: 0.18990  Avg mIoU:  67.85  
[Epoch: 84] [Batch: 0151/0570] Loss: 0.18102  Avg Loss: 0.19006  Avg mIoU:  68.95  
[Epoch: 84] [Batch: 0201/0570] Loss: 0.19906  Avg Loss: 0.18972  Avg mIoU:  68.95  
[Epoch: 84] [Batch: 0251/0570] Loss: 0.27479  Avg Loss: 0.19077  Avg mIoU:  69.30  
[Epoch: 84] [Batch: 0301/0570] Loss: 0.15346  Avg Loss: 0.19075  Avg mIoU:  69.05  
[Epoch: 84] [Batch: 0351/0570] Loss: 0.19308  Avg Loss: 0.18892  Avg mIoU:  69.20  
[Epoch: 84] [Batch: 0401/0570] Loss: 0.17606  Avg Loss: 0.19007  Avg mIoU:  69.20  
[Epoch: 84] [Batch: 0451/0570] Loss: 0.24735  Avg Loss: 0.19154  Avg mIoU:  69.10  
[Epoch: 84] [Batch: 0501/0570] Loss: 0.14565  Avg Loss: 0.19110  Avg mIoU:  69.20  
[Epoch: 84] [Batch: 0551/0570] Loss: 0.17978  Avg Loss: 0.19004  Avg mIoU:  69.45  

*** Training [@Epoch 84] Avg Loss: 0.18974  Avg mIoU:  69.47  ***

[Epoch: 84] [Batch: 0001/0050] Loss: 0.13436  Avg Loss: 0.13436  Avg mIoU:  62.63  

*** Validation [@Epoch 84] Avg Loss: 0.15335  Avg mIoU:  63.97  ***

[Epoch: 85] [Batch: 0001/0570] Loss: 0.15412  Avg Loss: 0.15412  Avg mIoU:  47.15  
[Epoch: 85] [Batch: 0051/0570] Loss: 0.19414  Avg Loss: 0.18937  Avg mIoU:  69.67  
[Epoch: 85] [Batch: 0101/0570] Loss: 0.15734  Avg Loss: 0.18740  Avg mIoU:  69.96  
[Epoch: 85] [Batch: 0151/0570] Loss: 0.12211  Avg Loss: 0.18881  Avg mIoU:  69.52  
[Epoch: 85] [Batch: 0201/0570] Loss: 0.17145  Avg Loss: 0.18741  Avg mIoU:  69.97  
[Epoch: 85] [Batch: 0251/0570] Loss: 0.24944  Avg Loss: 0.18765  Avg mIoU:  70.33  
[Epoch: 85] [Batch: 0301/0570] Loss: 0.26159  Avg Loss: 0.18876  Avg mIoU:  69.98  
[Epoch: 85] [Batch: 0351/0570] Loss: 0.24594  Avg Loss: 0.18836  Avg mIoU:  69.93  
[Epoch: 85] [Batch: 0401/0570] Loss: 0.15153  Avg Loss: 0.18837  Avg mIoU:  69.92  
[Epoch: 85] [Batch: 0451/0570] Loss: 0.16303  Avg Loss: 0.18835  Avg mIoU:  69.83  
[Epoch: 85] [Batch: 0501/0570] Loss: 0.15641  Avg Loss: 0.18852  Avg mIoU:  69.67  
[Epoch: 85] [Batch: 0551/0570] Loss: 0.17099  Avg Loss: 0.18835  Avg mIoU:  69.44  

*** Training [@Epoch 85] Avg Loss: 0.18804  Avg mIoU:  69.61  ***

[Epoch: 85] [Batch: 0001/0050] Loss: 0.16797  Avg Loss: 0.16797  Avg mIoU:  64.78  

*** Validation [@Epoch 85] Avg Loss: 0.18080  Avg mIoU:  64.22  ***

[Epoch: 86] [Batch: 0001/0570] Loss: 0.24215  Avg Loss: 0.24215  Avg mIoU:  27.49  
[Epoch: 86] [Batch: 0051/0570] Loss: 0.16771  Avg Loss: 0.19395  Avg mIoU:  68.73  
[Epoch: 86] [Batch: 0101/0570] Loss: 0.18135  Avg Loss: 0.18767  Avg mIoU:  69.03  
[Epoch: 86] [Batch: 0151/0570] Loss: 0.23626  Avg Loss: 0.18862  Avg mIoU:  68.27  
[Epoch: 86] [Batch: 0201/0570] Loss: 0.17041  Avg Loss: 0.18879  Avg mIoU:  68.71  
[Epoch: 86] [Batch: 0251/0570] Loss: 0.19571  Avg Loss: 0.18836  Avg mIoU:  68.97  
[Epoch: 86] [Batch: 0301/0570] Loss: 0.21109  Avg Loss: 0.18864  Avg mIoU:  69.11  
[Epoch: 86] [Batch: 0351/0570] Loss: 0.23988  Avg Loss: 0.18903  Avg mIoU:  69.02  
[Epoch: 86] [Batch: 0401/0570] Loss: 0.14227  Avg Loss: 0.18969  Avg mIoU:  68.82  
[Epoch: 86] [Batch: 0451/0570] Loss: 0.16433  Avg Loss: 0.18962  Avg mIoU:  68.92  
[Epoch: 86] [Batch: 0501/0570] Loss: 0.16572  Avg Loss: 0.18789  Avg mIoU:  69.18  
[Epoch: 86] [Batch: 0551/0570] Loss: 0.22917  Avg Loss: 0.18744  Avg mIoU:  69.36  

*** Training [@Epoch 86] Avg Loss: 0.18688  Avg mIoU:  69.35  ***

[Epoch: 86] [Batch: 0001/0050] Loss: 0.15393  Avg Loss: 0.15393  Avg mIoU:  62.55  

*** Validation [@Epoch 86] Avg Loss: 0.16885  Avg mIoU:  66.46  ***

Model saved @86 w/ val. mIoU: 66.46.

[Epoch: 87] [Batch: 0001/0570] Loss: 0.14736  Avg Loss: 0.14736  Avg mIoU:  51.41  
[Epoch: 87] [Batch: 0051/0570] Loss: 0.17952  Avg Loss: 0.18689  Avg mIoU:  69.37  
[Epoch: 87] [Batch: 0101/0570] Loss: 0.20228  Avg Loss: 0.18526  Avg mIoU:  69.49  
[Epoch: 87] [Batch: 0151/0570] Loss: 0.17959  Avg Loss: 0.18700  Avg mIoU:  69.74  
[Epoch: 87] [Batch: 0201/0570] Loss: 0.30054  Avg Loss: 0.18738  Avg mIoU:  69.77  
[Epoch: 87] [Batch: 0251/0570] Loss: 0.17562  Avg Loss: 0.18776  Avg mIoU:  69.50  
[Epoch: 87] [Batch: 0301/0570] Loss: 0.22726  Avg Loss: 0.18855  Avg mIoU:  69.18  
[Epoch: 87] [Batch: 0351/0570] Loss: 0.22448  Avg Loss: 0.18981  Avg mIoU:  69.08  
[Epoch: 87] [Batch: 0401/0570] Loss: 0.20231  Avg Loss: 0.18813  Avg mIoU:  69.08  
[Epoch: 87] [Batch: 0451/0570] Loss: 0.16997  Avg Loss: 0.18757  Avg mIoU:  69.26  
[Epoch: 87] [Batch: 0501/0570] Loss: 0.20093  Avg Loss: 0.18771  Avg mIoU:  69.07  
[Epoch: 87] [Batch: 0551/0570] Loss: 0.17387  Avg Loss: 0.18695  Avg mIoU:  69.35  

*** Training [@Epoch 87] Avg Loss: 0.18689  Avg mIoU:  69.35  ***

[Epoch: 87] [Batch: 0001/0050] Loss: 0.14702  Avg Loss: 0.14702  Avg mIoU:  64.45  

*** Validation [@Epoch 87] Avg Loss: 0.15877  Avg mIoU:  65.06  ***

[Epoch: 88] [Batch: 0001/0570] Loss: 0.20652  Avg Loss: 0.20652  Avg mIoU:  40.31  
[Epoch: 88] [Batch: 0051/0570] Loss: 0.21919  Avg Loss: 0.19037  Avg mIoU:  70.15  
[Epoch: 88] [Batch: 0101/0570] Loss: 0.24973  Avg Loss: 0.18799  Avg mIoU:  69.70  
[Epoch: 88] [Batch: 0151/0570] Loss: 0.17812  Avg Loss: 0.18507  Avg mIoU:  69.91  
[Epoch: 88] [Batch: 0201/0570] Loss: 0.16178  Avg Loss: 0.18741  Avg mIoU:  69.59  
[Epoch: 88] [Batch: 0251/0570] Loss: 0.18618  Avg Loss: 0.18654  Avg mIoU:  69.67  
[Epoch: 88] [Batch: 0301/0570] Loss: 0.18935  Avg Loss: 0.18750  Avg mIoU:  69.60  
[Epoch: 88] [Batch: 0351/0570] Loss: 0.16720  Avg Loss: 0.18748  Avg mIoU:  69.52  
[Epoch: 88] [Batch: 0401/0570] Loss: 0.19758  Avg Loss: 0.18771  Avg mIoU:  69.50  
[Epoch: 88] [Batch: 0451/0570] Loss: 0.22513  Avg Loss: 0.18757  Avg mIoU:  69.50  
[Epoch: 88] [Batch: 0501/0570] Loss: 0.20269  Avg Loss: 0.18642  Avg mIoU:  69.76  
[Epoch: 88] [Batch: 0551/0570] Loss: 0.20379  Avg Loss: 0.18717  Avg mIoU:  69.65  

*** Training [@Epoch 88] Avg Loss: 0.18686  Avg mIoU:  69.72  ***

[Epoch: 88] [Batch: 0001/0050] Loss: 0.14784  Avg Loss: 0.14784  Avg mIoU:  60.82  

*** Validation [@Epoch 88] Avg Loss: 0.15442  Avg mIoU:  65.56  ***

[Epoch: 89] [Batch: 0001/0570] Loss: 0.15500  Avg Loss: 0.15500  Avg mIoU:  30.20  
[Epoch: 89] [Batch: 0051/0570] Loss: 0.20873  Avg Loss: 0.18566  Avg mIoU:  70.23  
[Epoch: 89] [Batch: 0101/0570] Loss: 0.17037  Avg Loss: 0.18666  Avg mIoU:  70.12  
[Epoch: 89] [Batch: 0151/0570] Loss: 0.18112  Avg Loss: 0.18569  Avg mIoU:  70.21  
[Epoch: 89] [Batch: 0201/0570] Loss: 0.13286  Avg Loss: 0.18680  Avg mIoU:  70.08  
[Epoch: 89] [Batch: 0251/0570] Loss: 0.19593  Avg Loss: 0.18653  Avg mIoU:  69.91  
[Epoch: 89] [Batch: 0301/0570] Loss: 0.15296  Avg Loss: 0.18762  Avg mIoU:  69.69  
[Epoch: 89] [Batch: 0351/0570] Loss: 0.18526  Avg Loss: 0.18759  Avg mIoU:  69.61  
[Epoch: 89] [Batch: 0401/0570] Loss: 0.17278  Avg Loss: 0.18726  Avg mIoU:  70.02  
[Epoch: 89] [Batch: 0451/0570] Loss: 0.13505  Avg Loss: 0.18604  Avg mIoU:  70.08  
[Epoch: 89] [Batch: 0501/0570] Loss: 0.15334  Avg Loss: 0.18556  Avg mIoU:  70.08  
[Epoch: 89] [Batch: 0551/0570] Loss: 0.18211  Avg Loss: 0.18634  Avg mIoU:  69.88  

*** Training [@Epoch 89] Avg Loss: 0.18704  Avg mIoU:  69.79  ***

[Epoch: 89] [Batch: 0001/0050] Loss: 0.15064  Avg Loss: 0.15064  Avg mIoU:  61.75  

*** Validation [@Epoch 89] Avg Loss: 0.17507  Avg mIoU:  63.42  ***

[Epoch: 90] [Batch: 0001/0570] Loss: 0.14438  Avg Loss: 0.14438  Avg mIoU:  37.71  
[Epoch: 90] [Batch: 0051/0570] Loss: 0.15806  Avg Loss: 0.18723  Avg mIoU:  69.65  
[Epoch: 90] [Batch: 0101/0570] Loss: 0.20467  Avg Loss: 0.18725  Avg mIoU:  70.55  
[Epoch: 90] [Batch: 0151/0570] Loss: 0.28066  Avg Loss: 0.18667  Avg mIoU:  70.50  
[Epoch: 90] [Batch: 0201/0570] Loss: 0.14497  Avg Loss: 0.18583  Avg mIoU:  70.46  
[Epoch: 90] [Batch: 0251/0570] Loss: 0.18169  Avg Loss: 0.18497  Avg mIoU:  70.31  
[Epoch: 90] [Batch: 0301/0570] Loss: 0.18983  Avg Loss: 0.18661  Avg mIoU:  70.21  
[Epoch: 90] [Batch: 0351/0570] Loss: 0.26043  Avg Loss: 0.18751  Avg mIoU:  69.86  
[Epoch: 90] [Batch: 0401/0570] Loss: 0.13504  Avg Loss: 0.18765  Avg mIoU:  69.76  
[Epoch: 90] [Batch: 0451/0570] Loss: 0.16338  Avg Loss: 0.18746  Avg mIoU:  69.64  
[Epoch: 90] [Batch: 0501/0570] Loss: 0.18145  Avg Loss: 0.18703  Avg mIoU:  69.73  
[Epoch: 90] [Batch: 0551/0570] Loss: 0.15725  Avg Loss: 0.18587  Avg mIoU:  69.75  

*** Training [@Epoch 90] Avg Loss: 0.18597  Avg mIoU:  69.63  ***

[Epoch: 90] [Batch: 0001/0050] Loss: 0.12977  Avg Loss: 0.12977  Avg mIoU:  62.67  

*** Validation [@Epoch 90] Avg Loss: 0.15005  Avg mIoU:  64.38  ***

[Epoch: 91] [Batch: 0001/0570] Loss: 0.26747  Avg Loss: 0.26747  Avg mIoU:  46.86  
[Epoch: 91] [Batch: 0051/0570] Loss: 0.13775  Avg Loss: 0.19073  Avg mIoU:  70.53  
[Epoch: 91] [Batch: 0101/0570] Loss: 0.13530  Avg Loss: 0.18543  Avg mIoU:  70.26  
[Epoch: 91] [Batch: 0151/0570] Loss: 0.18099  Avg Loss: 0.18478  Avg mIoU:  70.38  
[Epoch: 91] [Batch: 0201/0570] Loss: 0.14656  Avg Loss: 0.18532  Avg mIoU:  70.32  
[Epoch: 91] [Batch: 0251/0570] Loss: 0.24782  Avg Loss: 0.18520  Avg mIoU:  70.30  
[Epoch: 91] [Batch: 0301/0570] Loss: 0.17390  Avg Loss: 0.18536  Avg mIoU:  69.86  
[Epoch: 91] [Batch: 0351/0570] Loss: 0.18953  Avg Loss: 0.18318  Avg mIoU:  70.24  
[Epoch: 91] [Batch: 0401/0570] Loss: 0.18525  Avg Loss: 0.18486  Avg mIoU:  70.12  
[Epoch: 91] [Batch: 0451/0570] Loss: 0.14732  Avg Loss: 0.18530  Avg mIoU:  70.10  
[Epoch: 91] [Batch: 0501/0570] Loss: 0.21931  Avg Loss: 0.18531  Avg mIoU:  70.11  
[Epoch: 91] [Batch: 0551/0570] Loss: 0.25128  Avg Loss: 0.18578  Avg mIoU:  69.91  

*** Training [@Epoch 91] Avg Loss: 0.18577  Avg mIoU:  69.96  ***

[Epoch: 91] [Batch: 0001/0050] Loss: 0.12726  Avg Loss: 0.12726  Avg mIoU:  63.20  

*** Validation [@Epoch 91] Avg Loss: 0.14909  Avg mIoU:  64.86  ***

[Epoch: 92] [Batch: 0001/0570] Loss: 0.14937  Avg Loss: 0.14937  Avg mIoU:  33.72  
[Epoch: 92] [Batch: 0051/0570] Loss: 0.12537  Avg Loss: 0.18698  Avg mIoU:  70.37  
[Epoch: 92] [Batch: 0101/0570] Loss: 0.17120  Avg Loss: 0.18478  Avg mIoU:  70.40  
[Epoch: 92] [Batch: 0151/0570] Loss: 0.15466  Avg Loss: 0.18207  Avg mIoU:  70.37  
[Epoch: 92] [Batch: 0201/0570] Loss: 0.27242  Avg Loss: 0.18442  Avg mIoU:  69.84  
[Epoch: 92] [Batch: 0251/0570] Loss: 0.13568  Avg Loss: 0.18563  Avg mIoU:  69.70  
[Epoch: 92] [Batch: 0301/0570] Loss: 0.17823  Avg Loss: 0.18538  Avg mIoU:  70.21  
[Epoch: 92] [Batch: 0351/0570] Loss: 0.17373  Avg Loss: 0.18760  Avg mIoU:  69.94  
[Epoch: 92] [Batch: 0401/0570] Loss: 0.20145  Avg Loss: 0.18708  Avg mIoU:  70.08  
[Epoch: 92] [Batch: 0451/0570] Loss: 0.12005  Avg Loss: 0.18622  Avg mIoU:  69.95  
[Epoch: 92] [Batch: 0501/0570] Loss: 0.14427  Avg Loss: 0.18542  Avg mIoU:  69.92  
[Epoch: 92] [Batch: 0551/0570] Loss: 0.16260  Avg Loss: 0.18458  Avg mIoU:  70.06  

*** Training [@Epoch 92] Avg Loss: 0.18463  Avg mIoU:  70.03  ***

[Epoch: 92] [Batch: 0001/0050] Loss: 0.15470  Avg Loss: 0.15470  Avg mIoU:  62.18  

*** Validation [@Epoch 92] Avg Loss: 0.17361  Avg mIoU:  64.09  ***

[Epoch: 93] [Batch: 0001/0570] Loss: 0.20122  Avg Loss: 0.20122  Avg mIoU:  38.26  
[Epoch: 93] [Batch: 0051/0570] Loss: 0.19538  Avg Loss: 0.17896  Avg mIoU:  68.61  
[Epoch: 93] [Batch: 0101/0570] Loss: 0.18811  Avg Loss: 0.18152  Avg mIoU:  69.74  
[Epoch: 93] [Batch: 0151/0570] Loss: 0.18847  Avg Loss: 0.18257  Avg mIoU:  70.42  
[Epoch: 93] [Batch: 0201/0570] Loss: 0.15199  Avg Loss: 0.18517  Avg mIoU:  69.88  
[Epoch: 93] [Batch: 0251/0570] Loss: 0.28828  Avg Loss: 0.18725  Avg mIoU:  69.43  
[Epoch: 93] [Batch: 0301/0570] Loss: 0.18114  Avg Loss: 0.18835  Avg mIoU:  69.34  
[Epoch: 93] [Batch: 0351/0570] Loss: 0.15543  Avg Loss: 0.18641  Avg mIoU:  69.48  
[Epoch: 93] [Batch: 0401/0570] Loss: 0.19463  Avg Loss: 0.18723  Avg mIoU:  69.41  
[Epoch: 93] [Batch: 0451/0570] Loss: 0.21659  Avg Loss: 0.18694  Avg mIoU:  69.26  
[Epoch: 93] [Batch: 0501/0570] Loss: 0.25363  Avg Loss: 0.18805  Avg mIoU:  69.20  
[Epoch: 93] [Batch: 0551/0570] Loss: 0.26742  Avg Loss: 0.18727  Avg mIoU:  69.33  

*** Training [@Epoch 93] Avg Loss: 0.18699  Avg mIoU:  69.43  ***

[Epoch: 93] [Batch: 0001/0050] Loss: 0.14097  Avg Loss: 0.14097  Avg mIoU:  65.93  

*** Validation [@Epoch 93] Avg Loss: 0.16097  Avg mIoU:  64.66  ***

[Epoch: 94] [Batch: 0001/0570] Loss: 0.16408  Avg Loss: 0.16408  Avg mIoU:  33.69  
[Epoch: 94] [Batch: 0051/0570] Loss: 0.14947  Avg Loss: 0.18056  Avg mIoU:  70.70  
[Epoch: 94] [Batch: 0101/0570] Loss: 0.24237  Avg Loss: 0.18664  Avg mIoU:  70.55  
[Epoch: 94] [Batch: 0151/0570] Loss: 0.20501  Avg Loss: 0.18340  Avg mIoU:  70.96  
[Epoch: 94] [Batch: 0201/0570] Loss: 0.18589  Avg Loss: 0.18348  Avg mIoU:  70.67  
[Epoch: 94] [Batch: 0251/0570] Loss: 0.14279  Avg Loss: 0.18359  Avg mIoU:  70.93  
[Epoch: 94] [Batch: 0301/0570] Loss: 0.20484  Avg Loss: 0.18388  Avg mIoU:  70.71  
[Epoch: 94] [Batch: 0351/0570] Loss: 0.27368  Avg Loss: 0.18424  Avg mIoU:  70.42  
[Epoch: 94] [Batch: 0401/0570] Loss: 0.23898  Avg Loss: 0.18271  Avg mIoU:  70.32  
[Epoch: 94] [Batch: 0451/0570] Loss: 0.13525  Avg Loss: 0.18314  Avg mIoU:  70.50  
[Epoch: 94] [Batch: 0501/0570] Loss: 0.16858  Avg Loss: 0.18362  Avg mIoU:  70.50  
[Epoch: 94] [Batch: 0551/0570] Loss: 0.11913  Avg Loss: 0.18270  Avg mIoU:  70.59  

*** Training [@Epoch 94] Avg Loss: 0.18291  Avg mIoU:  70.51  ***

[Epoch: 94] [Batch: 0001/0050] Loss: 0.13978  Avg Loss: 0.13978  Avg mIoU:  64.53  

*** Validation [@Epoch 94] Avg Loss: 0.15923  Avg mIoU:  65.16  ***

[Epoch: 95] [Batch: 0001/0570] Loss: 0.23817  Avg Loss: 0.23817  Avg mIoU:  56.63  
[Epoch: 95] [Batch: 0051/0570] Loss: 0.17117  Avg Loss: 0.18694  Avg mIoU:  69.54  
[Epoch: 95] [Batch: 0101/0570] Loss: 0.16549  Avg Loss: 0.18951  Avg mIoU:  68.96  
[Epoch: 95] [Batch: 0151/0570] Loss: 0.15298  Avg Loss: 0.18761  Avg mIoU:  69.63  
[Epoch: 95] [Batch: 0201/0570] Loss: 0.17405  Avg Loss: 0.18580  Avg mIoU:  69.75  
[Epoch: 95] [Batch: 0251/0570] Loss: 0.17152  Avg Loss: 0.18638  Avg mIoU:  69.68  
[Epoch: 95] [Batch: 0301/0570] Loss: 0.17196  Avg Loss: 0.18398  Avg mIoU:  69.94  
[Epoch: 95] [Batch: 0351/0570] Loss: 0.19036  Avg Loss: 0.18376  Avg mIoU:  69.90  
[Epoch: 95] [Batch: 0401/0570] Loss: 0.11707  Avg Loss: 0.18323  Avg mIoU:  69.81  
[Epoch: 95] [Batch: 0451/0570] Loss: 0.25215  Avg Loss: 0.18369  Avg mIoU:  69.80  
[Epoch: 95] [Batch: 0501/0570] Loss: 0.23219  Avg Loss: 0.18265  Avg mIoU:  69.93  
[Epoch: 95] [Batch: 0551/0570] Loss: 0.15699  Avg Loss: 0.18256  Avg mIoU:  69.90  

*** Training [@Epoch 95] Avg Loss: 0.18271  Avg mIoU:  69.92  ***

[Epoch: 95] [Batch: 0001/0050] Loss: 0.14470  Avg Loss: 0.14470  Avg mIoU:  63.47  

*** Validation [@Epoch 95] Avg Loss: 0.16117  Avg mIoU:  65.40  ***

[Epoch: 96] [Batch: 0001/0570] Loss: 0.14851  Avg Loss: 0.14851  Avg mIoU:  46.95  
[Epoch: 96] [Batch: 0051/0570] Loss: 0.17072  Avg Loss: 0.18868  Avg mIoU:  70.84  
[Epoch: 96] [Batch: 0101/0570] Loss: 0.18547  Avg Loss: 0.18907  Avg mIoU:  69.74  
[Epoch: 96] [Batch: 0151/0570] Loss: 0.19542  Avg Loss: 0.18844  Avg mIoU:  69.53  
[Epoch: 96] [Batch: 0201/0570] Loss: 0.15215  Avg Loss: 0.18645  Avg mIoU:  70.04  
[Epoch: 96] [Batch: 0251/0570] Loss: 0.20715  Avg Loss: 0.18577  Avg mIoU:  70.11  
[Epoch: 96] [Batch: 0301/0570] Loss: 0.13543  Avg Loss: 0.18406  Avg mIoU:  70.43  
[Epoch: 96] [Batch: 0351/0570] Loss: 0.16052  Avg Loss: 0.18397  Avg mIoU:  70.29  
[Epoch: 96] [Batch: 0401/0570] Loss: 0.21631  Avg Loss: 0.18374  Avg mIoU:  70.34  
[Epoch: 96] [Batch: 0451/0570] Loss: 0.22129  Avg Loss: 0.18474  Avg mIoU:  70.39  
[Epoch: 96] [Batch: 0501/0570] Loss: 0.21453  Avg Loss: 0.18489  Avg mIoU:  70.34  
[Epoch: 96] [Batch: 0551/0570] Loss: 0.18345  Avg Loss: 0.18453  Avg mIoU:  70.28  

*** Training [@Epoch 96] Avg Loss: 0.18472  Avg mIoU:  70.17  ***

[Epoch: 96] [Batch: 0001/0050] Loss: 0.13636  Avg Loss: 0.13636  Avg mIoU:  64.11  

*** Validation [@Epoch 96] Avg Loss: 0.15417  Avg mIoU:  64.97  ***

[Epoch: 97] [Batch: 0001/0570] Loss: 0.20145  Avg Loss: 0.20145  Avg mIoU:  37.62  
[Epoch: 97] [Batch: 0051/0570] Loss: 0.23758  Avg Loss: 0.18310  Avg mIoU:  68.49  
[Epoch: 97] [Batch: 0101/0570] Loss: 0.19129  Avg Loss: 0.18733  Avg mIoU:  69.60  
[Epoch: 97] [Batch: 0151/0570] Loss: 0.20805  Avg Loss: 0.18318  Avg mIoU:  69.70  
[Epoch: 97] [Batch: 0201/0570] Loss: 0.18622  Avg Loss: 0.18087  Avg mIoU:  70.37  
[Epoch: 97] [Batch: 0251/0570] Loss: 0.17334  Avg Loss: 0.18132  Avg mIoU:  70.39  
[Epoch: 97] [Batch: 0301/0570] Loss: 0.18766  Avg Loss: 0.18081  Avg mIoU:  70.40  
[Epoch: 97] [Batch: 0351/0570] Loss: 0.25253  Avg Loss: 0.17971  Avg mIoU:  70.60  
[Epoch: 97] [Batch: 0401/0570] Loss: 0.22139  Avg Loss: 0.18006  Avg mIoU:  70.48  
[Epoch: 97] [Batch: 0451/0570] Loss: 0.14445  Avg Loss: 0.18316  Avg mIoU:  70.28  
[Epoch: 97] [Batch: 0501/0570] Loss: 0.22051  Avg Loss: 0.18338  Avg mIoU:  70.16  
[Epoch: 97] [Batch: 0551/0570] Loss: 0.18220  Avg Loss: 0.18313  Avg mIoU:  70.07  

*** Training [@Epoch 97] Avg Loss: 0.18281  Avg mIoU:  70.04  ***

[Epoch: 97] [Batch: 0001/0050] Loss: 0.14575  Avg Loss: 0.14575  Avg mIoU:  64.19  

*** Validation [@Epoch 97] Avg Loss: 0.16417  Avg mIoU:  65.24  ***

[Epoch: 98] [Batch: 0001/0570] Loss: 0.24258  Avg Loss: 0.24258  Avg mIoU:  52.85  
[Epoch: 98] [Batch: 0051/0570] Loss: 0.15158  Avg Loss: 0.17716  Avg mIoU:  70.03  
[Epoch: 98] [Batch: 0101/0570] Loss: 0.13665  Avg Loss: 0.17714  Avg mIoU:  70.01  
[Epoch: 98] [Batch: 0151/0570] Loss: 0.18579  Avg Loss: 0.17653  Avg mIoU:  70.26  
[Epoch: 98] [Batch: 0201/0570] Loss: 0.20136  Avg Loss: 0.17625  Avg mIoU:  70.77  
[Epoch: 98] [Batch: 0251/0570] Loss: 0.16316  Avg Loss: 0.17633  Avg mIoU:  70.82  
[Epoch: 98] [Batch: 0301/0570] Loss: 0.16307  Avg Loss: 0.17856  Avg mIoU:  70.96  
[Epoch: 98] [Batch: 0351/0570] Loss: 0.14417  Avg Loss: 0.17954  Avg mIoU:  71.03  
[Epoch: 98] [Batch: 0401/0570] Loss: 0.12580  Avg Loss: 0.18065  Avg mIoU:  70.82  
[Epoch: 98] [Batch: 0451/0570] Loss: 0.20099  Avg Loss: 0.18130  Avg mIoU:  70.58  
[Epoch: 98] [Batch: 0501/0570] Loss: 0.23674  Avg Loss: 0.18184  Avg mIoU:  70.52  
[Epoch: 98] [Batch: 0551/0570] Loss: 0.17727  Avg Loss: 0.18203  Avg mIoU:  70.55  

*** Training [@Epoch 98] Avg Loss: 0.18239  Avg mIoU:  70.49  ***

[Epoch: 98] [Batch: 0001/0050] Loss: 0.16197  Avg Loss: 0.16197  Avg mIoU:  61.49  

*** Validation [@Epoch 98] Avg Loss: 0.17652  Avg mIoU:  64.20  ***

[Epoch: 99] [Batch: 0001/0570] Loss: 0.19057  Avg Loss: 0.19057  Avg mIoU:  28.84  
[Epoch: 99] [Batch: 0051/0570] Loss: 0.20273  Avg Loss: 0.17443  Avg mIoU:  70.17  
[Epoch: 99] [Batch: 0101/0570] Loss: 0.25728  Avg Loss: 0.17728  Avg mIoU:  68.74  
[Epoch: 99] [Batch: 0151/0570] Loss: 0.18018  Avg Loss: 0.17883  Avg mIoU:  68.97  
[Epoch: 99] [Batch: 0201/0570] Loss: 0.11379  Avg Loss: 0.18117  Avg mIoU:  69.53  
[Epoch: 99] [Batch: 0251/0570] Loss: 0.15661  Avg Loss: 0.18053  Avg mIoU:  69.59  
[Epoch: 99] [Batch: 0301/0570] Loss: 0.20390  Avg Loss: 0.17967  Avg mIoU:  70.17  
[Epoch: 99] [Batch: 0351/0570] Loss: 0.09968  Avg Loss: 0.17970  Avg mIoU:  70.35  
[Epoch: 99] [Batch: 0401/0570] Loss: 0.18825  Avg Loss: 0.18022  Avg mIoU:  70.45  
[Epoch: 99] [Batch: 0451/0570] Loss: 0.16883  Avg Loss: 0.18161  Avg mIoU:  70.34  
[Epoch: 99] [Batch: 0501/0570] Loss: 0.18308  Avg Loss: 0.18208  Avg mIoU:  70.29  
[Epoch: 99] [Batch: 0551/0570] Loss: 0.13859  Avg Loss: 0.18235  Avg mIoU:  70.32  

*** Training [@Epoch 99] Avg Loss: 0.18267  Avg mIoU:  70.25  ***

[Epoch: 99] [Batch: 0001/0050] Loss: 0.14251  Avg Loss: 0.14251  Avg mIoU:  64.45  

*** Validation [@Epoch 99] Avg Loss: 0.15549  Avg mIoU:  64.63  ***

[Epoch: 100] [Batch: 0001/0570] Loss: 0.11548  Avg Loss: 0.11548  Avg mIoU:  50.97  
[Epoch: 100] [Batch: 0051/0570] Loss: 0.11943  Avg Loss: 0.18326  Avg mIoU:  70.55  
[Epoch: 100] [Batch: 0101/0570] Loss: 0.13911  Avg Loss: 0.18509  Avg mIoU:  70.18  
[Epoch: 100] [Batch: 0151/0570] Loss: 0.19104  Avg Loss: 0.18248  Avg mIoU:  69.89  
[Epoch: 100] [Batch: 0201/0570] Loss: 0.25030  Avg Loss: 0.18564  Avg mIoU:  69.81  
[Epoch: 100] [Batch: 0251/0570] Loss: 0.16155  Avg Loss: 0.18386  Avg mIoU:  69.95  
[Epoch: 100] [Batch: 0301/0570] Loss: 0.20340  Avg Loss: 0.18334  Avg mIoU:  70.51  
[Epoch: 100] [Batch: 0351/0570] Loss: 0.18627  Avg Loss: 0.18306  Avg mIoU:  70.44  
[Epoch: 100] [Batch: 0401/0570] Loss: 0.17240  Avg Loss: 0.18253  Avg mIoU:  70.34  
[Epoch: 100] [Batch: 0451/0570] Loss: 0.18468  Avg Loss: 0.18275  Avg mIoU:  70.27  
[Epoch: 100] [Batch: 0501/0570] Loss: 0.19688  Avg Loss: 0.18281  Avg mIoU:  70.27  
[Epoch: 100] [Batch: 0551/0570] Loss: 0.18746  Avg Loss: 0.18283  Avg mIoU:  70.16  

*** Training [@Epoch 100] Avg Loss: 0.18225  Avg mIoU:  70.20  ***

[Epoch: 100] [Batch: 0001/0050] Loss: 0.13504  Avg Loss: 0.13504  Avg mIoU:  64.71  

*** Validation [@Epoch 100] Avg Loss: 0.17705  Avg mIoU:  65.24  ***

[Epoch: 101] [Batch: 0001/0570] Loss: 0.14496  Avg Loss: 0.14496  Avg mIoU:  43.61  
[Epoch: 101] [Batch: 0051/0570] Loss: 0.17120  Avg Loss: 0.18399  Avg mIoU:  70.63  
[Epoch: 101] [Batch: 0101/0570] Loss: 0.17648  Avg Loss: 0.17931  Avg mIoU:  71.21  
[Epoch: 101] [Batch: 0151/0570] Loss: 0.19761  Avg Loss: 0.17973  Avg mIoU:  71.16  
[Epoch: 101] [Batch: 0201/0570] Loss: 0.18547  Avg Loss: 0.18072  Avg mIoU:  70.96  
[Epoch: 101] [Batch: 0251/0570] Loss: 0.26891  Avg Loss: 0.18041  Avg mIoU:  70.65  
[Epoch: 101] [Batch: 0301/0570] Loss: 0.20149  Avg Loss: 0.18103  Avg mIoU:  70.57  
[Epoch: 101] [Batch: 0351/0570] Loss: 0.15315  Avg Loss: 0.18063  Avg mIoU:  70.66  
[Epoch: 101] [Batch: 0401/0570] Loss: 0.14967  Avg Loss: 0.18138  Avg mIoU:  70.43  
[Epoch: 101] [Batch: 0451/0570] Loss: 0.14784  Avg Loss: 0.18218  Avg mIoU:  70.49  
[Epoch: 101] [Batch: 0501/0570] Loss: 0.15972  Avg Loss: 0.18234  Avg mIoU:  70.44  
[Epoch: 101] [Batch: 0551/0570] Loss: 0.23144  Avg Loss: 0.18258  Avg mIoU:  70.44  

*** Training [@Epoch 101] Avg Loss: 0.18232  Avg mIoU:  70.34  ***

[Epoch: 101] [Batch: 0001/0050] Loss: 0.13084  Avg Loss: 0.13084  Avg mIoU:  64.35  

*** Validation [@Epoch 101] Avg Loss: 0.15561  Avg mIoU:  66.19  ***

[Epoch: 102] [Batch: 0001/0570] Loss: 0.24008  Avg Loss: 0.24008  Avg mIoU:  36.24  
[Epoch: 102] [Batch: 0051/0570] Loss: 0.13326  Avg Loss: 0.17339  Avg mIoU:  71.97  
[Epoch: 102] [Batch: 0101/0570] Loss: 0.17612  Avg Loss: 0.17826  Avg mIoU:  71.93  
[Epoch: 102] [Batch: 0151/0570] Loss: 0.12709  Avg Loss: 0.17725  Avg mIoU:  71.97  
[Epoch: 102] [Batch: 0201/0570] Loss: 0.21759  Avg Loss: 0.17766  Avg mIoU:  71.78  
[Epoch: 102] [Batch: 0251/0570] Loss: 0.16733  Avg Loss: 0.17821  Avg mIoU:  71.54  
[Epoch: 102] [Batch: 0301/0570] Loss: 0.18473  Avg Loss: 0.18007  Avg mIoU:  71.46  
[Epoch: 102] [Batch: 0351/0570] Loss: 0.23085  Avg Loss: 0.17881  Avg mIoU:  71.42  
[Epoch: 102] [Batch: 0401/0570] Loss: 0.17751  Avg Loss: 0.17881  Avg mIoU:  71.20  
[Epoch: 102] [Batch: 0451/0570] Loss: 0.20548  Avg Loss: 0.17991  Avg mIoU:  71.05  
[Epoch: 102] [Batch: 0501/0570] Loss: 0.14829  Avg Loss: 0.18042  Avg mIoU:  70.71  
[Epoch: 102] [Batch: 0551/0570] Loss: 0.12696  Avg Loss: 0.17998  Avg mIoU:  70.65  

*** Training [@Epoch 102] Avg Loss: 0.18127  Avg mIoU:  70.46  ***

[Epoch: 102] [Batch: 0001/0050] Loss: 0.14517  Avg Loss: 0.14517  Avg mIoU:  63.97  

*** Validation [@Epoch 102] Avg Loss: 0.15559  Avg mIoU:  63.36  ***

[Epoch: 103] [Batch: 0001/0570] Loss: 0.22109  Avg Loss: 0.22109  Avg mIoU:  41.51  
[Epoch: 103] [Batch: 0051/0570] Loss: 0.21523  Avg Loss: 0.17910  Avg mIoU:  70.26  
[Epoch: 103] [Batch: 0101/0570] Loss: 0.17831  Avg Loss: 0.18063  Avg mIoU:  71.32  
[Epoch: 103] [Batch: 0151/0570] Loss: 0.31532  Avg Loss: 0.18221  Avg mIoU:  70.29  
[Epoch: 103] [Batch: 0201/0570] Loss: 0.27953  Avg Loss: 0.18280  Avg mIoU:  70.08  
[Epoch: 103] [Batch: 0251/0570] Loss: 0.20635  Avg Loss: 0.18441  Avg mIoU:  70.09  
[Epoch: 103] [Batch: 0301/0570] Loss: 0.11437  Avg Loss: 0.18176  Avg mIoU:  70.38  
[Epoch: 103] [Batch: 0351/0570] Loss: 0.14887  Avg Loss: 0.18289  Avg mIoU:  70.23  
[Epoch: 103] [Batch: 0401/0570] Loss: 0.23257  Avg Loss: 0.18234  Avg mIoU:  70.16  
[Epoch: 103] [Batch: 0451/0570] Loss: 0.15769  Avg Loss: 0.18112  Avg mIoU:  70.36  
[Epoch: 103] [Batch: 0501/0570] Loss: 0.32503  Avg Loss: 0.18179  Avg mIoU:  70.28  
[Epoch: 103] [Batch: 0551/0570] Loss: 0.21872  Avg Loss: 0.18156  Avg mIoU:  70.24  

*** Training [@Epoch 103] Avg Loss: 0.18176  Avg mIoU:  70.32  ***

[Epoch: 103] [Batch: 0001/0050] Loss: 0.13683  Avg Loss: 0.13683  Avg mIoU:  63.33  

*** Validation [@Epoch 103] Avg Loss: 0.16315  Avg mIoU:  65.09  ***

[Epoch: 104] [Batch: 0001/0570] Loss: 0.15353  Avg Loss: 0.15353  Avg mIoU:  49.55  
[Epoch: 104] [Batch: 0051/0570] Loss: 0.16109  Avg Loss: 0.17116  Avg mIoU:  70.53  
[Epoch: 104] [Batch: 0101/0570] Loss: 0.16662  Avg Loss: 0.17601  Avg mIoU:  70.66  
[Epoch: 104] [Batch: 0151/0570] Loss: 0.20502  Avg Loss: 0.17432  Avg mIoU:  71.05  
[Epoch: 104] [Batch: 0201/0570] Loss: 0.25023  Avg Loss: 0.17635  Avg mIoU:  70.89  
[Epoch: 104] [Batch: 0251/0570] Loss: 0.20594  Avg Loss: 0.17810  Avg mIoU:  70.71  
[Epoch: 104] [Batch: 0301/0570] Loss: 0.11183  Avg Loss: 0.17898  Avg mIoU:  70.48  
[Epoch: 104] [Batch: 0351/0570] Loss: 0.14737  Avg Loss: 0.18194  Avg mIoU:  70.34  
[Epoch: 104] [Batch: 0401/0570] Loss: 0.17043  Avg Loss: 0.18218  Avg mIoU:  69.96  
[Epoch: 104] [Batch: 0451/0570] Loss: 0.23064  Avg Loss: 0.18243  Avg mIoU:  70.03  
[Epoch: 104] [Batch: 0501/0570] Loss: 0.13233  Avg Loss: 0.18256  Avg mIoU:  70.24  
[Epoch: 104] [Batch: 0551/0570] Loss: 0.16361  Avg Loss: 0.18285  Avg mIoU:  70.28  

*** Training [@Epoch 104] Avg Loss: 0.18345  Avg mIoU:  70.20  ***

[Epoch: 104] [Batch: 0001/0050] Loss: 0.12574  Avg Loss: 0.12574  Avg mIoU:  65.18  

*** Validation [@Epoch 104] Avg Loss: 0.15799  Avg mIoU:  64.41  ***

[Epoch: 105] [Batch: 0001/0570] Loss: 0.21879  Avg Loss: 0.21879  Avg mIoU:  37.44  
[Epoch: 105] [Batch: 0051/0570] Loss: 0.24227  Avg Loss: 0.17190  Avg mIoU:  72.63  
[Epoch: 105] [Batch: 0101/0570] Loss: 0.15734  Avg Loss: 0.17512  Avg mIoU:  73.28  
[Epoch: 105] [Batch: 0151/0570] Loss: 0.15178  Avg Loss: 0.17849  Avg mIoU:  71.94  
[Epoch: 105] [Batch: 0201/0570] Loss: 0.19533  Avg Loss: 0.17870  Avg mIoU:  71.62  
[Epoch: 105] [Batch: 0251/0570] Loss: 0.23536  Avg Loss: 0.17853  Avg mIoU:  71.49  
[Epoch: 105] [Batch: 0301/0570] Loss: 0.27015  Avg Loss: 0.17901  Avg mIoU:  71.09  
[Epoch: 105] [Batch: 0351/0570] Loss: 0.25935  Avg Loss: 0.18076  Avg mIoU:  70.76  
[Epoch: 105] [Batch: 0401/0570] Loss: 0.12995  Avg Loss: 0.18152  Avg mIoU:  70.86  
[Epoch: 105] [Batch: 0451/0570] Loss: 0.12860  Avg Loss: 0.18188  Avg mIoU:  70.79  
[Epoch: 105] [Batch: 0501/0570] Loss: 0.25415  Avg Loss: 0.18189  Avg mIoU:  70.79  
[Epoch: 105] [Batch: 0551/0570] Loss: 0.21961  Avg Loss: 0.18200  Avg mIoU:  70.83  

*** Training [@Epoch 105] Avg Loss: 0.18207  Avg mIoU:  70.81  ***

[Epoch: 105] [Batch: 0001/0050] Loss: 0.13330  Avg Loss: 0.13330  Avg mIoU:  64.51  

*** Validation [@Epoch 105] Avg Loss: 0.16241  Avg mIoU:  64.20  ***

[Epoch: 106] [Batch: 0001/0570] Loss: 0.17865  Avg Loss: 0.17865  Avg mIoU:  32.72  
[Epoch: 106] [Batch: 0051/0570] Loss: 0.25624  Avg Loss: 0.17948  Avg mIoU:  72.31  
[Epoch: 106] [Batch: 0101/0570] Loss: 0.20115  Avg Loss: 0.17445  Avg mIoU:  71.88  
[Epoch: 106] [Batch: 0151/0570] Loss: 0.21450  Avg Loss: 0.17475  Avg mIoU:  72.48  
[Epoch: 106] [Batch: 0201/0570] Loss: 0.14608  Avg Loss: 0.17420  Avg mIoU:  72.06  
[Epoch: 106] [Batch: 0251/0570] Loss: 0.17220  Avg Loss: 0.17557  Avg mIoU:  71.72  
[Epoch: 106] [Batch: 0301/0570] Loss: 0.14778  Avg Loss: 0.17614  Avg mIoU:  71.54  
[Epoch: 106] [Batch: 0351/0570] Loss: 0.20060  Avg Loss: 0.17633  Avg mIoU:  71.53  
[Epoch: 106] [Batch: 0401/0570] Loss: 0.18726  Avg Loss: 0.17861  Avg mIoU:  71.28  
[Epoch: 106] [Batch: 0451/0570] Loss: 0.28998  Avg Loss: 0.17865  Avg mIoU:  71.43  
[Epoch: 106] [Batch: 0501/0570] Loss: 0.17911  Avg Loss: 0.17925  Avg mIoU:  71.33  
[Epoch: 106] [Batch: 0551/0570] Loss: 0.20299  Avg Loss: 0.17897  Avg mIoU:  71.19  

*** Training [@Epoch 106] Avg Loss: 0.17878  Avg mIoU:  71.17  ***

[Epoch: 106] [Batch: 0001/0050] Loss: 0.13514  Avg Loss: 0.13514  Avg mIoU:  64.04  

*** Validation [@Epoch 106] Avg Loss: 0.15245  Avg mIoU:  63.79  ***

[Epoch: 107] [Batch: 0001/0570] Loss: 0.19317  Avg Loss: 0.19317  Avg mIoU:  44.99  
[Epoch: 107] [Batch: 0051/0570] Loss: 0.22288  Avg Loss: 0.17138  Avg mIoU:  73.16  
[Epoch: 107] [Batch: 0101/0570] Loss: 0.16498  Avg Loss: 0.16957  Avg mIoU:  72.45  
[Epoch: 107] [Batch: 0151/0570] Loss: 0.25660  Avg Loss: 0.17652  Avg mIoU:  71.83  
[Epoch: 107] [Batch: 0201/0570] Loss: 0.15605  Avg Loss: 0.17647  Avg mIoU:  71.86  
[Epoch: 107] [Batch: 0251/0570] Loss: 0.14988  Avg Loss: 0.17675  Avg mIoU:  71.17  
[Epoch: 107] [Batch: 0301/0570] Loss: 0.19177  Avg Loss: 0.17824  Avg mIoU:  71.15  
[Epoch: 107] [Batch: 0351/0570] Loss: 0.18158  Avg Loss: 0.17899  Avg mIoU:  71.13  
[Epoch: 107] [Batch: 0401/0570] Loss: 0.13695  Avg Loss: 0.17952  Avg mIoU:  71.01  
[Epoch: 107] [Batch: 0451/0570] Loss: 0.25553  Avg Loss: 0.17975  Avg mIoU:  70.99  
[Epoch: 107] [Batch: 0501/0570] Loss: 0.20896  Avg Loss: 0.17987  Avg mIoU:  70.89  
[Epoch: 107] [Batch: 0551/0570] Loss: 0.16933  Avg Loss: 0.17955  Avg mIoU:  70.88  

*** Training [@Epoch 107] Avg Loss: 0.17968  Avg mIoU:  70.90  ***

[Epoch: 107] [Batch: 0001/0050] Loss: 0.16526  Avg Loss: 0.16526  Avg mIoU:  63.37  

*** Validation [@Epoch 107] Avg Loss: 0.17441  Avg mIoU:  64.74  ***

[Epoch: 108] [Batch: 0001/0570] Loss: 0.20770  Avg Loss: 0.20770  Avg mIoU:  40.35  
[Epoch: 108] [Batch: 0051/0570] Loss: 0.18613  Avg Loss: 0.18120  Avg mIoU:  70.02  
[Epoch: 108] [Batch: 0101/0570] Loss: 0.16626  Avg Loss: 0.18017  Avg mIoU:  70.37  
[Epoch: 108] [Batch: 0151/0570] Loss: 0.20714  Avg Loss: 0.18003  Avg mIoU:  70.66  
[Epoch: 108] [Batch: 0201/0570] Loss: 0.21725  Avg Loss: 0.17886  Avg mIoU:  70.79  
[Epoch: 108] [Batch: 0251/0570] Loss: 0.23649  Avg Loss: 0.17750  Avg mIoU:  71.10  
[Epoch: 108] [Batch: 0301/0570] Loss: 0.26704  Avg Loss: 0.17703  Avg mIoU:  71.51  
[Epoch: 108] [Batch: 0351/0570] Loss: 0.22210  Avg Loss: 0.17822  Avg mIoU:  71.17  
[Epoch: 108] [Batch: 0401/0570] Loss: 0.14408  Avg Loss: 0.17877  Avg mIoU:  71.21  
[Epoch: 108] [Batch: 0451/0570] Loss: 0.21664  Avg Loss: 0.17881  Avg mIoU:  71.13  
[Epoch: 108] [Batch: 0501/0570] Loss: 0.09777  Avg Loss: 0.17803  Avg mIoU:  71.07  
[Epoch: 108] [Batch: 0551/0570] Loss: 0.20119  Avg Loss: 0.17837  Avg mIoU:  71.02  

*** Training [@Epoch 108] Avg Loss: 0.17775  Avg mIoU:  71.06  ***

[Epoch: 108] [Batch: 0001/0050] Loss: 0.16338  Avg Loss: 0.16338  Avg mIoU:  63.59  

*** Validation [@Epoch 108] Avg Loss: 0.16927  Avg mIoU:  65.78  ***

[Epoch: 109] [Batch: 0001/0570] Loss: 0.19609  Avg Loss: 0.19609  Avg mIoU:  56.04  
[Epoch: 109] [Batch: 0051/0570] Loss: 0.32541  Avg Loss: 0.18058  Avg mIoU:  69.35  
[Epoch: 109] [Batch: 0101/0570] Loss: 0.21695  Avg Loss: 0.18215  Avg mIoU:  69.89  
[Epoch: 109] [Batch: 0151/0570] Loss: 0.21400  Avg Loss: 0.18452  Avg mIoU:  70.09  
[Epoch: 109] [Batch: 0201/0570] Loss: 0.17242  Avg Loss: 0.18189  Avg mIoU:  70.38  
[Epoch: 109] [Batch: 0251/0570] Loss: 0.23150  Avg Loss: 0.17961  Avg mIoU:  70.73  
[Epoch: 109] [Batch: 0301/0570] Loss: 0.15399  Avg Loss: 0.17929  Avg mIoU:  70.93  
[Epoch: 109] [Batch: 0351/0570] Loss: 0.16704  Avg Loss: 0.17912  Avg mIoU:  70.95  
[Epoch: 109] [Batch: 0401/0570] Loss: 0.12611  Avg Loss: 0.17985  Avg mIoU:  71.04  
[Epoch: 109] [Batch: 0451/0570] Loss: 0.11520  Avg Loss: 0.17922  Avg mIoU:  71.15  
[Epoch: 109] [Batch: 0501/0570] Loss: 0.14701  Avg Loss: 0.18023  Avg mIoU:  71.04  
[Epoch: 109] [Batch: 0551/0570] Loss: 0.12961  Avg Loss: 0.17980  Avg mIoU:  70.95  

*** Training [@Epoch 109] Avg Loss: 0.17921  Avg mIoU:  70.94  ***

[Epoch: 109] [Batch: 0001/0050] Loss: 0.13119  Avg Loss: 0.13119  Avg mIoU:  65.44  

*** Validation [@Epoch 109] Avg Loss: 0.15801  Avg mIoU:  64.59  ***

[Epoch: 110] [Batch: 0001/0570] Loss: 0.24324  Avg Loss: 0.24324  Avg mIoU:  44.79  
[Epoch: 110] [Batch: 0051/0570] Loss: 0.21875  Avg Loss: 0.17432  Avg mIoU:  70.64  
[Epoch: 110] [Batch: 0101/0570] Loss: 0.18779  Avg Loss: 0.17726  Avg mIoU:  71.46  
[Epoch: 110] [Batch: 0151/0570] Loss: 0.20368  Avg Loss: 0.18218  Avg mIoU:  70.60  
[Epoch: 110] [Batch: 0201/0570] Loss: 0.14618  Avg Loss: 0.18206  Avg mIoU:  70.52  
[Epoch: 110] [Batch: 0251/0570] Loss: 0.17945  Avg Loss: 0.18289  Avg mIoU:  70.80  
[Epoch: 110] [Batch: 0301/0570] Loss: 0.17872  Avg Loss: 0.18063  Avg mIoU:  71.09  
[Epoch: 110] [Batch: 0351/0570] Loss: 0.20736  Avg Loss: 0.17986  Avg mIoU:  70.99  
[Epoch: 110] [Batch: 0401/0570] Loss: 0.25673  Avg Loss: 0.17946  Avg mIoU:  70.89  
[Epoch: 110] [Batch: 0451/0570] Loss: 0.15422  Avg Loss: 0.17919  Avg mIoU:  70.95  
[Epoch: 110] [Batch: 0501/0570] Loss: 0.13170  Avg Loss: 0.17974  Avg mIoU:  70.95  
[Epoch: 110] [Batch: 0551/0570] Loss: 0.15883  Avg Loss: 0.17972  Avg mIoU:  70.90  

*** Training [@Epoch 110] Avg Loss: 0.17968  Avg mIoU:  70.85  ***

[Epoch: 110] [Batch: 0001/0050] Loss: 0.15491  Avg Loss: 0.15491  Avg mIoU:  63.16  

*** Validation [@Epoch 110] Avg Loss: 0.18019  Avg mIoU:  63.19  ***

[Epoch: 111] [Batch: 0001/0570] Loss: 0.12470  Avg Loss: 0.12470  Avg mIoU:  36.91  
[Epoch: 111] [Batch: 0051/0570] Loss: 0.13915  Avg Loss: 0.18283  Avg mIoU:  71.94  
[Epoch: 111] [Batch: 0101/0570] Loss: 0.15986  Avg Loss: 0.17297  Avg mIoU:  71.97  
[Epoch: 111] [Batch: 0151/0570] Loss: 0.13316  Avg Loss: 0.17520  Avg mIoU:  71.52  
[Epoch: 111] [Batch: 0201/0570] Loss: 0.13209  Avg Loss: 0.17589  Avg mIoU:  71.05  
[Epoch: 111] [Batch: 0251/0570] Loss: 0.11006  Avg Loss: 0.17532  Avg mIoU:  71.46  
[Epoch: 111] [Batch: 0301/0570] Loss: 0.19597  Avg Loss: 0.17516  Avg mIoU:  71.20  
[Epoch: 111] [Batch: 0351/0570] Loss: 0.20646  Avg Loss: 0.17694  Avg mIoU:  70.97  
[Epoch: 111] [Batch: 0401/0570] Loss: 0.15171  Avg Loss: 0.17695  Avg mIoU:  71.09  
[Epoch: 111] [Batch: 0451/0570] Loss: 0.13933  Avg Loss: 0.17697  Avg mIoU:  71.33  
[Epoch: 111] [Batch: 0501/0570] Loss: 0.12183  Avg Loss: 0.17739  Avg mIoU:  71.30  
[Epoch: 111] [Batch: 0551/0570] Loss: 0.17194  Avg Loss: 0.17699  Avg mIoU:  71.36  

*** Training [@Epoch 111] Avg Loss: 0.17695  Avg mIoU:  71.30  ***

[Epoch: 111] [Batch: 0001/0050] Loss: 0.14005  Avg Loss: 0.14005  Avg mIoU:  62.73  

*** Validation [@Epoch 111] Avg Loss: 0.14607  Avg mIoU:  65.46  ***

[Epoch: 112] [Batch: 0001/0570] Loss: 0.13173  Avg Loss: 0.13173  Avg mIoU:  43.77  
[Epoch: 112] [Batch: 0051/0570] Loss: 0.19443  Avg Loss: 0.18907  Avg mIoU:  69.85  
[Epoch: 112] [Batch: 0101/0570] Loss: 0.18543  Avg Loss: 0.17999  Avg mIoU:  70.42  
[Epoch: 112] [Batch: 0151/0570] Loss: 0.16849  Avg Loss: 0.17939  Avg mIoU:  70.16  
[Epoch: 112] [Batch: 0201/0570] Loss: 0.19487  Avg Loss: 0.17910  Avg mIoU:  70.62  
[Epoch: 112] [Batch: 0251/0570] Loss: 0.30731  Avg Loss: 0.17910  Avg mIoU:  70.47  
[Epoch: 112] [Batch: 0301/0570] Loss: 0.16813  Avg Loss: 0.17928  Avg mIoU:  70.35  
[Epoch: 112] [Batch: 0351/0570] Loss: 0.19831  Avg Loss: 0.17981  Avg mIoU:  70.15  
[Epoch: 112] [Batch: 0401/0570] Loss: 0.17117  Avg Loss: 0.17797  Avg mIoU:  70.70  
[Epoch: 112] [Batch: 0451/0570] Loss: 0.15364  Avg Loss: 0.17856  Avg mIoU:  70.58  
[Epoch: 112] [Batch: 0501/0570] Loss: 0.17371  Avg Loss: 0.17821  Avg mIoU:  70.71  
[Epoch: 112] [Batch: 0551/0570] Loss: 0.21738  Avg Loss: 0.17872  Avg mIoU:  70.68  

*** Training [@Epoch 112] Avg Loss: 0.17889  Avg mIoU:  70.66  ***

[Epoch: 112] [Batch: 0001/0050] Loss: 0.15520  Avg Loss: 0.15520  Avg mIoU:  61.42  

*** Validation [@Epoch 112] Avg Loss: 0.17041  Avg mIoU:  64.82  ***

[Epoch: 113] [Batch: 0001/0570] Loss: 0.17132  Avg Loss: 0.17132  Avg mIoU:  32.27  
[Epoch: 113] [Batch: 0051/0570] Loss: 0.18192  Avg Loss: 0.17665  Avg mIoU:  73.03  
[Epoch: 113] [Batch: 0101/0570] Loss: 0.21305  Avg Loss: 0.17760  Avg mIoU:  72.23  
[Epoch: 113] [Batch: 0151/0570] Loss: 0.16845  Avg Loss: 0.17502  Avg mIoU:  71.87  
[Epoch: 113] [Batch: 0201/0570] Loss: 0.20968  Avg Loss: 0.17616  Avg mIoU:  71.67  
[Epoch: 113] [Batch: 0251/0570] Loss: 0.19540  Avg Loss: 0.17629  Avg mIoU:  71.65  
[Epoch: 113] [Batch: 0301/0570] Loss: 0.18912  Avg Loss: 0.17645  Avg mIoU:  71.40  
[Epoch: 113] [Batch: 0351/0570] Loss: 0.16650  Avg Loss: 0.17495  Avg mIoU:  71.49  
[Epoch: 113] [Batch: 0401/0570] Loss: 0.20411  Avg Loss: 0.17668  Avg mIoU:  71.25  
[Epoch: 113] [Batch: 0451/0570] Loss: 0.22832  Avg Loss: 0.17689  Avg mIoU:  71.15  
[Epoch: 113] [Batch: 0501/0570] Loss: 0.14041  Avg Loss: 0.17599  Avg mIoU:  71.17  
[Epoch: 113] [Batch: 0551/0570] Loss: 0.20855  Avg Loss: 0.17562  Avg mIoU:  71.24  

*** Training [@Epoch 113] Avg Loss: 0.17531  Avg mIoU:  71.30  ***

[Epoch: 113] [Batch: 0001/0050] Loss: 0.14713  Avg Loss: 0.14713  Avg mIoU:  63.87  

*** Validation [@Epoch 113] Avg Loss: 0.16571  Avg mIoU:  66.86  ***

Model saved @113 w/ val. mIoU: 66.86.

[Epoch: 114] [Batch: 0001/0570] Loss: 0.21497  Avg Loss: 0.21497  Avg mIoU:  47.14  
[Epoch: 114] [Batch: 0051/0570] Loss: 0.13630  Avg Loss: 0.16277  Avg mIoU:  70.60  
[Epoch: 114] [Batch: 0101/0570] Loss: 0.24990  Avg Loss: 0.17518  Avg mIoU:  70.33  
[Epoch: 114] [Batch: 0151/0570] Loss: 0.10839  Avg Loss: 0.17455  Avg mIoU:  70.56  
[Epoch: 114] [Batch: 0201/0570] Loss: 0.16960  Avg Loss: 0.17678  Avg mIoU:  70.70  
[Epoch: 114] [Batch: 0251/0570] Loss: 0.27487  Avg Loss: 0.17780  Avg mIoU:  70.61  
[Epoch: 114] [Batch: 0301/0570] Loss: 0.19858  Avg Loss: 0.17832  Avg mIoU:  70.64  
[Epoch: 114] [Batch: 0351/0570] Loss: 0.16006  Avg Loss: 0.17713  Avg mIoU:  70.66  
[Epoch: 114] [Batch: 0401/0570] Loss: 0.16750  Avg Loss: 0.17688  Avg mIoU:  70.50  
[Epoch: 114] [Batch: 0451/0570] Loss: 0.17282  Avg Loss: 0.17690  Avg mIoU:  70.85  
[Epoch: 114] [Batch: 0501/0570] Loss: 0.20557  Avg Loss: 0.17728  Avg mIoU:  70.83  
[Epoch: 114] [Batch: 0551/0570] Loss: 0.24737  Avg Loss: 0.17757  Avg mIoU:  70.70  

*** Training [@Epoch 114] Avg Loss: 0.17775  Avg mIoU:  70.64  ***

[Epoch: 114] [Batch: 0001/0050] Loss: 0.15588  Avg Loss: 0.15588  Avg mIoU:  65.36  

*** Validation [@Epoch 114] Avg Loss: 0.16852  Avg mIoU:  65.79  ***

[Epoch: 115] [Batch: 0001/0570] Loss: 0.17570  Avg Loss: 0.17570  Avg mIoU:  27.07  
[Epoch: 115] [Batch: 0051/0570] Loss: 0.13764  Avg Loss: 0.18522  Avg mIoU:  70.59  
[Epoch: 115] [Batch: 0101/0570] Loss: 0.17979  Avg Loss: 0.18098  Avg mIoU:  71.93  
[Epoch: 115] [Batch: 0151/0570] Loss: 0.21300  Avg Loss: 0.17918  Avg mIoU:  71.03  
[Epoch: 115] [Batch: 0201/0570] Loss: 0.17820  Avg Loss: 0.17806  Avg mIoU:  70.96  
[Epoch: 115] [Batch: 0251/0570] Loss: 0.15598  Avg Loss: 0.17857  Avg mIoU:  71.41  
[Epoch: 115] [Batch: 0301/0570] Loss: 0.16910  Avg Loss: 0.17764  Avg mIoU:  71.54  
[Epoch: 115] [Batch: 0351/0570] Loss: 0.19313  Avg Loss: 0.17855  Avg mIoU:  71.27  
[Epoch: 115] [Batch: 0401/0570] Loss: 0.19107  Avg Loss: 0.17765  Avg mIoU:  71.42  
[Epoch: 115] [Batch: 0451/0570] Loss: 0.20773  Avg Loss: 0.17848  Avg mIoU:  71.21  
[Epoch: 115] [Batch: 0501/0570] Loss: 0.14410  Avg Loss: 0.17817  Avg mIoU:  71.31  
[Epoch: 115] [Batch: 0551/0570] Loss: 0.19245  Avg Loss: 0.17781  Avg mIoU:  71.15  

*** Training [@Epoch 115] Avg Loss: 0.17827  Avg mIoU:  71.16  ***

[Epoch: 115] [Batch: 0001/0050] Loss: 0.14221  Avg Loss: 0.14221  Avg mIoU:  63.01  

*** Validation [@Epoch 115] Avg Loss: 0.15937  Avg mIoU:  63.43  ***

[Epoch: 116] [Batch: 0001/0570] Loss: 0.18688  Avg Loss: 0.18688  Avg mIoU:  48.02  
[Epoch: 116] [Batch: 0051/0570] Loss: 0.12882  Avg Loss: 0.18198  Avg mIoU:  69.42  
[Epoch: 116] [Batch: 0101/0570] Loss: 0.13029  Avg Loss: 0.17830  Avg mIoU:  71.07  
[Epoch: 116] [Batch: 0151/0570] Loss: 0.13306  Avg Loss: 0.18013  Avg mIoU:  70.99  
[Epoch: 116] [Batch: 0201/0570] Loss: 0.24791  Avg Loss: 0.18069  Avg mIoU:  70.85  
[Epoch: 116] [Batch: 0251/0570] Loss: 0.22829  Avg Loss: 0.18164  Avg mIoU:  70.52  
[Epoch: 116] [Batch: 0301/0570] Loss: 0.15606  Avg Loss: 0.18039  Avg mIoU:  70.68  
[Epoch: 116] [Batch: 0351/0570] Loss: 0.19095  Avg Loss: 0.18137  Avg mIoU:  70.77  
[Epoch: 116] [Batch: 0401/0570] Loss: 0.23845  Avg Loss: 0.18065  Avg mIoU:  71.04  
[Epoch: 116] [Batch: 0451/0570] Loss: 0.23812  Avg Loss: 0.17911  Avg mIoU:  71.04  
[Epoch: 116] [Batch: 0501/0570] Loss: 0.13012  Avg Loss: 0.17935  Avg mIoU:  70.96  
[Epoch: 116] [Batch: 0551/0570] Loss: 0.23336  Avg Loss: 0.17910  Avg mIoU:  71.11  

*** Training [@Epoch 116] Avg Loss: 0.17883  Avg mIoU:  71.06  ***

[Epoch: 116] [Batch: 0001/0050] Loss: 0.13585  Avg Loss: 0.13585  Avg mIoU:  61.95  

*** Validation [@Epoch 116] Avg Loss: 0.15010  Avg mIoU:  65.42  ***

[Epoch: 117] [Batch: 0001/0570] Loss: 0.14976  Avg Loss: 0.14976  Avg mIoU:  45.92  
[Epoch: 117] [Batch: 0051/0570] Loss: 0.19126  Avg Loss: 0.17816  Avg mIoU:  70.77  
[Epoch: 117] [Batch: 0101/0570] Loss: 0.20685  Avg Loss: 0.17975  Avg mIoU:  70.83  
[Epoch: 117] [Batch: 0151/0570] Loss: 0.10783  Avg Loss: 0.17904  Avg mIoU:  70.75  
[Epoch: 117] [Batch: 0201/0570] Loss: 0.20929  Avg Loss: 0.17586  Avg mIoU:  71.19  
[Epoch: 117] [Batch: 0251/0570] Loss: 0.12222  Avg Loss: 0.17579  Avg mIoU:  70.91  
[Epoch: 117] [Batch: 0301/0570] Loss: 0.19515  Avg Loss: 0.17441  Avg mIoU:  71.23  
[Epoch: 117] [Batch: 0351/0570] Loss: 0.14939  Avg Loss: 0.17500  Avg mIoU:  71.57  
[Epoch: 117] [Batch: 0401/0570] Loss: 0.13225  Avg Loss: 0.17543  Avg mIoU:  71.49  
[Epoch: 117] [Batch: 0451/0570] Loss: 0.13259  Avg Loss: 0.17489  Avg mIoU:  71.43  
[Epoch: 117] [Batch: 0501/0570] Loss: 0.24887  Avg Loss: 0.17491  Avg mIoU:  71.56  
[Epoch: 117] [Batch: 0551/0570] Loss: 0.12747  Avg Loss: 0.17521  Avg mIoU:  71.35  

*** Training [@Epoch 117] Avg Loss: 0.17498  Avg mIoU:  71.44  ***

[Epoch: 117] [Batch: 0001/0050] Loss: 0.14775  Avg Loss: 0.14775  Avg mIoU:  62.59  

*** Validation [@Epoch 117] Avg Loss: 0.16456  Avg mIoU:  64.80  ***

[Epoch: 118] [Batch: 0001/0570] Loss: 0.12550  Avg Loss: 0.12550  Avg mIoU:  39.55  
[Epoch: 118] [Batch: 0051/0570] Loss: 0.14103  Avg Loss: 0.17392  Avg mIoU:  71.46  
[Epoch: 118] [Batch: 0101/0570] Loss: 0.12461  Avg Loss: 0.17060  Avg mIoU:  71.71  
[Epoch: 118] [Batch: 0151/0570] Loss: 0.21913  Avg Loss: 0.17383  Avg mIoU:  71.47  
[Epoch: 118] [Batch: 0201/0570] Loss: 0.18714  Avg Loss: 0.17742  Avg mIoU:  71.08  
[Epoch: 118] [Batch: 0251/0570] Loss: 0.11772  Avg Loss: 0.17622  Avg mIoU:  71.18  
[Epoch: 118] [Batch: 0301/0570] Loss: 0.21427  Avg Loss: 0.17703  Avg mIoU:  71.10  
[Epoch: 118] [Batch: 0351/0570] Loss: 0.23906  Avg Loss: 0.17748  Avg mIoU:  70.99  
[Epoch: 118] [Batch: 0401/0570] Loss: 0.20256  Avg Loss: 0.17829  Avg mIoU:  70.97  
[Epoch: 118] [Batch: 0451/0570] Loss: 0.17957  Avg Loss: 0.17724  Avg mIoU:  71.07  
[Epoch: 118] [Batch: 0501/0570] Loss: 0.12887  Avg Loss: 0.17726  Avg mIoU:  71.03  
[Epoch: 118] [Batch: 0551/0570] Loss: 0.19624  Avg Loss: 0.17775  Avg mIoU:  70.91  

*** Training [@Epoch 118] Avg Loss: 0.17727  Avg mIoU:  70.96  ***

[Epoch: 118] [Batch: 0001/0050] Loss: 0.14296  Avg Loss: 0.14296  Avg mIoU:  62.21  

*** Validation [@Epoch 118] Avg Loss: 0.15866  Avg mIoU:  64.86  ***

[Epoch: 119] [Batch: 0001/0570] Loss: 0.16686  Avg Loss: 0.16686  Avg mIoU:  47.29  
[Epoch: 119] [Batch: 0051/0570] Loss: 0.16694  Avg Loss: 0.17046  Avg mIoU:  71.67  
[Epoch: 119] [Batch: 0101/0570] Loss: 0.19794  Avg Loss: 0.17763  Avg mIoU:  71.23  
[Epoch: 119] [Batch: 0151/0570] Loss: 0.13580  Avg Loss: 0.17809  Avg mIoU:  70.86  
[Epoch: 119] [Batch: 0201/0570] Loss: 0.16066  Avg Loss: 0.17935  Avg mIoU:  71.14  
[Epoch: 119] [Batch: 0251/0570] Loss: 0.17253  Avg Loss: 0.17831  Avg mIoU:  71.22  
[Epoch: 119] [Batch: 0301/0570] Loss: 0.19031  Avg Loss: 0.17778  Avg mIoU:  71.16  
[Epoch: 119] [Batch: 0351/0570] Loss: 0.14451  Avg Loss: 0.17661  Avg mIoU:  71.16  
[Epoch: 119] [Batch: 0401/0570] Loss: 0.14580  Avg Loss: 0.17526  Avg mIoU:  71.25  
[Epoch: 119] [Batch: 0451/0570] Loss: 0.16396  Avg Loss: 0.17460  Avg mIoU:  71.26  
[Epoch: 119] [Batch: 0501/0570] Loss: 0.16915  Avg Loss: 0.17466  Avg mIoU:  71.16  
[Epoch: 119] [Batch: 0551/0570] Loss: 0.20234  Avg Loss: 0.17679  Avg mIoU:  70.96  

*** Training [@Epoch 119] Avg Loss: 0.17664  Avg mIoU:  71.05  ***

[Epoch: 119] [Batch: 0001/0050] Loss: 0.14658  Avg Loss: 0.14658  Avg mIoU:  64.95  

*** Validation [@Epoch 119] Avg Loss: 0.16135  Avg mIoU:  65.91  ***

[Epoch: 120] [Batch: 0001/0570] Loss: 0.13659  Avg Loss: 0.13659  Avg mIoU:  52.50  
[Epoch: 120] [Batch: 0051/0570] Loss: 0.19004  Avg Loss: 0.18265  Avg mIoU:  70.72  
[Epoch: 120] [Batch: 0101/0570] Loss: 0.09969  Avg Loss: 0.17789  Avg mIoU:  71.90  
[Epoch: 120] [Batch: 0151/0570] Loss: 0.16400  Avg Loss: 0.17335  Avg mIoU:  72.23  
[Epoch: 120] [Batch: 0201/0570] Loss: 0.13053  Avg Loss: 0.17213  Avg mIoU:  71.96  
[Epoch: 120] [Batch: 0251/0570] Loss: 0.17036  Avg Loss: 0.17332  Avg mIoU:  71.93  
[Epoch: 120] [Batch: 0301/0570] Loss: 0.18129  Avg Loss: 0.17477  Avg mIoU:  71.76  
[Epoch: 120] [Batch: 0351/0570] Loss: 0.14391  Avg Loss: 0.17516  Avg mIoU:  71.51  
[Epoch: 120] [Batch: 0401/0570] Loss: 0.15323  Avg Loss: 0.17515  Avg mIoU:  71.37  
[Epoch: 120] [Batch: 0451/0570] Loss: 0.18558  Avg Loss: 0.17518  Avg mIoU:  71.34  
[Epoch: 120] [Batch: 0501/0570] Loss: 0.11171  Avg Loss: 0.17550  Avg mIoU:  71.28  
[Epoch: 120] [Batch: 0551/0570] Loss: 0.19561  Avg Loss: 0.17674  Avg mIoU:  71.10  

*** Training [@Epoch 120] Avg Loss: 0.17690  Avg mIoU:  71.13  ***

[Epoch: 120] [Batch: 0001/0050] Loss: 0.16088  Avg Loss: 0.16088  Avg mIoU:  63.49  

*** Validation [@Epoch 120] Avg Loss: 0.17733  Avg mIoU:  64.76  ***

[Epoch: 121] [Batch: 0001/0570] Loss: 0.13670  Avg Loss: 0.13670  Avg mIoU:  52.64  
[Epoch: 121] [Batch: 0051/0570] Loss: 0.14751  Avg Loss: 0.17226  Avg mIoU:  74.34  
[Epoch: 121] [Batch: 0101/0570] Loss: 0.21758  Avg Loss: 0.17512  Avg mIoU:  71.66  
[Epoch: 121] [Batch: 0151/0570] Loss: 0.12459  Avg Loss: 0.17420  Avg mIoU:  71.99  
[Epoch: 121] [Batch: 0201/0570] Loss: 0.15370  Avg Loss: 0.17485  Avg mIoU:  71.91  
[Epoch: 121] [Batch: 0251/0570] Loss: 0.17404  Avg Loss: 0.17595  Avg mIoU:  71.47  
[Epoch: 121] [Batch: 0301/0570] Loss: 0.11993  Avg Loss: 0.17642  Avg mIoU:  71.09  
[Epoch: 121] [Batch: 0351/0570] Loss: 0.33309  Avg Loss: 0.17651  Avg mIoU:  70.83  
[Epoch: 121] [Batch: 0401/0570] Loss: 0.17171  Avg Loss: 0.17542  Avg mIoU:  71.13  
[Epoch: 121] [Batch: 0451/0570] Loss: 0.16310  Avg Loss: 0.17501  Avg mIoU:  71.07  
[Epoch: 121] [Batch: 0501/0570] Loss: 0.11506  Avg Loss: 0.17497  Avg mIoU:  71.09  
[Epoch: 121] [Batch: 0551/0570] Loss: 0.23009  Avg Loss: 0.17540  Avg mIoU:  71.09  

*** Training [@Epoch 121] Avg Loss: 0.17582  Avg mIoU:  71.10  ***

[Epoch: 121] [Batch: 0001/0050] Loss: 0.15090  Avg Loss: 0.15090  Avg mIoU:  63.16  

*** Validation [@Epoch 121] Avg Loss: 0.16908  Avg mIoU:  63.81  ***

[Epoch: 122] [Batch: 0001/0570] Loss: 0.17264  Avg Loss: 0.17264  Avg mIoU:  37.09  
[Epoch: 122] [Batch: 0051/0570] Loss: 0.24518  Avg Loss: 0.17027  Avg mIoU:  72.14  
[Epoch: 122] [Batch: 0101/0570] Loss: 0.20072  Avg Loss: 0.17450  Avg mIoU:  71.55  
[Epoch: 122] [Batch: 0151/0570] Loss: 0.24277  Avg Loss: 0.17165  Avg mIoU:  72.25  
[Epoch: 122] [Batch: 0201/0570] Loss: 0.20198  Avg Loss: 0.17522  Avg mIoU:  71.70  
[Epoch: 122] [Batch: 0251/0570] Loss: 0.14406  Avg Loss: 0.17696  Avg mIoU:  71.52  
[Epoch: 122] [Batch: 0301/0570] Loss: 0.16100  Avg Loss: 0.17501  Avg mIoU:  71.82  
[Epoch: 122] [Batch: 0351/0570] Loss: 0.18371  Avg Loss: 0.17478  Avg mIoU:  71.58  
[Epoch: 122] [Batch: 0401/0570] Loss: 0.15940  Avg Loss: 0.17442  Avg mIoU:  71.56  
[Epoch: 122] [Batch: 0451/0570] Loss: 0.22772  Avg Loss: 0.17484  Avg mIoU:  71.69  
[Epoch: 122] [Batch: 0501/0570] Loss: 0.16385  Avg Loss: 0.17505  Avg mIoU:  71.73  
[Epoch: 122] [Batch: 0551/0570] Loss: 0.21948  Avg Loss: 0.17536  Avg mIoU:  71.67  

*** Training [@Epoch 122] Avg Loss: 0.17563  Avg mIoU:  71.54  ***

[Epoch: 122] [Batch: 0001/0050] Loss: 0.13385  Avg Loss: 0.13385  Avg mIoU:  62.73  

*** Validation [@Epoch 122] Avg Loss: 0.15301  Avg mIoU:  65.01  ***

[Epoch: 123] [Batch: 0001/0570] Loss: 0.19122  Avg Loss: 0.19122  Avg mIoU:  44.57  
[Epoch: 123] [Batch: 0051/0570] Loss: 0.19805  Avg Loss: 0.17718  Avg mIoU:  74.00  
[Epoch: 123] [Batch: 0101/0570] Loss: 0.14989  Avg Loss: 0.17639  Avg mIoU:  72.94  
[Epoch: 123] [Batch: 0151/0570] Loss: 0.19454  Avg Loss: 0.17434  Avg mIoU:  72.55  
[Epoch: 123] [Batch: 0201/0570] Loss: 0.18060  Avg Loss: 0.17340  Avg mIoU:  72.37  
[Epoch: 123] [Batch: 0251/0570] Loss: 0.22543  Avg Loss: 0.17317  Avg mIoU:  71.88  
[Epoch: 123] [Batch: 0301/0570] Loss: 0.13693  Avg Loss: 0.17371  Avg mIoU:  72.10  
[Epoch: 123] [Batch: 0351/0570] Loss: 0.17871  Avg Loss: 0.17327  Avg mIoU:  72.46  
[Epoch: 123] [Batch: 0401/0570] Loss: 0.22044  Avg Loss: 0.17435  Avg mIoU:  72.24  
[Epoch: 123] [Batch: 0451/0570] Loss: 0.24949  Avg Loss: 0.17448  Avg mIoU:  72.06  
[Epoch: 123] [Batch: 0501/0570] Loss: 0.12141  Avg Loss: 0.17411  Avg mIoU:  71.93  
[Epoch: 123] [Batch: 0551/0570] Loss: 0.18076  Avg Loss: 0.17477  Avg mIoU:  71.73  

*** Training [@Epoch 123] Avg Loss: 0.17449  Avg mIoU:  71.75  ***

[Epoch: 123] [Batch: 0001/0050] Loss: 0.13401  Avg Loss: 0.13401  Avg mIoU:  63.12  

*** Validation [@Epoch 123] Avg Loss: 0.16886  Avg mIoU:  65.11  ***

[Epoch: 124] [Batch: 0001/0570] Loss: 0.20833  Avg Loss: 0.20833  Avg mIoU:  46.97  
[Epoch: 124] [Batch: 0051/0570] Loss: 0.18106  Avg Loss: 0.16780  Avg mIoU:  72.43  
[Epoch: 124] [Batch: 0101/0570] Loss: 0.13480  Avg Loss: 0.16883  Avg mIoU:  71.56  
[Epoch: 124] [Batch: 0151/0570] Loss: 0.35137  Avg Loss: 0.17144  Avg mIoU:  70.94  
[Epoch: 124] [Batch: 0201/0570] Loss: 0.12259  Avg Loss: 0.17085  Avg mIoU:  71.22  
[Epoch: 124] [Batch: 0251/0570] Loss: 0.19504  Avg Loss: 0.17281  Avg mIoU:  71.19  
[Epoch: 124] [Batch: 0301/0570] Loss: 0.18040  Avg Loss: 0.17152  Avg mIoU:  71.62  
[Epoch: 124] [Batch: 0351/0570] Loss: 0.11470  Avg Loss: 0.17206  Avg mIoU:  71.68  
[Epoch: 124] [Batch: 0401/0570] Loss: 0.11806  Avg Loss: 0.17177  Avg mIoU:  71.99  
[Epoch: 124] [Batch: 0451/0570] Loss: 0.23351  Avg Loss: 0.17181  Avg mIoU:  72.06  
[Epoch: 124] [Batch: 0501/0570] Loss: 0.21869  Avg Loss: 0.17259  Avg mIoU:  71.83  
[Epoch: 124] [Batch: 0551/0570] Loss: 0.16563  Avg Loss: 0.17355  Avg mIoU:  71.80  

*** Training [@Epoch 124] Avg Loss: 0.17381  Avg mIoU:  71.78  ***

[Epoch: 124] [Batch: 0001/0050] Loss: 0.14834  Avg Loss: 0.14834  Avg mIoU:  63.08  

*** Validation [@Epoch 124] Avg Loss: 0.16486  Avg mIoU:  65.72  ***

[Epoch: 125] [Batch: 0001/0570] Loss: 0.15628  Avg Loss: 0.15628  Avg mIoU:  36.28  
[Epoch: 125] [Batch: 0051/0570] Loss: 0.16042  Avg Loss: 0.18063  Avg mIoU:  71.65  
[Epoch: 125] [Batch: 0101/0570] Loss: 0.17616  Avg Loss: 0.17999  Avg mIoU:  72.22  
[Epoch: 125] [Batch: 0151/0570] Loss: 0.12888  Avg Loss: 0.17853  Avg mIoU:  71.77  
[Epoch: 125] [Batch: 0201/0570] Loss: 0.17489  Avg Loss: 0.17917  Avg mIoU:  71.49  
[Epoch: 125] [Batch: 0251/0570] Loss: 0.14053  Avg Loss: 0.17665  Avg mIoU:  71.89  
[Epoch: 125] [Batch: 0301/0570] Loss: 0.12934  Avg Loss: 0.17515  Avg mIoU:  71.86  
[Epoch: 125] [Batch: 0351/0570] Loss: 0.17986  Avg Loss: 0.17515  Avg mIoU:  71.80  
[Epoch: 125] [Batch: 0401/0570] Loss: 0.18412  Avg Loss: 0.17563  Avg mIoU:  71.63  
[Epoch: 125] [Batch: 0451/0570] Loss: 0.18283  Avg Loss: 0.17382  Avg mIoU:  71.77  
[Epoch: 125] [Batch: 0501/0570] Loss: 0.14073  Avg Loss: 0.17419  Avg mIoU:  71.80  
[Epoch: 125] [Batch: 0551/0570] Loss: 0.12971  Avg Loss: 0.17415  Avg mIoU:  71.75  

*** Training [@Epoch 125] Avg Loss: 0.17407  Avg mIoU:  71.80  ***

[Epoch: 125] [Batch: 0001/0050] Loss: 0.13561  Avg Loss: 0.13561  Avg mIoU:  63.00  

*** Validation [@Epoch 125] Avg Loss: 0.16625  Avg mIoU:  65.71  ***

[Epoch: 126] [Batch: 0001/0570] Loss: 0.15838  Avg Loss: 0.15838  Avg mIoU:  49.04  
[Epoch: 126] [Batch: 0051/0570] Loss: 0.11430  Avg Loss: 0.16543  Avg mIoU:  72.95  
[Epoch: 126] [Batch: 0101/0570] Loss: 0.11573  Avg Loss: 0.16384  Avg mIoU:  73.30  
[Epoch: 126] [Batch: 0151/0570] Loss: 0.17647  Avg Loss: 0.17164  Avg mIoU:  72.43  
[Epoch: 126] [Batch: 0201/0570] Loss: 0.13619  Avg Loss: 0.17400  Avg mIoU:  71.84  
[Epoch: 126] [Batch: 0251/0570] Loss: 0.15406  Avg Loss: 0.17421  Avg mIoU:  71.60  
[Epoch: 126] [Batch: 0301/0570] Loss: 0.15400  Avg Loss: 0.17357  Avg mIoU:  71.61  
[Epoch: 126] [Batch: 0351/0570] Loss: 0.18910  Avg Loss: 0.17365  Avg mIoU:  71.46  
[Epoch: 126] [Batch: 0401/0570] Loss: 0.19062  Avg Loss: 0.17392  Avg mIoU:  71.70  
[Epoch: 126] [Batch: 0451/0570] Loss: 0.18466  Avg Loss: 0.17343  Avg mIoU:  71.52  
[Epoch: 126] [Batch: 0501/0570] Loss: 0.14620  Avg Loss: 0.17280  Avg mIoU:  71.74  
[Epoch: 126] [Batch: 0551/0570] Loss: 0.13930  Avg Loss: 0.17363  Avg mIoU:  71.57  

*** Training [@Epoch 126] Avg Loss: 0.17389  Avg mIoU:  71.56  ***

[Epoch: 126] [Batch: 0001/0050] Loss: 0.14861  Avg Loss: 0.14861  Avg mIoU:  64.41  

*** Validation [@Epoch 126] Avg Loss: 0.16454  Avg mIoU:  66.21  ***

[Epoch: 127] [Batch: 0001/0570] Loss: 0.19699  Avg Loss: 0.19699  Avg mIoU:  39.93  
[Epoch: 127] [Batch: 0051/0570] Loss: 0.16552  Avg Loss: 0.17899  Avg mIoU:  71.48  
[Epoch: 127] [Batch: 0101/0570] Loss: 0.13796  Avg Loss: 0.18013  Avg mIoU:  71.18  
[Epoch: 127] [Batch: 0151/0570] Loss: 0.13856  Avg Loss: 0.17434  Avg mIoU:  71.68  
[Epoch: 127] [Batch: 0201/0570] Loss: 0.19833  Avg Loss: 0.17416  Avg mIoU:  71.63  
[Epoch: 127] [Batch: 0251/0570] Loss: 0.19176  Avg Loss: 0.17481  Avg mIoU:  71.55  
[Epoch: 127] [Batch: 0301/0570] Loss: 0.24696  Avg Loss: 0.17544  Avg mIoU:  71.62  
[Epoch: 127] [Batch: 0351/0570] Loss: 0.18558  Avg Loss: 0.17575  Avg mIoU:  71.45  
[Epoch: 127] [Batch: 0401/0570] Loss: 0.22507  Avg Loss: 0.17649  Avg mIoU:  71.40  
[Epoch: 127] [Batch: 0451/0570] Loss: 0.11991  Avg Loss: 0.17611  Avg mIoU:  71.56  
[Epoch: 127] [Batch: 0501/0570] Loss: 0.23036  Avg Loss: 0.17548  Avg mIoU:  71.67  
[Epoch: 127] [Batch: 0551/0570] Loss: 0.21815  Avg Loss: 0.17475  Avg mIoU:  71.72  

*** Training [@Epoch 127] Avg Loss: 0.17522  Avg mIoU:  71.62  ***

[Epoch: 127] [Batch: 0001/0050] Loss: 0.14485  Avg Loss: 0.14485  Avg mIoU:  66.02  

*** Validation [@Epoch 127] Avg Loss: 0.17216  Avg mIoU:  65.49  ***

[Epoch: 128] [Batch: 0001/0570] Loss: 0.19118  Avg Loss: 0.19118  Avg mIoU:  41.32  
[Epoch: 128] [Batch: 0051/0570] Loss: 0.14227  Avg Loss: 0.16709  Avg mIoU:  72.19  
[Epoch: 128] [Batch: 0101/0570] Loss: 0.12070  Avg Loss: 0.16968  Avg mIoU:  72.02  
[Epoch: 128] [Batch: 0151/0570] Loss: 0.15988  Avg Loss: 0.17158  Avg mIoU:  71.92  
[Epoch: 128] [Batch: 0201/0570] Loss: 0.17327  Avg Loss: 0.17161  Avg mIoU:  72.07  
[Epoch: 128] [Batch: 0251/0570] Loss: 0.09683  Avg Loss: 0.17250  Avg mIoU:  72.41  
[Epoch: 128] [Batch: 0301/0570] Loss: 0.12694  Avg Loss: 0.17281  Avg mIoU:  72.26  
[Epoch: 128] [Batch: 0351/0570] Loss: 0.16183  Avg Loss: 0.17293  Avg mIoU:  72.16  
[Epoch: 128] [Batch: 0401/0570] Loss: 0.20726  Avg Loss: 0.17291  Avg mIoU:  72.06  
[Epoch: 128] [Batch: 0451/0570] Loss: 0.21599  Avg Loss: 0.17311  Avg mIoU:  71.79  
[Epoch: 128] [Batch: 0501/0570] Loss: 0.18614  Avg Loss: 0.17216  Avg mIoU:  71.91  
[Epoch: 128] [Batch: 0551/0570] Loss: 0.22887  Avg Loss: 0.17236  Avg mIoU:  71.93  

*** Training [@Epoch 128] Avg Loss: 0.17241  Avg mIoU:  71.86  ***

[Epoch: 128] [Batch: 0001/0050] Loss: 0.17160  Avg Loss: 0.17160  Avg mIoU:  63.42  

*** Validation [@Epoch 128] Avg Loss: 0.18534  Avg mIoU:  63.60  ***

[Epoch: 129] [Batch: 0001/0570] Loss: 0.13043  Avg Loss: 0.13043  Avg mIoU:  48.21  
[Epoch: 129] [Batch: 0051/0570] Loss: 0.15007  Avg Loss: 0.17034  Avg mIoU:  69.90  
[Epoch: 129] [Batch: 0101/0570] Loss: 0.12818  Avg Loss: 0.17494  Avg mIoU:  71.53  
[Epoch: 129] [Batch: 0151/0570] Loss: 0.20632  Avg Loss: 0.17089  Avg mIoU:  72.19  
[Epoch: 129] [Batch: 0201/0570] Loss: 0.20259  Avg Loss: 0.16921  Avg mIoU:  72.36  
[Epoch: 129] [Batch: 0251/0570] Loss: 0.20742  Avg Loss: 0.17166  Avg mIoU:  71.81  
[Epoch: 129] [Batch: 0301/0570] Loss: 0.11897  Avg Loss: 0.17255  Avg mIoU:  71.78  
[Epoch: 129] [Batch: 0351/0570] Loss: 0.14349  Avg Loss: 0.17357  Avg mIoU:  71.65  
[Epoch: 129] [Batch: 0401/0570] Loss: 0.21420  Avg Loss: 0.17343  Avg mIoU:  71.72  
[Epoch: 129] [Batch: 0451/0570] Loss: 0.14837  Avg Loss: 0.17212  Avg mIoU:  71.94  
[Epoch: 129] [Batch: 0501/0570] Loss: 0.18724  Avg Loss: 0.17218  Avg mIoU:  71.82  
[Epoch: 129] [Batch: 0551/0570] Loss: 0.13101  Avg Loss: 0.17291  Avg mIoU:  71.80  

*** Training [@Epoch 129] Avg Loss: 0.17335  Avg mIoU:  71.76  ***

[Epoch: 129] [Batch: 0001/0050] Loss: 0.14926  Avg Loss: 0.14926  Avg mIoU:  61.55  

*** Validation [@Epoch 129] Avg Loss: 0.16275  Avg mIoU:  64.16  ***

[Epoch: 130] [Batch: 0001/0570] Loss: 0.13069  Avg Loss: 0.13069  Avg mIoU:  50.93  
[Epoch: 130] [Batch: 0051/0570] Loss: 0.22543  Avg Loss: 0.16817  Avg mIoU:  73.18  
[Epoch: 130] [Batch: 0101/0570] Loss: 0.16485  Avg Loss: 0.17372  Avg mIoU:  71.93  
[Epoch: 130] [Batch: 0151/0570] Loss: 0.19193  Avg Loss: 0.17213  Avg mIoU:  71.67  
[Epoch: 130] [Batch: 0201/0570] Loss: 0.24274  Avg Loss: 0.17362  Avg mIoU:  71.63  
[Epoch: 130] [Batch: 0251/0570] Loss: 0.13777  Avg Loss: 0.17305  Avg mIoU:  71.91  
[Epoch: 130] [Batch: 0301/0570] Loss: 0.21089  Avg Loss: 0.17294  Avg mIoU:  71.89  
[Epoch: 130] [Batch: 0351/0570] Loss: 0.09554  Avg Loss: 0.17286  Avg mIoU:  71.84  
[Epoch: 130] [Batch: 0401/0570] Loss: 0.14529  Avg Loss: 0.17331  Avg mIoU:  71.85  
[Epoch: 130] [Batch: 0451/0570] Loss: 0.19062  Avg Loss: 0.17328  Avg mIoU:  71.77  
[Epoch: 130] [Batch: 0501/0570] Loss: 0.14173  Avg Loss: 0.17393  Avg mIoU:  71.82  
[Epoch: 130] [Batch: 0551/0570] Loss: 0.16822  Avg Loss: 0.17385  Avg mIoU:  71.72  

*** Training [@Epoch 130] Avg Loss: 0.17334  Avg mIoU:  71.72  ***

[Epoch: 130] [Batch: 0001/0050] Loss: 0.15665  Avg Loss: 0.15665  Avg mIoU:  62.64  

*** Validation [@Epoch 130] Avg Loss: 0.16394  Avg mIoU:  64.90  ***

[Epoch: 131] [Batch: 0001/0570] Loss: 0.14247  Avg Loss: 0.14247  Avg mIoU:  52.68  
[Epoch: 131] [Batch: 0051/0570] Loss: 0.17682  Avg Loss: 0.17448  Avg mIoU:  71.24  
[Epoch: 131] [Batch: 0101/0570] Loss: 0.23058  Avg Loss: 0.17361  Avg mIoU:  71.43  
[Epoch: 131] [Batch: 0151/0570] Loss: 0.21213  Avg Loss: 0.17238  Avg mIoU:  71.63  
[Epoch: 131] [Batch: 0201/0570] Loss: 0.18019  Avg Loss: 0.17188  Avg mIoU:  71.59  
[Epoch: 131] [Batch: 0251/0570] Loss: 0.14814  Avg Loss: 0.17354  Avg mIoU:  71.44  
[Epoch: 131] [Batch: 0301/0570] Loss: 0.14262  Avg Loss: 0.17416  Avg mIoU:  71.62  
[Epoch: 131] [Batch: 0351/0570] Loss: 0.16960  Avg Loss: 0.17348  Avg mIoU:  71.65  
[Epoch: 131] [Batch: 0401/0570] Loss: 0.14991  Avg Loss: 0.17369  Avg mIoU:  71.64  
[Epoch: 131] [Batch: 0451/0570] Loss: 0.14253  Avg Loss: 0.17385  Avg mIoU:  71.74  
[Epoch: 131] [Batch: 0501/0570] Loss: 0.11410  Avg Loss: 0.17355  Avg mIoU:  71.55  
[Epoch: 131] [Batch: 0551/0570] Loss: 0.13085  Avg Loss: 0.17331  Avg mIoU:  71.64  

*** Training [@Epoch 131] Avg Loss: 0.17416  Avg mIoU:  71.53  ***

[Epoch: 131] [Batch: 0001/0050] Loss: 0.20916  Avg Loss: 0.20916  Avg mIoU:  60.75  

*** Validation [@Epoch 131] Avg Loss: 0.21731  Avg mIoU:  61.20  ***

[Epoch: 132] [Batch: 0001/0570] Loss: 0.21790  Avg Loss: 0.21790  Avg mIoU:  41.87  
[Epoch: 132] [Batch: 0051/0570] Loss: 0.13824  Avg Loss: 0.18061  Avg mIoU:  69.61  
[Epoch: 132] [Batch: 0101/0570] Loss: 0.14955  Avg Loss: 0.17743  Avg mIoU:  71.78  
[Epoch: 132] [Batch: 0151/0570] Loss: 0.18506  Avg Loss: 0.17378  Avg mIoU:  72.11  
[Epoch: 132] [Batch: 0201/0570] Loss: 0.11276  Avg Loss: 0.17403  Avg mIoU:  71.84  
[Epoch: 132] [Batch: 0251/0570] Loss: 0.17141  Avg Loss: 0.17251  Avg mIoU:  71.80  
[Epoch: 132] [Batch: 0301/0570] Loss: 0.13181  Avg Loss: 0.17369  Avg mIoU:  71.73  
[Epoch: 132] [Batch: 0351/0570] Loss: 0.11237  Avg Loss: 0.17342  Avg mIoU:  71.87  
[Epoch: 132] [Batch: 0401/0570] Loss: 0.19769  Avg Loss: 0.17369  Avg mIoU:  71.79  
[Epoch: 132] [Batch: 0451/0570] Loss: 0.19052  Avg Loss: 0.17382  Avg mIoU:  71.74  
[Epoch: 132] [Batch: 0501/0570] Loss: 0.18360  Avg Loss: 0.17377  Avg mIoU:  71.70  
[Epoch: 132] [Batch: 0551/0570] Loss: 0.22074  Avg Loss: 0.17422  Avg mIoU:  71.57  

*** Training [@Epoch 132] Avg Loss: 0.17386  Avg mIoU:  71.55  ***

[Epoch: 132] [Batch: 0001/0050] Loss: 0.14889  Avg Loss: 0.14889  Avg mIoU:  64.51  

*** Validation [@Epoch 132] Avg Loss: 0.16480  Avg mIoU:  65.50  ***

[Epoch: 133] [Batch: 0001/0570] Loss: 0.19842  Avg Loss: 0.19842  Avg mIoU:  56.59  
[Epoch: 133] [Batch: 0051/0570] Loss: 0.15398  Avg Loss: 0.17136  Avg mIoU:  71.65  
[Epoch: 133] [Batch: 0101/0570] Loss: 0.13771  Avg Loss: 0.17456  Avg mIoU:  70.64  
[Epoch: 133] [Batch: 0151/0570] Loss: 0.15294  Avg Loss: 0.17405  Avg mIoU:  70.98  
[Epoch: 133] [Batch: 0201/0570] Loss: 0.17191  Avg Loss: 0.17418  Avg mIoU:  71.15  
[Epoch: 133] [Batch: 0251/0570] Loss: 0.22868  Avg Loss: 0.17339  Avg mIoU:  71.25  
[Epoch: 133] [Batch: 0301/0570] Loss: 0.15800  Avg Loss: 0.17362  Avg mIoU:  70.83  
[Epoch: 133] [Batch: 0351/0570] Loss: 0.28755  Avg Loss: 0.17369  Avg mIoU:  70.99  
[Epoch: 133] [Batch: 0401/0570] Loss: 0.22965  Avg Loss: 0.17317  Avg mIoU:  71.29  
[Epoch: 133] [Batch: 0451/0570] Loss: 0.15844  Avg Loss: 0.17278  Avg mIoU:  71.42  
[Epoch: 133] [Batch: 0501/0570] Loss: 0.15785  Avg Loss: 0.17269  Avg mIoU:  71.48  
[Epoch: 133] [Batch: 0551/0570] Loss: 0.27776  Avg Loss: 0.17211  Avg mIoU:  71.65  

*** Training [@Epoch 133] Avg Loss: 0.17183  Avg mIoU:  71.69  ***

[Epoch: 133] [Batch: 0001/0050] Loss: 0.13792  Avg Loss: 0.13792  Avg mIoU:  63.96  

*** Validation [@Epoch 133] Avg Loss: 0.15459  Avg mIoU:  65.57  ***

[Epoch: 134] [Batch: 0001/0570] Loss: 0.14769  Avg Loss: 0.14769  Avg mIoU:  41.64  
[Epoch: 134] [Batch: 0051/0570] Loss: 0.16836  Avg Loss: 0.16946  Avg mIoU:  70.93  
[Epoch: 134] [Batch: 0101/0570] Loss: 0.12864  Avg Loss: 0.17129  Avg mIoU:  71.40  
[Epoch: 134] [Batch: 0151/0570] Loss: 0.16607  Avg Loss: 0.16860  Avg mIoU:  72.10  
[Epoch: 134] [Batch: 0201/0570] Loss: 0.17426  Avg Loss: 0.16740  Avg mIoU:  72.40  
[Epoch: 134] [Batch: 0251/0570] Loss: 0.18978  Avg Loss: 0.16719  Avg mIoU:  72.42  
[Epoch: 134] [Batch: 0301/0570] Loss: 0.18849  Avg Loss: 0.16771  Avg mIoU:  72.46  
[Epoch: 134] [Batch: 0351/0570] Loss: 0.20057  Avg Loss: 0.16923  Avg mIoU:  72.42  
[Epoch: 134] [Batch: 0401/0570] Loss: 0.17395  Avg Loss: 0.16847  Avg mIoU:  72.10  
[Epoch: 134] [Batch: 0451/0570] Loss: 0.15312  Avg Loss: 0.16932  Avg mIoU:  72.02  
[Epoch: 134] [Batch: 0501/0570] Loss: 0.12037  Avg Loss: 0.16949  Avg mIoU:  72.09  
[Epoch: 134] [Batch: 0551/0570] Loss: 0.12168  Avg Loss: 0.17009  Avg mIoU:  72.03  

*** Training [@Epoch 134] Avg Loss: 0.17001  Avg mIoU:  72.11  ***

[Epoch: 134] [Batch: 0001/0050] Loss: 0.13960  Avg Loss: 0.13960  Avg mIoU:  61.87  

*** Validation [@Epoch 134] Avg Loss: 0.15166  Avg mIoU:  63.39  ***

[Epoch: 135] [Batch: 0001/0570] Loss: 0.22717  Avg Loss: 0.22717  Avg mIoU:  30.96  
[Epoch: 135] [Batch: 0051/0570] Loss: 0.16850  Avg Loss: 0.17687  Avg mIoU:  70.82  
[Epoch: 135] [Batch: 0101/0570] Loss: 0.16345  Avg Loss: 0.17217  Avg mIoU:  71.54  
[Epoch: 135] [Batch: 0151/0570] Loss: 0.26372  Avg Loss: 0.16992  Avg mIoU:  72.12  
[Epoch: 135] [Batch: 0201/0570] Loss: 0.24991  Avg Loss: 0.17042  Avg mIoU:  71.83  
[Epoch: 135] [Batch: 0251/0570] Loss: 0.14090  Avg Loss: 0.16899  Avg mIoU:  71.78  
[Epoch: 135] [Batch: 0301/0570] Loss: 0.17007  Avg Loss: 0.16788  Avg mIoU:  71.99  
[Epoch: 135] [Batch: 0351/0570] Loss: 0.17321  Avg Loss: 0.16866  Avg mIoU:  71.81  
[Epoch: 135] [Batch: 0401/0570] Loss: 0.15237  Avg Loss: 0.16937  Avg mIoU:  71.87  
[Epoch: 135] [Batch: 0451/0570] Loss: 0.14607  Avg Loss: 0.17094  Avg mIoU:  71.83  
[Epoch: 135] [Batch: 0501/0570] Loss: 0.13494  Avg Loss: 0.17148  Avg mIoU:  71.78  
[Epoch: 135] [Batch: 0551/0570] Loss: 0.13494  Avg Loss: 0.17037  Avg mIoU:  72.05  

*** Training [@Epoch 135] Avg Loss: 0.17063  Avg mIoU:  72.10  ***

[Epoch: 135] [Batch: 0001/0050] Loss: 0.17358  Avg Loss: 0.17358  Avg mIoU:  61.46  

*** Validation [@Epoch 135] Avg Loss: 0.16411  Avg mIoU:  65.48  ***

[Epoch: 136] [Batch: 0001/0570] Loss: 0.20179  Avg Loss: 0.20179  Avg mIoU:  47.36  
[Epoch: 136] [Batch: 0051/0570] Loss: 0.18930  Avg Loss: 0.16937  Avg mIoU:  73.12  
[Epoch: 136] [Batch: 0101/0570] Loss: 0.20934  Avg Loss: 0.17277  Avg mIoU:  72.35  
[Epoch: 136] [Batch: 0151/0570] Loss: 0.19876  Avg Loss: 0.17232  Avg mIoU:  72.69  
[Epoch: 136] [Batch: 0201/0570] Loss: 0.16463  Avg Loss: 0.17399  Avg mIoU:  72.11  
[Epoch: 136] [Batch: 0251/0570] Loss: 0.15674  Avg Loss: 0.17196  Avg mIoU:  72.78  
[Epoch: 136] [Batch: 0301/0570] Loss: 0.14112  Avg Loss: 0.17064  Avg mIoU:  72.66  
[Epoch: 136] [Batch: 0351/0570] Loss: 0.19330  Avg Loss: 0.17016  Avg mIoU:  72.80  
[Epoch: 136] [Batch: 0401/0570] Loss: 0.15007  Avg Loss: 0.16914  Avg mIoU:  72.81  
[Epoch: 136] [Batch: 0451/0570] Loss: 0.14049  Avg Loss: 0.16929  Avg mIoU:  72.67  
[Epoch: 136] [Batch: 0501/0570] Loss: 0.19675  Avg Loss: 0.16984  Avg mIoU:  72.49  
[Epoch: 136] [Batch: 0551/0570] Loss: 0.14306  Avg Loss: 0.16968  Avg mIoU:  72.51  

*** Training [@Epoch 136] Avg Loss: 0.16993  Avg mIoU:  72.50  ***

[Epoch: 136] [Batch: 0001/0050] Loss: 0.14491  Avg Loss: 0.14491  Avg mIoU:  62.61  

*** Validation [@Epoch 136] Avg Loss: 0.15987  Avg mIoU:  63.65  ***

[Epoch: 137] [Batch: 0001/0570] Loss: 0.12860  Avg Loss: 0.12860  Avg mIoU:  45.51  
[Epoch: 137] [Batch: 0051/0570] Loss: 0.22070  Avg Loss: 0.17038  Avg mIoU:  71.58  
[Epoch: 137] [Batch: 0101/0570] Loss: 0.15895  Avg Loss: 0.17392  Avg mIoU:  71.98  
[Epoch: 137] [Batch: 0151/0570] Loss: 0.20531  Avg Loss: 0.17140  Avg mIoU:  72.03  
[Epoch: 137] [Batch: 0201/0570] Loss: 0.19787  Avg Loss: 0.16911  Avg mIoU:  72.40  
[Epoch: 137] [Batch: 0251/0570] Loss: 0.14903  Avg Loss: 0.16906  Avg mIoU:  72.23  
[Epoch: 137] [Batch: 0301/0570] Loss: 0.19811  Avg Loss: 0.17143  Avg mIoU:  72.17  
[Epoch: 137] [Batch: 0351/0570] Loss: 0.18740  Avg Loss: 0.17041  Avg mIoU:  72.09  
[Epoch: 137] [Batch: 0401/0570] Loss: 0.13047  Avg Loss: 0.17110  Avg mIoU:  72.02  
[Epoch: 137] [Batch: 0451/0570] Loss: 0.19480  Avg Loss: 0.17075  Avg mIoU:  72.24  
[Epoch: 137] [Batch: 0501/0570] Loss: 0.21987  Avg Loss: 0.17067  Avg mIoU:  72.24  
[Epoch: 137] [Batch: 0551/0570] Loss: 0.13984  Avg Loss: 0.17053  Avg mIoU:  72.26  

*** Training [@Epoch 137] Avg Loss: 0.17018  Avg mIoU:  72.29  ***

[Epoch: 137] [Batch: 0001/0050] Loss: 0.13131  Avg Loss: 0.13131  Avg mIoU:  65.92  

*** Validation [@Epoch 137] Avg Loss: 0.16368  Avg mIoU:  65.27  ***

[Epoch: 138] [Batch: 0001/0570] Loss: 0.23697  Avg Loss: 0.23697  Avg mIoU:  43.39  
[Epoch: 138] [Batch: 0051/0570] Loss: 0.10300  Avg Loss: 0.17281  Avg mIoU:  71.65  
[Epoch: 138] [Batch: 0101/0570] Loss: 0.15292  Avg Loss: 0.17238  Avg mIoU:  71.94  
[Epoch: 138] [Batch: 0151/0570] Loss: 0.13433  Avg Loss: 0.17019  Avg mIoU:  72.36  
[Epoch: 138] [Batch: 0201/0570] Loss: 0.19413  Avg Loss: 0.17076  Avg mIoU:  71.90  
[Epoch: 138] [Batch: 0251/0570] Loss: 0.13606  Avg Loss: 0.17100  Avg mIoU:  71.69  
[Epoch: 138] [Batch: 0301/0570] Loss: 0.18059  Avg Loss: 0.17112  Avg mIoU:  71.77  
[Epoch: 138] [Batch: 0351/0570] Loss: 0.21355  Avg Loss: 0.17106  Avg mIoU:  71.88  
[Epoch: 138] [Batch: 0401/0570] Loss: 0.17052  Avg Loss: 0.17072  Avg mIoU:  72.02  
[Epoch: 138] [Batch: 0451/0570] Loss: 0.15061  Avg Loss: 0.16988  Avg mIoU:  72.16  
[Epoch: 138] [Batch: 0501/0570] Loss: 0.19135  Avg Loss: 0.17099  Avg mIoU:  72.04  
[Epoch: 138] [Batch: 0551/0570] Loss: 0.22202  Avg Loss: 0.17258  Avg mIoU:  71.81  

*** Training [@Epoch 138] Avg Loss: 0.17247  Avg mIoU:  71.88  ***

[Epoch: 138] [Batch: 0001/0050] Loss: 0.14382  Avg Loss: 0.14382  Avg mIoU:  62.56  

*** Validation [@Epoch 138] Avg Loss: 0.16309  Avg mIoU:  64.79  ***

[Epoch: 139] [Batch: 0001/0570] Loss: 0.15413  Avg Loss: 0.15413  Avg mIoU:  47.04  
[Epoch: 139] [Batch: 0051/0570] Loss: 0.17662  Avg Loss: 0.16669  Avg mIoU:  71.36  
[Epoch: 139] [Batch: 0101/0570] Loss: 0.14583  Avg Loss: 0.16615  Avg mIoU:  71.72  
[Epoch: 139] [Batch: 0151/0570] Loss: 0.12919  Avg Loss: 0.16753  Avg mIoU:  72.39  
[Epoch: 139] [Batch: 0201/0570] Loss: 0.20958  Avg Loss: 0.16864  Avg mIoU:  72.71  
[Epoch: 139] [Batch: 0251/0570] Loss: 0.16722  Avg Loss: 0.17102  Avg mIoU:  72.53  
[Epoch: 139] [Batch: 0301/0570] Loss: 0.16706  Avg Loss: 0.17160  Avg mIoU:  72.44  
[Epoch: 139] [Batch: 0351/0570] Loss: 0.16663  Avg Loss: 0.17019  Avg mIoU:  72.65  
[Epoch: 139] [Batch: 0401/0570] Loss: 0.19240  Avg Loss: 0.16972  Avg mIoU:  72.65  
[Epoch: 139] [Batch: 0451/0570] Loss: 0.14314  Avg Loss: 0.16971  Avg mIoU:  72.70  
[Epoch: 139] [Batch: 0501/0570] Loss: 0.17700  Avg Loss: 0.16929  Avg mIoU:  72.59  
[Epoch: 139] [Batch: 0551/0570] Loss: 0.11649  Avg Loss: 0.16883  Avg mIoU:  72.76  

*** Training [@Epoch 139] Avg Loss: 0.16901  Avg mIoU:  72.76  ***

[Epoch: 139] [Batch: 0001/0050] Loss: 0.13709  Avg Loss: 0.13709  Avg mIoU:  64.98  

*** Validation [@Epoch 139] Avg Loss: 0.16476  Avg mIoU:  63.76  ***

[Epoch: 140] [Batch: 0001/0570] Loss: 0.19520  Avg Loss: 0.19520  Avg mIoU:  46.69  
[Epoch: 140] [Batch: 0051/0570] Loss: 0.17697  Avg Loss: 0.18181  Avg mIoU:  72.26  
[Epoch: 140] [Batch: 0101/0570] Loss: 0.15038  Avg Loss: 0.17714  Avg mIoU:  72.65  
[Epoch: 140] [Batch: 0151/0570] Loss: 0.15279  Avg Loss: 0.17487  Avg mIoU:  72.37  
[Epoch: 140] [Batch: 0201/0570] Loss: 0.18355  Avg Loss: 0.17357  Avg mIoU:  72.26  
[Epoch: 140] [Batch: 0251/0570] Loss: 0.14917  Avg Loss: 0.17278  Avg mIoU:  71.96  
[Epoch: 140] [Batch: 0301/0570] Loss: 0.15570  Avg Loss: 0.17170  Avg mIoU:  71.97  
[Epoch: 140] [Batch: 0351/0570] Loss: 0.16141  Avg Loss: 0.17133  Avg mIoU:  71.92  
[Epoch: 140] [Batch: 0401/0570] Loss: 0.16289  Avg Loss: 0.17088  Avg mIoU:  71.98  
[Epoch: 140] [Batch: 0451/0570] Loss: 0.14500  Avg Loss: 0.17088  Avg mIoU:  72.00  
[Epoch: 140] [Batch: 0501/0570] Loss: 0.19832  Avg Loss: 0.17190  Avg mIoU:  72.03  
[Epoch: 140] [Batch: 0551/0570] Loss: 0.16612  Avg Loss: 0.17072  Avg mIoU:  72.16  

*** Training [@Epoch 140] Avg Loss: 0.17057  Avg mIoU:  72.20  ***

[Epoch: 140] [Batch: 0001/0050] Loss: 0.15464  Avg Loss: 0.15464  Avg mIoU:  60.46  

*** Validation [@Epoch 140] Avg Loss: 0.15806  Avg mIoU:  64.66  ***

[Epoch: 141] [Batch: 0001/0570] Loss: 0.12794  Avg Loss: 0.12794  Avg mIoU:  46.65  
[Epoch: 141] [Batch: 0051/0570] Loss: 0.12599  Avg Loss: 0.16920  Avg mIoU:  71.59  
[Epoch: 141] [Batch: 0101/0570] Loss: 0.14918  Avg Loss: 0.16245  Avg mIoU:  72.16  
[Epoch: 141] [Batch: 0151/0570] Loss: 0.15792  Avg Loss: 0.16639  Avg mIoU:  71.84  
[Epoch: 141] [Batch: 0201/0570] Loss: 0.11740  Avg Loss: 0.16951  Avg mIoU:  71.75  
[Epoch: 141] [Batch: 0251/0570] Loss: 0.12710  Avg Loss: 0.16787  Avg mIoU:  72.38  
[Epoch: 141] [Batch: 0301/0570] Loss: 0.17672  Avg Loss: 0.16908  Avg mIoU:  72.46  
[Epoch: 141] [Batch: 0351/0570] Loss: 0.16904  Avg Loss: 0.16951  Avg mIoU:  72.49  
[Epoch: 141] [Batch: 0401/0570] Loss: 0.13739  Avg Loss: 0.17052  Avg mIoU:  72.23  
[Epoch: 141] [Batch: 0451/0570] Loss: 0.13139  Avg Loss: 0.17010  Avg mIoU:  72.32  
[Epoch: 141] [Batch: 0501/0570] Loss: 0.12534  Avg Loss: 0.16994  Avg mIoU:  72.35  
[Epoch: 141] [Batch: 0551/0570] Loss: 0.13857  Avg Loss: 0.16994  Avg mIoU:  72.45  

*** Training [@Epoch 141] Avg Loss: 0.16988  Avg mIoU:  72.45  ***

[Epoch: 141] [Batch: 0001/0050] Loss: 0.13736  Avg Loss: 0.13736  Avg mIoU:  63.64  

*** Validation [@Epoch 141] Avg Loss: 0.15216  Avg mIoU:  62.98  ***

[Epoch: 142] [Batch: 0001/0570] Loss: 0.14847  Avg Loss: 0.14847  Avg mIoU:  41.78  
[Epoch: 142] [Batch: 0051/0570] Loss: 0.26006  Avg Loss: 0.16163  Avg mIoU:  74.48  
[Epoch: 142] [Batch: 0101/0570] Loss: 0.16958  Avg Loss: 0.16644  Avg mIoU:  73.10  
[Epoch: 142] [Batch: 0151/0570] Loss: 0.15920  Avg Loss: 0.17119  Avg mIoU:  72.39  
[Epoch: 142] [Batch: 0201/0570] Loss: 0.20826  Avg Loss: 0.16894  Avg mIoU:  72.36  
[Epoch: 142] [Batch: 0251/0570] Loss: 0.15309  Avg Loss: 0.16859  Avg mIoU:  72.63  
[Epoch: 142] [Batch: 0301/0570] Loss: 0.15225  Avg Loss: 0.16879  Avg mIoU:  72.57  
[Epoch: 142] [Batch: 0351/0570] Loss: 0.17227  Avg Loss: 0.16966  Avg mIoU:  72.52  
[Epoch: 142] [Batch: 0401/0570] Loss: 0.16614  Avg Loss: 0.16938  Avg mIoU:  72.43  
[Epoch: 142] [Batch: 0451/0570] Loss: 0.15294  Avg Loss: 0.16934  Avg mIoU:  72.55  
[Epoch: 142] [Batch: 0501/0570] Loss: 0.18110  Avg Loss: 0.17044  Avg mIoU:  72.17  
[Epoch: 142] [Batch: 0551/0570] Loss: 0.18305  Avg Loss: 0.17043  Avg mIoU:  72.15  

*** Training [@Epoch 142] Avg Loss: 0.17062  Avg mIoU:  72.09  ***

[Epoch: 142] [Batch: 0001/0050] Loss: 0.12901  Avg Loss: 0.12901  Avg mIoU:  68.58  

*** Validation [@Epoch 142] Avg Loss: 0.16405  Avg mIoU:  64.76  ***

[Epoch: 143] [Batch: 0001/0570] Loss: 0.13199  Avg Loss: 0.13199  Avg mIoU:  27.92  
[Epoch: 143] [Batch: 0051/0570] Loss: 0.21893  Avg Loss: 0.16861  Avg mIoU:  72.81  
[Epoch: 143] [Batch: 0101/0570] Loss: 0.13768  Avg Loss: 0.16635  Avg mIoU:  72.08  
[Epoch: 143] [Batch: 0151/0570] Loss: 0.16713  Avg Loss: 0.16756  Avg mIoU:  72.27  
[Epoch: 143] [Batch: 0201/0570] Loss: 0.21402  Avg Loss: 0.16864  Avg mIoU:  72.17  
[Epoch: 143] [Batch: 0251/0570] Loss: 0.15723  Avg Loss: 0.16938  Avg mIoU:  72.10  
[Epoch: 143] [Batch: 0301/0570] Loss: 0.17581  Avg Loss: 0.16898  Avg mIoU:  72.10  
[Epoch: 143] [Batch: 0351/0570] Loss: 0.17721  Avg Loss: 0.16991  Avg mIoU:  72.08  
[Epoch: 143] [Batch: 0401/0570] Loss: 0.17887  Avg Loss: 0.17068  Avg mIoU:  71.97  
[Epoch: 143] [Batch: 0451/0570] Loss: 0.12124  Avg Loss: 0.17056  Avg mIoU:  72.19  
[Epoch: 143] [Batch: 0501/0570] Loss: 0.19024  Avg Loss: 0.17050  Avg mIoU:  72.02  
[Epoch: 143] [Batch: 0551/0570] Loss: 0.16976  Avg Loss: 0.16966  Avg mIoU:  72.07  

*** Training [@Epoch 143] Avg Loss: 0.17010  Avg mIoU:  72.04  ***

[Epoch: 143] [Batch: 0001/0050] Loss: 0.13544  Avg Loss: 0.13544  Avg mIoU:  65.11  

*** Validation [@Epoch 143] Avg Loss: 0.15810  Avg mIoU:  64.07  ***

[Epoch: 144] [Batch: 0001/0570] Loss: 0.26389  Avg Loss: 0.26389  Avg mIoU:  43.58  
[Epoch: 144] [Batch: 0051/0570] Loss: 0.19632  Avg Loss: 0.17876  Avg mIoU:  70.71  
[Epoch: 144] [Batch: 0101/0570] Loss: 0.12884  Avg Loss: 0.17260  Avg mIoU:  71.56  
[Epoch: 144] [Batch: 0151/0570] Loss: 0.26758  Avg Loss: 0.16925  Avg mIoU:  72.52  
[Epoch: 144] [Batch: 0201/0570] Loss: 0.18760  Avg Loss: 0.17017  Avg mIoU:  72.39  
[Epoch: 144] [Batch: 0251/0570] Loss: 0.17154  Avg Loss: 0.17068  Avg mIoU:  72.12  
[Epoch: 144] [Batch: 0301/0570] Loss: 0.16177  Avg Loss: 0.17158  Avg mIoU:  72.04  
[Epoch: 144] [Batch: 0351/0570] Loss: 0.18424  Avg Loss: 0.17126  Avg mIoU:  72.01  
[Epoch: 144] [Batch: 0401/0570] Loss: 0.17930  Avg Loss: 0.17061  Avg mIoU:  72.02  
[Epoch: 144] [Batch: 0451/0570] Loss: 0.17422  Avg Loss: 0.17058  Avg mIoU:  72.10  
[Epoch: 144] [Batch: 0501/0570] Loss: 0.22229  Avg Loss: 0.17210  Avg mIoU:  72.08  
[Epoch: 144] [Batch: 0551/0570] Loss: 0.10652  Avg Loss: 0.17149  Avg mIoU:  72.12  

*** Training [@Epoch 144] Avg Loss: 0.17115  Avg mIoU:  72.12  ***

[Epoch: 144] [Batch: 0001/0050] Loss: 0.12346  Avg Loss: 0.12346  Avg mIoU:  65.57  

*** Validation [@Epoch 144] Avg Loss: 0.15468  Avg mIoU:  65.96  ***

[Epoch: 145] [Batch: 0001/0570] Loss: 0.12915  Avg Loss: 0.12915  Avg mIoU:  42.14  
[Epoch: 145] [Batch: 0051/0570] Loss: 0.17641  Avg Loss: 0.16116  Avg mIoU:  72.58  
[Epoch: 145] [Batch: 0101/0570] Loss: 0.26315  Avg Loss: 0.16685  Avg mIoU:  72.09  
[Epoch: 145] [Batch: 0151/0570] Loss: 0.13885  Avg Loss: 0.16394  Avg mIoU:  73.24  
[Epoch: 145] [Batch: 0201/0570] Loss: 0.11268  Avg Loss: 0.16352  Avg mIoU:  73.38  
[Epoch: 145] [Batch: 0251/0570] Loss: 0.16510  Avg Loss: 0.16437  Avg mIoU:  73.15  
[Epoch: 145] [Batch: 0301/0570] Loss: 0.20250  Avg Loss: 0.16551  Avg mIoU:  73.09  
[Epoch: 145] [Batch: 0351/0570] Loss: 0.11046  Avg Loss: 0.16759  Avg mIoU:  72.63  
[Epoch: 145] [Batch: 0401/0570] Loss: 0.23282  Avg Loss: 0.16675  Avg mIoU:  72.42  
[Epoch: 145] [Batch: 0451/0570] Loss: 0.18925  Avg Loss: 0.16687  Avg mIoU:  72.40  
[Epoch: 145] [Batch: 0501/0570] Loss: 0.25616  Avg Loss: 0.16748  Avg mIoU:  72.36  
[Epoch: 145] [Batch: 0551/0570] Loss: 0.14082  Avg Loss: 0.16793  Avg mIoU:  72.62  

*** Training [@Epoch 145] Avg Loss: 0.16844  Avg mIoU:  72.56  ***

[Epoch: 145] [Batch: 0001/0050] Loss: 0.14428  Avg Loss: 0.14428  Avg mIoU:  64.37  

*** Validation [@Epoch 145] Avg Loss: 0.16303  Avg mIoU:  65.38  ***

[Epoch: 146] [Batch: 0001/0570] Loss: 0.16504  Avg Loss: 0.16504  Avg mIoU:  41.00  
[Epoch: 146] [Batch: 0051/0570] Loss: 0.12067  Avg Loss: 0.16877  Avg mIoU:  72.73  
[Epoch: 146] [Batch: 0101/0570] Loss: 0.22266  Avg Loss: 0.16860  Avg mIoU:  72.64  
[Epoch: 146] [Batch: 0151/0570] Loss: 0.25840  Avg Loss: 0.17030  Avg mIoU:  72.29  
[Epoch: 146] [Batch: 0201/0570] Loss: 0.16302  Avg Loss: 0.17049  Avg mIoU:  72.61  
[Epoch: 146] [Batch: 0251/0570] Loss: 0.16841  Avg Loss: 0.17061  Avg mIoU:  72.72  
[Epoch: 146] [Batch: 0301/0570] Loss: 0.16522  Avg Loss: 0.17081  Avg mIoU:  72.78  
[Epoch: 146] [Batch: 0351/0570] Loss: 0.19727  Avg Loss: 0.17070  Avg mIoU:  72.48  
[Epoch: 146] [Batch: 0401/0570] Loss: 0.20454  Avg Loss: 0.16926  Avg mIoU:  72.53  
[Epoch: 146] [Batch: 0451/0570] Loss: 0.15417  Avg Loss: 0.16866  Avg mIoU:  72.46  
[Epoch: 146] [Batch: 0501/0570] Loss: 0.16253  Avg Loss: 0.16926  Avg mIoU:  72.43  
[Epoch: 146] [Batch: 0551/0570] Loss: 0.20965  Avg Loss: 0.16929  Avg mIoU:  72.43  

*** Training [@Epoch 146] Avg Loss: 0.16926  Avg mIoU:  72.59  ***

[Epoch: 146] [Batch: 0001/0050] Loss: 0.13972  Avg Loss: 0.13972  Avg mIoU:  66.01  

*** Validation [@Epoch 146] Avg Loss: 0.17104  Avg mIoU:  65.66  ***

[Epoch: 147] [Batch: 0001/0570] Loss: 0.23629  Avg Loss: 0.23629  Avg mIoU:  37.78  
[Epoch: 147] [Batch: 0051/0570] Loss: 0.11284  Avg Loss: 0.16783  Avg mIoU:  70.77  
[Epoch: 147] [Batch: 0101/0570] Loss: 0.16787  Avg Loss: 0.16811  Avg mIoU:  71.83  
[Epoch: 147] [Batch: 0151/0570] Loss: 0.16839  Avg Loss: 0.16513  Avg mIoU:  72.91  
[Epoch: 147] [Batch: 0201/0570] Loss: 0.17969  Avg Loss: 0.16650  Avg mIoU:  72.66  
[Epoch: 147] [Batch: 0251/0570] Loss: 0.16091  Avg Loss: 0.16599  Avg mIoU:  72.60  
[Epoch: 147] [Batch: 0301/0570] Loss: 0.13588  Avg Loss: 0.16716  Avg mIoU:  72.69  
[Epoch: 147] [Batch: 0351/0570] Loss: 0.17449  Avg Loss: 0.16599  Avg mIoU:  72.63  
[Epoch: 147] [Batch: 0401/0570] Loss: 0.14300  Avg Loss: 0.16603  Avg mIoU:  72.82  
[Epoch: 147] [Batch: 0451/0570] Loss: 0.16740  Avg Loss: 0.16701  Avg mIoU:  72.64  
[Epoch: 147] [Batch: 0501/0570] Loss: 0.13734  Avg Loss: 0.16680  Avg mIoU:  72.79  
[Epoch: 147] [Batch: 0551/0570] Loss: 0.10736  Avg Loss: 0.16748  Avg mIoU:  72.71  

*** Training [@Epoch 147] Avg Loss: 0.16775  Avg mIoU:  72.62  ***

[Epoch: 147] [Batch: 0001/0050] Loss: 0.14308  Avg Loss: 0.14308  Avg mIoU:  62.65  

*** Validation [@Epoch 147] Avg Loss: 0.16969  Avg mIoU:  65.20  ***

[Epoch: 148] [Batch: 0001/0570] Loss: 0.16464  Avg Loss: 0.16464  Avg mIoU:  39.02  
[Epoch: 148] [Batch: 0051/0570] Loss: 0.10639  Avg Loss: 0.16837  Avg mIoU:  71.49  
[Epoch: 148] [Batch: 0101/0570] Loss: 0.17337  Avg Loss: 0.17188  Avg mIoU:  71.99  
[Epoch: 148] [Batch: 0151/0570] Loss: 0.16642  Avg Loss: 0.17254  Avg mIoU:  72.07  
[Epoch: 148] [Batch: 0201/0570] Loss: 0.09613  Avg Loss: 0.16922  Avg mIoU:  73.19  
[Epoch: 148] [Batch: 0251/0570] Loss: 0.14523  Avg Loss: 0.16873  Avg mIoU:  73.11  
[Epoch: 148] [Batch: 0301/0570] Loss: 0.17377  Avg Loss: 0.16893  Avg mIoU:  73.03  
[Epoch: 148] [Batch: 0351/0570] Loss: 0.15432  Avg Loss: 0.16821  Avg mIoU:  73.20  
[Epoch: 148] [Batch: 0401/0570] Loss: 0.17024  Avg Loss: 0.16763  Avg mIoU:  73.25  
[Epoch: 148] [Batch: 0451/0570] Loss: 0.18712  Avg Loss: 0.16674  Avg mIoU:  73.14  
[Epoch: 148] [Batch: 0501/0570] Loss: 0.25618  Avg Loss: 0.16730  Avg mIoU:  72.88  
[Epoch: 148] [Batch: 0551/0570] Loss: 0.15476  Avg Loss: 0.16715  Avg mIoU:  72.81  

*** Training [@Epoch 148] Avg Loss: 0.16744  Avg mIoU:  72.84  ***

[Epoch: 148] [Batch: 0001/0050] Loss: 0.13964  Avg Loss: 0.13964  Avg mIoU:  64.70  

*** Validation [@Epoch 148] Avg Loss: 0.15985  Avg mIoU:  64.63  ***

[Epoch: 149] [Batch: 0001/0570] Loss: 0.25449  Avg Loss: 0.25449  Avg mIoU:  34.32  
[Epoch: 149] [Batch: 0051/0570] Loss: 0.16734  Avg Loss: 0.16958  Avg mIoU:  71.98  
[Epoch: 149] [Batch: 0101/0570] Loss: 0.21073  Avg Loss: 0.17177  Avg mIoU:  72.09  
[Epoch: 149] [Batch: 0151/0570] Loss: 0.17869  Avg Loss: 0.16856  Avg mIoU:  72.26  
[Epoch: 149] [Batch: 0201/0570] Loss: 0.16668  Avg Loss: 0.16805  Avg mIoU:  72.32  
[Epoch: 149] [Batch: 0251/0570] Loss: 0.18781  Avg Loss: 0.16703  Avg mIoU:  72.49  
[Epoch: 149] [Batch: 0301/0570] Loss: 0.12787  Avg Loss: 0.16721  Avg mIoU:  72.62  
[Epoch: 149] [Batch: 0351/0570] Loss: 0.14483  Avg Loss: 0.16675  Avg mIoU:  72.65  
[Epoch: 149] [Batch: 0401/0570] Loss: 0.14745  Avg Loss: 0.16607  Avg mIoU:  72.58  
[Epoch: 149] [Batch: 0451/0570] Loss: 0.08991  Avg Loss: 0.16665  Avg mIoU:  72.58  
[Epoch: 149] [Batch: 0501/0570] Loss: 0.16645  Avg Loss: 0.16768  Avg mIoU:  72.49  
[Epoch: 149] [Batch: 0551/0570] Loss: 0.13424  Avg Loss: 0.16713  Avg mIoU:  72.52  

*** Training [@Epoch 149] Avg Loss: 0.16696  Avg mIoU:  72.59  ***

[Epoch: 149] [Batch: 0001/0050] Loss: 0.17703  Avg Loss: 0.17703  Avg mIoU:  63.75  

*** Validation [@Epoch 149] Avg Loss: 0.18054  Avg mIoU:  64.59  ***

[Epoch: 150] [Batch: 0001/0570] Loss: 0.17369  Avg Loss: 0.17369  Avg mIoU:  50.53  
[Epoch: 150] [Batch: 0051/0570] Loss: 0.18790  Avg Loss: 0.17021  Avg mIoU:  71.74  
[Epoch: 150] [Batch: 0101/0570] Loss: 0.18214  Avg Loss: 0.16923  Avg mIoU:  71.58  
[Epoch: 150] [Batch: 0151/0570] Loss: 0.14415  Avg Loss: 0.16738  Avg mIoU:  71.66  
[Epoch: 150] [Batch: 0201/0570] Loss: 0.10692  Avg Loss: 0.16682  Avg mIoU:  71.68  
[Epoch: 150] [Batch: 0251/0570] Loss: 0.18568  Avg Loss: 0.16746  Avg mIoU:  71.63  
[Epoch: 150] [Batch: 0301/0570] Loss: 0.18109  Avg Loss: 0.16753  Avg mIoU:  71.84  
[Epoch: 150] [Batch: 0351/0570] Loss: 0.15686  Avg Loss: 0.16742  Avg mIoU:  72.13  
[Epoch: 150] [Batch: 0401/0570] Loss: 0.26060  Avg Loss: 0.16795  Avg mIoU:  72.25  
[Epoch: 150] [Batch: 0451/0570] Loss: 0.19174  Avg Loss: 0.16713  Avg mIoU:  72.35  
[Epoch: 150] [Batch: 0501/0570] Loss: 0.16741  Avg Loss: 0.16661  Avg mIoU:  72.46  
[Epoch: 150] [Batch: 0551/0570] Loss: 0.13816  Avg Loss: 0.16714  Avg mIoU:  72.53  

*** Training [@Epoch 150] Avg Loss: 0.16711  Avg mIoU:  72.54  ***

[Epoch: 150] [Batch: 0001/0050] Loss: 0.17597  Avg Loss: 0.17597  Avg mIoU:  62.11  

*** Validation [@Epoch 150] Avg Loss: 0.19059  Avg mIoU:  66.23  ***

[Epoch: 151] [Batch: 0001/0570] Loss: 0.14679  Avg Loss: 0.14679  Avg mIoU:  32.44  
[Epoch: 151] [Batch: 0051/0570] Loss: 0.15847  Avg Loss: 0.15834  Avg mIoU:  73.92  
[Epoch: 151] [Batch: 0101/0570] Loss: 0.16804  Avg Loss: 0.16384  Avg mIoU:  73.21  
[Epoch: 151] [Batch: 0151/0570] Loss: 0.21332  Avg Loss: 0.16517  Avg mIoU:  72.94  
[Epoch: 151] [Batch: 0201/0570] Loss: 0.15527  Avg Loss: 0.16723  Avg mIoU:  72.61  
[Epoch: 151] [Batch: 0251/0570] Loss: 0.13219  Avg Loss: 0.16652  Avg mIoU:  72.63  
[Epoch: 151] [Batch: 0301/0570] Loss: 0.15149  Avg Loss: 0.16743  Avg mIoU:  72.55  
[Epoch: 151] [Batch: 0351/0570] Loss: 0.18784  Avg Loss: 0.16623  Avg mIoU:  72.50  
[Epoch: 151] [Batch: 0401/0570] Loss: 0.19670  Avg Loss: 0.16699  Avg mIoU:  72.50  
[Epoch: 151] [Batch: 0451/0570] Loss: 0.13925  Avg Loss: 0.16766  Avg mIoU:  72.50  
[Epoch: 151] [Batch: 0501/0570] Loss: 0.12855  Avg Loss: 0.16835  Avg mIoU:  72.57  
[Epoch: 151] [Batch: 0551/0570] Loss: 0.21012  Avg Loss: 0.16868  Avg mIoU:  72.49  

*** Training [@Epoch 151] Avg Loss: 0.16805  Avg mIoU:  72.50  ***

[Epoch: 151] [Batch: 0001/0050] Loss: 0.14556  Avg Loss: 0.14556  Avg mIoU:  63.03  

*** Validation [@Epoch 151] Avg Loss: 0.16055  Avg mIoU:  65.31  ***

[Epoch: 152] [Batch: 0001/0570] Loss: 0.15681  Avg Loss: 0.15681  Avg mIoU:  41.70  
[Epoch: 152] [Batch: 0051/0570] Loss: 0.22782  Avg Loss: 0.16423  Avg mIoU:  73.24  
[Epoch: 152] [Batch: 0101/0570] Loss: 0.18446  Avg Loss: 0.16604  Avg mIoU:  73.20  
[Epoch: 152] [Batch: 0151/0570] Loss: 0.12880  Avg Loss: 0.16552  Avg mIoU:  72.62  
[Epoch: 152] [Batch: 0201/0570] Loss: 0.14020  Avg Loss: 0.16381  Avg mIoU:  72.91  
[Epoch: 152] [Batch: 0251/0570] Loss: 0.19965  Avg Loss: 0.16367  Avg mIoU:  72.83  
[Epoch: 152] [Batch: 0301/0570] Loss: 0.16751  Avg Loss: 0.16325  Avg mIoU:  72.80  
[Epoch: 152] [Batch: 0351/0570] Loss: 0.15395  Avg Loss: 0.16311  Avg mIoU:  73.12  
[Epoch: 152] [Batch: 0401/0570] Loss: 0.18883  Avg Loss: 0.16442  Avg mIoU:  73.02  
[Epoch: 152] [Batch: 0451/0570] Loss: 0.17616  Avg Loss: 0.16436  Avg mIoU:  73.01  
[Epoch: 152] [Batch: 0501/0570] Loss: 0.14589  Avg Loss: 0.16579  Avg mIoU:  72.98  
[Epoch: 152] [Batch: 0551/0570] Loss: 0.14416  Avg Loss: 0.16594  Avg mIoU:  72.85  

*** Training [@Epoch 152] Avg Loss: 0.16573  Avg mIoU:  72.78  ***

[Epoch: 152] [Batch: 0001/0050] Loss: 0.14206  Avg Loss: 0.14206  Avg mIoU:  61.38  

*** Validation [@Epoch 152] Avg Loss: 0.16304  Avg mIoU:  64.89  ***

[Epoch: 153] [Batch: 0001/0570] Loss: 0.13987  Avg Loss: 0.13987  Avg mIoU:  37.24  
[Epoch: 153] [Batch: 0051/0570] Loss: 0.13993  Avg Loss: 0.16821  Avg mIoU:  72.39  
[Epoch: 153] [Batch: 0101/0570] Loss: 0.18757  Avg Loss: 0.16755  Avg mIoU:  73.12  
[Epoch: 153] [Batch: 0151/0570] Loss: 0.14799  Avg Loss: 0.16532  Avg mIoU:  72.55  
[Epoch: 153] [Batch: 0201/0570] Loss: 0.14189  Avg Loss: 0.16527  Avg mIoU:  72.53  
[Epoch: 153] [Batch: 0251/0570] Loss: 0.17690  Avg Loss: 0.16569  Avg mIoU:  72.35  
[Epoch: 153] [Batch: 0301/0570] Loss: 0.20296  Avg Loss: 0.16707  Avg mIoU:  72.31  
[Epoch: 153] [Batch: 0351/0570] Loss: 0.17338  Avg Loss: 0.16679  Avg mIoU:  72.42  
[Epoch: 153] [Batch: 0401/0570] Loss: 0.13618  Avg Loss: 0.16693  Avg mIoU:  72.59  
[Epoch: 153] [Batch: 0451/0570] Loss: 0.10737  Avg Loss: 0.16645  Avg mIoU:  72.64  
[Epoch: 153] [Batch: 0501/0570] Loss: 0.14102  Avg Loss: 0.16645  Avg mIoU:  72.58  
[Epoch: 153] [Batch: 0551/0570] Loss: 0.13601  Avg Loss: 0.16614  Avg mIoU:  72.67  

*** Training [@Epoch 153] Avg Loss: 0.16661  Avg mIoU:  72.70  ***

[Epoch: 153] [Batch: 0001/0050] Loss: 0.17885  Avg Loss: 0.17885  Avg mIoU:  63.32  

*** Validation [@Epoch 153] Avg Loss: 0.19174  Avg mIoU:  62.48  ***

[Epoch: 154] [Batch: 0001/0570] Loss: 0.15077  Avg Loss: 0.15077  Avg mIoU:  59.19  
[Epoch: 154] [Batch: 0051/0570] Loss: 0.15082  Avg Loss: 0.16768  Avg mIoU:  72.11  
[Epoch: 154] [Batch: 0101/0570] Loss: 0.09633  Avg Loss: 0.16315  Avg mIoU:  72.95  
[Epoch: 154] [Batch: 0151/0570] Loss: 0.12958  Avg Loss: 0.16364  Avg mIoU:  72.76  
[Epoch: 154] [Batch: 0201/0570] Loss: 0.17667  Avg Loss: 0.16409  Avg mIoU:  72.81  
[Epoch: 154] [Batch: 0251/0570] Loss: 0.15348  Avg Loss: 0.16632  Avg mIoU:  72.68  
[Epoch: 154] [Batch: 0301/0570] Loss: 0.12814  Avg Loss: 0.16575  Avg mIoU:  72.63  
[Epoch: 154] [Batch: 0351/0570] Loss: 0.16472  Avg Loss: 0.16611  Avg mIoU:  72.88  
[Epoch: 154] [Batch: 0401/0570] Loss: 0.18128  Avg Loss: 0.16709  Avg mIoU:  72.77  
[Epoch: 154] [Batch: 0451/0570] Loss: 0.09620  Avg Loss: 0.16639  Avg mIoU:  72.95  
[Epoch: 154] [Batch: 0501/0570] Loss: 0.13064  Avg Loss: 0.16638  Avg mIoU:  72.86  
[Epoch: 154] [Batch: 0551/0570] Loss: 0.16463  Avg Loss: 0.16691  Avg mIoU:  72.75  

*** Training [@Epoch 154] Avg Loss: 0.16718  Avg mIoU:  72.75  ***

[Epoch: 154] [Batch: 0001/0050] Loss: 0.14856  Avg Loss: 0.14856  Avg mIoU:  64.80  

*** Validation [@Epoch 154] Avg Loss: 0.17857  Avg mIoU:  64.88  ***

[Epoch: 155] [Batch: 0001/0570] Loss: 0.12758  Avg Loss: 0.12758  Avg mIoU:  42.63  
[Epoch: 155] [Batch: 0051/0570] Loss: 0.16281  Avg Loss: 0.16567  Avg mIoU:  72.52  
[Epoch: 155] [Batch: 0101/0570] Loss: 0.18680  Avg Loss: 0.16487  Avg mIoU:  72.97  
[Epoch: 155] [Batch: 0151/0570] Loss: 0.12980  Avg Loss: 0.16408  Avg mIoU:  73.04  
[Epoch: 155] [Batch: 0201/0570] Loss: 0.18103  Avg Loss: 0.16454  Avg mIoU:  73.35  
[Epoch: 155] [Batch: 0251/0570] Loss: 0.13516  Avg Loss: 0.16406  Avg mIoU:  73.39  
[Epoch: 155] [Batch: 0301/0570] Loss: 0.12312  Avg Loss: 0.16390  Avg mIoU:  73.25  
[Epoch: 155] [Batch: 0351/0570] Loss: 0.17492  Avg Loss: 0.16685  Avg mIoU:  72.88  
[Epoch: 155] [Batch: 0401/0570] Loss: 0.12094  Avg Loss: 0.16815  Avg mIoU:  72.35  
[Epoch: 155] [Batch: 0451/0570] Loss: 0.14602  Avg Loss: 0.16828  Avg mIoU:  72.53  
[Epoch: 155] [Batch: 0501/0570] Loss: 0.33436  Avg Loss: 0.16782  Avg mIoU:  72.78  
[Epoch: 155] [Batch: 0551/0570] Loss: 0.11996  Avg Loss: 0.16710  Avg mIoU:  72.79  

*** Training [@Epoch 155] Avg Loss: 0.16699  Avg mIoU:  72.81  ***

[Epoch: 155] [Batch: 0001/0050] Loss: 0.13947  Avg Loss: 0.13947  Avg mIoU:  64.27  

*** Validation [@Epoch 155] Avg Loss: 0.15836  Avg mIoU:  64.65  ***

[Epoch: 156] [Batch: 0001/0570] Loss: 0.11475  Avg Loss: 0.11475  Avg mIoU:  44.26  
[Epoch: 156] [Batch: 0051/0570] Loss: 0.22973  Avg Loss: 0.16188  Avg mIoU:  73.16  
[Epoch: 156] [Batch: 0101/0570] Loss: 0.12450  Avg Loss: 0.16342  Avg mIoU:  72.89  
[Epoch: 156] [Batch: 0151/0570] Loss: 0.10454  Avg Loss: 0.16278  Avg mIoU:  73.17  
[Epoch: 156] [Batch: 0201/0570] Loss: 0.19047  Avg Loss: 0.16001  Avg mIoU:  73.48  
[Epoch: 156] [Batch: 0251/0570] Loss: 0.23124  Avg Loss: 0.16127  Avg mIoU:  73.39  
[Epoch: 156] [Batch: 0301/0570] Loss: 0.13717  Avg Loss: 0.16168  Avg mIoU:  73.38  
[Epoch: 156] [Batch: 0351/0570] Loss: 0.15317  Avg Loss: 0.16428  Avg mIoU:  73.20  
[Epoch: 156] [Batch: 0401/0570] Loss: 0.17998  Avg Loss: 0.16522  Avg mIoU:  73.10  
[Epoch: 156] [Batch: 0451/0570] Loss: 0.17899  Avg Loss: 0.16462  Avg mIoU:  73.13  
[Epoch: 156] [Batch: 0501/0570] Loss: 0.18424  Avg Loss: 0.16541  Avg mIoU:  72.81  
[Epoch: 156] [Batch: 0551/0570] Loss: 0.18010  Avg Loss: 0.16713  Avg mIoU:  72.63  

*** Training [@Epoch 156] Avg Loss: 0.16722  Avg mIoU:  72.64  ***

[Epoch: 156] [Batch: 0001/0050] Loss: 0.12491  Avg Loss: 0.12491  Avg mIoU:  64.16  

*** Validation [@Epoch 156] Avg Loss: 0.14857  Avg mIoU:  63.90  ***

[Epoch: 157] [Batch: 0001/0570] Loss: 0.16965  Avg Loss: 0.16965  Avg mIoU:  31.45  
[Epoch: 157] [Batch: 0051/0570] Loss: 0.20394  Avg Loss: 0.15916  Avg mIoU:  72.03  
[Epoch: 157] [Batch: 0101/0570] Loss: 0.13075  Avg Loss: 0.15862  Avg mIoU:  73.26  
[Epoch: 157] [Batch: 0151/0570] Loss: 0.25326  Avg Loss: 0.16268  Avg mIoU:  73.39  
[Epoch: 157] [Batch: 0201/0570] Loss: 0.15173  Avg Loss: 0.16417  Avg mIoU:  73.32  
[Epoch: 157] [Batch: 0251/0570] Loss: 0.13167  Avg Loss: 0.16370  Avg mIoU:  73.25  
[Epoch: 157] [Batch: 0301/0570] Loss: 0.15724  Avg Loss: 0.16667  Avg mIoU:  73.27  
[Epoch: 157] [Batch: 0351/0570] Loss: 0.13910  Avg Loss: 0.16635  Avg mIoU:  73.24  
[Epoch: 157] [Batch: 0401/0570] Loss: 0.12381  Avg Loss: 0.16512  Avg mIoU:  73.39  
[Epoch: 157] [Batch: 0451/0570] Loss: 0.15803  Avg Loss: 0.16510  Avg mIoU:  73.14  
[Epoch: 157] [Batch: 0501/0570] Loss: 0.12913  Avg Loss: 0.16497  Avg mIoU:  73.26  
[Epoch: 157] [Batch: 0551/0570] Loss: 0.11598  Avg Loss: 0.16493  Avg mIoU:  73.13  

*** Training [@Epoch 157] Avg Loss: 0.16496  Avg mIoU:  73.11  ***

[Epoch: 157] [Batch: 0001/0050] Loss: 0.14490  Avg Loss: 0.14490  Avg mIoU:  66.58  

*** Validation [@Epoch 157] Avg Loss: 0.16782  Avg mIoU:  64.89  ***

[Epoch: 158] [Batch: 0001/0570] Loss: 0.19058  Avg Loss: 0.19058  Avg mIoU:  48.40  
[Epoch: 158] [Batch: 0051/0570] Loss: 0.14776  Avg Loss: 0.16507  Avg mIoU:  70.98  
[Epoch: 158] [Batch: 0101/0570] Loss: 0.13978  Avg Loss: 0.15770  Avg mIoU:  72.53  
[Epoch: 158] [Batch: 0151/0570] Loss: 0.18168  Avg Loss: 0.16254  Avg mIoU:  72.91  
[Epoch: 158] [Batch: 0201/0570] Loss: 0.19459  Avg Loss: 0.16405  Avg mIoU:  72.74  
[Epoch: 158] [Batch: 0251/0570] Loss: 0.15897  Avg Loss: 0.16348  Avg mIoU:  72.50  
[Epoch: 158] [Batch: 0301/0570] Loss: 0.13060  Avg Loss: 0.16441  Avg mIoU:  72.69  
[Epoch: 158] [Batch: 0351/0570] Loss: 0.12741  Avg Loss: 0.16336  Avg mIoU:  73.01  
[Epoch: 158] [Batch: 0401/0570] Loss: 0.11833  Avg Loss: 0.16343  Avg mIoU:  72.89  
[Epoch: 158] [Batch: 0451/0570] Loss: 0.13113  Avg Loss: 0.16347  Avg mIoU:  72.84  
[Epoch: 158] [Batch: 0501/0570] Loss: 0.17732  Avg Loss: 0.16369  Avg mIoU:  72.95  
[Epoch: 158] [Batch: 0551/0570] Loss: 0.12920  Avg Loss: 0.16414  Avg mIoU:  72.88  

*** Training [@Epoch 158] Avg Loss: 0.16415  Avg mIoU:  72.89  ***

[Epoch: 158] [Batch: 0001/0050] Loss: 0.12905  Avg Loss: 0.12905  Avg mIoU:  66.41  

*** Validation [@Epoch 158] Avg Loss: 0.16022  Avg mIoU:  65.83  ***

[Epoch: 159] [Batch: 0001/0570] Loss: 0.14211  Avg Loss: 0.14211  Avg mIoU:  44.07  
[Epoch: 159] [Batch: 0051/0570] Loss: 0.17911  Avg Loss: 0.16965  Avg mIoU:  72.32  
[Epoch: 159] [Batch: 0101/0570] Loss: 0.12726  Avg Loss: 0.16515  Avg mIoU:  71.60  
[Epoch: 159] [Batch: 0151/0570] Loss: 0.18107  Avg Loss: 0.16214  Avg mIoU:  71.77  
[Epoch: 159] [Batch: 0201/0570] Loss: 0.20936  Avg Loss: 0.16507  Avg mIoU:  71.59  
[Epoch: 159] [Batch: 0251/0570] Loss: 0.13326  Avg Loss: 0.16516  Avg mIoU:  72.22  
[Epoch: 159] [Batch: 0301/0570] Loss: 0.23150  Avg Loss: 0.16546  Avg mIoU:  72.75  
[Epoch: 159] [Batch: 0351/0570] Loss: 0.18386  Avg Loss: 0.16456  Avg mIoU:  73.28  
[Epoch: 159] [Batch: 0401/0570] Loss: 0.11059  Avg Loss: 0.16381  Avg mIoU:  73.42  
[Epoch: 159] [Batch: 0451/0570] Loss: 0.14100  Avg Loss: 0.16344  Avg mIoU:  73.21  
[Epoch: 159] [Batch: 0501/0570] Loss: 0.15040  Avg Loss: 0.16356  Avg mIoU:  73.11  
[Epoch: 159] [Batch: 0551/0570] Loss: 0.25000  Avg Loss: 0.16380  Avg mIoU:  73.07  

*** Training [@Epoch 159] Avg Loss: 0.16418  Avg mIoU:  72.97  ***

[Epoch: 159] [Batch: 0001/0050] Loss: 0.13352  Avg Loss: 0.13352  Avg mIoU:  66.32  

*** Validation [@Epoch 159] Avg Loss: 0.16024  Avg mIoU:  65.97  ***

[Epoch: 160] [Batch: 0001/0570] Loss: 0.13373  Avg Loss: 0.13373  Avg mIoU:  51.09  
[Epoch: 160] [Batch: 0051/0570] Loss: 0.17431  Avg Loss: 0.16144  Avg mIoU:  74.29  
[Epoch: 160] [Batch: 0101/0570] Loss: 0.20283  Avg Loss: 0.16528  Avg mIoU:  72.94  
[Epoch: 160] [Batch: 0151/0570] Loss: 0.13156  Avg Loss: 0.16596  Avg mIoU:  72.66  
[Epoch: 160] [Batch: 0201/0570] Loss: 0.19376  Avg Loss: 0.16634  Avg mIoU:  72.64  
[Epoch: 160] [Batch: 0251/0570] Loss: 0.19457  Avg Loss: 0.16723  Avg mIoU:  72.61  
[Epoch: 160] [Batch: 0301/0570] Loss: 0.20181  Avg Loss: 0.16542  Avg mIoU:  72.85  
[Epoch: 160] [Batch: 0351/0570] Loss: 0.12831  Avg Loss: 0.16495  Avg mIoU:  73.08  
[Epoch: 160] [Batch: 0401/0570] Loss: 0.18587  Avg Loss: 0.16515  Avg mIoU:  73.04  
[Epoch: 160] [Batch: 0451/0570] Loss: 0.19765  Avg Loss: 0.16485  Avg mIoU:  73.07  
[Epoch: 160] [Batch: 0501/0570] Loss: 0.20038  Avg Loss: 0.16506  Avg mIoU:  72.94  
[Epoch: 160] [Batch: 0551/0570] Loss: 0.21507  Avg Loss: 0.16500  Avg mIoU:  72.86  

*** Training [@Epoch 160] Avg Loss: 0.16468  Avg mIoU:  73.01  ***

[Epoch: 160] [Batch: 0001/0050] Loss: 0.13249  Avg Loss: 0.13249  Avg mIoU:  66.75  

*** Validation [@Epoch 160] Avg Loss: 0.16899  Avg mIoU:  64.87  ***

[Epoch: 161] [Batch: 0001/0570] Loss: 0.22565  Avg Loss: 0.22565  Avg mIoU:  44.58  
[Epoch: 161] [Batch: 0051/0570] Loss: 0.15157  Avg Loss: 0.15742  Avg mIoU:  72.60  
[Epoch: 161] [Batch: 0101/0570] Loss: 0.17491  Avg Loss: 0.16052  Avg mIoU:  72.18  
[Epoch: 161] [Batch: 0151/0570] Loss: 0.15589  Avg Loss: 0.16042  Avg mIoU:  72.56  
[Epoch: 161] [Batch: 0201/0570] Loss: 0.12488  Avg Loss: 0.15925  Avg mIoU:  72.55  
[Epoch: 161] [Batch: 0251/0570] Loss: 0.12317  Avg Loss: 0.16009  Avg mIoU:  72.99  
[Epoch: 161] [Batch: 0301/0570] Loss: 0.13522  Avg Loss: 0.16118  Avg mIoU:  72.69  
[Epoch: 161] [Batch: 0351/0570] Loss: 0.16272  Avg Loss: 0.16211  Avg mIoU:  72.57  
[Epoch: 161] [Batch: 0401/0570] Loss: 0.20439  Avg Loss: 0.16265  Avg mIoU:  72.75  
[Epoch: 161] [Batch: 0451/0570] Loss: 0.09964  Avg Loss: 0.16316  Avg mIoU:  72.87  
[Epoch: 161] [Batch: 0501/0570] Loss: 0.11257  Avg Loss: 0.16308  Avg mIoU:  73.04  
[Epoch: 161] [Batch: 0551/0570] Loss: 0.28083  Avg Loss: 0.16317  Avg mIoU:  73.10  

*** Training [@Epoch 161] Avg Loss: 0.16378  Avg mIoU:  73.08  ***

[Epoch: 161] [Batch: 0001/0050] Loss: 0.14649  Avg Loss: 0.14649  Avg mIoU:  64.50  

*** Validation [@Epoch 161] Avg Loss: 0.17682  Avg mIoU:  65.19  ***

[Epoch: 162] [Batch: 0001/0570] Loss: 0.18682  Avg Loss: 0.18682  Avg mIoU:  28.88  
[Epoch: 162] [Batch: 0051/0570] Loss: 0.15643  Avg Loss: 0.16779  Avg mIoU:  72.13  
[Epoch: 162] [Batch: 0101/0570] Loss: 0.15496  Avg Loss: 0.16874  Avg mIoU:  72.24  
[Epoch: 162] [Batch: 0151/0570] Loss: 0.20876  Avg Loss: 0.16945  Avg mIoU:  72.19  
[Epoch: 162] [Batch: 0201/0570] Loss: 0.17831  Avg Loss: 0.17043  Avg mIoU:  72.48  
[Epoch: 162] [Batch: 0251/0570] Loss: 0.15551  Avg Loss: 0.16885  Avg mIoU:  72.49  
[Epoch: 162] [Batch: 0301/0570] Loss: 0.12335  Avg Loss: 0.16788  Avg mIoU:  72.55  
[Epoch: 162] [Batch: 0351/0570] Loss: 0.16251  Avg Loss: 0.16772  Avg mIoU:  72.53  
[Epoch: 162] [Batch: 0401/0570] Loss: 0.12774  Avg Loss: 0.16735  Avg mIoU:  72.42  
[Epoch: 162] [Batch: 0451/0570] Loss: 0.20435  Avg Loss: 0.16693  Avg mIoU:  72.54  
[Epoch: 162] [Batch: 0501/0570] Loss: 0.08694  Avg Loss: 0.16680  Avg mIoU:  72.62  
[Epoch: 162] [Batch: 0551/0570] Loss: 0.13387  Avg Loss: 0.16679  Avg mIoU:  72.68  

*** Training [@Epoch 162] Avg Loss: 0.16662  Avg mIoU:  72.81  ***

[Epoch: 162] [Batch: 0001/0050] Loss: 0.13039  Avg Loss: 0.13039  Avg mIoU:  63.13  

*** Validation [@Epoch 162] Avg Loss: 0.15371  Avg mIoU:  64.59  ***

[Epoch: 163] [Batch: 0001/0570] Loss: 0.11126  Avg Loss: 0.11126  Avg mIoU:  38.49  
[Epoch: 163] [Batch: 0051/0570] Loss: 0.10732  Avg Loss: 0.16344  Avg mIoU:  73.87  
[Epoch: 163] [Batch: 0101/0570] Loss: 0.20580  Avg Loss: 0.16693  Avg mIoU:  72.09  
[Epoch: 163] [Batch: 0151/0570] Loss: 0.21138  Avg Loss: 0.16710  Avg mIoU:  72.25  
[Epoch: 163] [Batch: 0201/0570] Loss: 0.19376  Avg Loss: 0.16809  Avg mIoU:  72.09  
[Epoch: 163] [Batch: 0251/0570] Loss: 0.19689  Avg Loss: 0.16656  Avg mIoU:  72.74  
[Epoch: 163] [Batch: 0301/0570] Loss: 0.09326  Avg Loss: 0.16512  Avg mIoU:  72.84  
[Epoch: 163] [Batch: 0351/0570] Loss: 0.12303  Avg Loss: 0.16479  Avg mIoU:  73.12  
[Epoch: 163] [Batch: 0401/0570] Loss: 0.16652  Avg Loss: 0.16448  Avg mIoU:  73.08  
[Epoch: 163] [Batch: 0451/0570] Loss: 0.12048  Avg Loss: 0.16502  Avg mIoU:  73.20  
[Epoch: 163] [Batch: 0501/0570] Loss: 0.16672  Avg Loss: 0.16537  Avg mIoU:  72.95  
[Epoch: 163] [Batch: 0551/0570] Loss: 0.26506  Avg Loss: 0.16551  Avg mIoU:  72.90  

*** Training [@Epoch 163] Avg Loss: 0.16556  Avg mIoU:  72.94  ***

[Epoch: 163] [Batch: 0001/0050] Loss: 0.17794  Avg Loss: 0.17794  Avg mIoU:  56.23  

*** Validation [@Epoch 163] Avg Loss: 0.16177  Avg mIoU:  64.58  ***

[Epoch: 164] [Batch: 0001/0570] Loss: 0.17994  Avg Loss: 0.17994  Avg mIoU:  45.57  
[Epoch: 164] [Batch: 0051/0570] Loss: 0.17940  Avg Loss: 0.15630  Avg mIoU:  74.22  
[Epoch: 164] [Batch: 0101/0570] Loss: 0.19756  Avg Loss: 0.15614  Avg mIoU:  74.03  
[Epoch: 164] [Batch: 0151/0570] Loss: 0.22716  Avg Loss: 0.15566  Avg mIoU:  73.94  
[Epoch: 164] [Batch: 0201/0570] Loss: 0.11956  Avg Loss: 0.16124  Avg mIoU:  73.38  
[Epoch: 164] [Batch: 0251/0570] Loss: 0.22549  Avg Loss: 0.16330  Avg mIoU:  73.58  
[Epoch: 164] [Batch: 0301/0570] Loss: 0.16596  Avg Loss: 0.16566  Avg mIoU:  73.33  
[Epoch: 164] [Batch: 0351/0570] Loss: 0.14497  Avg Loss: 0.16467  Avg mIoU:  73.53  
[Epoch: 164] [Batch: 0401/0570] Loss: 0.13029  Avg Loss: 0.16442  Avg mIoU:  73.50  
[Epoch: 164] [Batch: 0451/0570] Loss: 0.22190  Avg Loss: 0.16461  Avg mIoU:  73.43  
[Epoch: 164] [Batch: 0501/0570] Loss: 0.24083  Avg Loss: 0.16444  Avg mIoU:  73.37  
[Epoch: 164] [Batch: 0551/0570] Loss: 0.16924  Avg Loss: 0.16459  Avg mIoU:  73.15  

*** Training [@Epoch 164] Avg Loss: 0.16444  Avg mIoU:  73.08  ***

[Epoch: 164] [Batch: 0001/0050] Loss: 0.13729  Avg Loss: 0.13729  Avg mIoU:  64.98  

*** Validation [@Epoch 164] Avg Loss: 0.15925  Avg mIoU:  65.09  ***

[Epoch: 165] [Batch: 0001/0570] Loss: 0.16089  Avg Loss: 0.16089  Avg mIoU:  44.13  
[Epoch: 165] [Batch: 0051/0570] Loss: 0.15673  Avg Loss: 0.15819  Avg mIoU:  74.78  
[Epoch: 165] [Batch: 0101/0570] Loss: 0.21243  Avg Loss: 0.16352  Avg mIoU:  73.44  
[Epoch: 165] [Batch: 0151/0570] Loss: 0.12355  Avg Loss: 0.16052  Avg mIoU:  73.87  
[Epoch: 165] [Batch: 0201/0570] Loss: 0.17115  Avg Loss: 0.16062  Avg mIoU:  73.84  
[Epoch: 165] [Batch: 0251/0570] Loss: 0.17138  Avg Loss: 0.15953  Avg mIoU:  73.94  
[Epoch: 165] [Batch: 0301/0570] Loss: 0.23458  Avg Loss: 0.16003  Avg mIoU:  74.01  
[Epoch: 165] [Batch: 0351/0570] Loss: 0.15954  Avg Loss: 0.15999  Avg mIoU:  73.99  
[Epoch: 165] [Batch: 0401/0570] Loss: 0.13029  Avg Loss: 0.15962  Avg mIoU:  73.96  
[Epoch: 165] [Batch: 0451/0570] Loss: 0.11903  Avg Loss: 0.16041  Avg mIoU:  73.71  
[Epoch: 165] [Batch: 0501/0570] Loss: 0.15969  Avg Loss: 0.16161  Avg mIoU:  73.66  
[Epoch: 165] [Batch: 0551/0570] Loss: 0.15390  Avg Loss: 0.16187  Avg mIoU:  73.70  

*** Training [@Epoch 165] Avg Loss: 0.16177  Avg mIoU:  73.76  ***

[Epoch: 165] [Batch: 0001/0050] Loss: 0.14845  Avg Loss: 0.14845  Avg mIoU:  64.57  

*** Validation [@Epoch 165] Avg Loss: 0.18741  Avg mIoU:  64.84  ***

[Epoch: 166] [Batch: 0001/0570] Loss: 0.12269  Avg Loss: 0.12269  Avg mIoU:  46.82  
[Epoch: 166] [Batch: 0051/0570] Loss: 0.17730  Avg Loss: 0.16276  Avg mIoU:  74.74  
[Epoch: 166] [Batch: 0101/0570] Loss: 0.18698  Avg Loss: 0.16631  Avg mIoU:  74.16  
[Epoch: 166] [Batch: 0151/0570] Loss: 0.12230  Avg Loss: 0.16503  Avg mIoU:  73.94  
[Epoch: 166] [Batch: 0201/0570] Loss: 0.20544  Avg Loss: 0.16599  Avg mIoU:  73.59  
[Epoch: 166] [Batch: 0251/0570] Loss: 0.17339  Avg Loss: 0.16317  Avg mIoU:  73.84  
[Epoch: 166] [Batch: 0301/0570] Loss: 0.13223  Avg Loss: 0.16220  Avg mIoU:  73.73  
[Epoch: 166] [Batch: 0351/0570] Loss: 0.19777  Avg Loss: 0.16293  Avg mIoU:  73.57  
[Epoch: 166] [Batch: 0401/0570] Loss: 0.18182  Avg Loss: 0.16323  Avg mIoU:  73.68  
[Epoch: 166] [Batch: 0451/0570] Loss: 0.16974  Avg Loss: 0.16356  Avg mIoU:  73.49  
[Epoch: 166] [Batch: 0501/0570] Loss: 0.19475  Avg Loss: 0.16297  Avg mIoU:  73.45  
[Epoch: 166] [Batch: 0551/0570] Loss: 0.17403  Avg Loss: 0.16240  Avg mIoU:  73.63  

*** Training [@Epoch 166] Avg Loss: 0.16272  Avg mIoU:  73.68  ***

[Epoch: 166] [Batch: 0001/0050] Loss: 0.13142  Avg Loss: 0.13142  Avg mIoU:  63.13  

*** Validation [@Epoch 166] Avg Loss: 0.16468  Avg mIoU:  64.47  ***

[Epoch: 167] [Batch: 0001/0570] Loss: 0.14741  Avg Loss: 0.14741  Avg mIoU:  33.45  
[Epoch: 167] [Batch: 0051/0570] Loss: 0.17938  Avg Loss: 0.16448  Avg mIoU:  74.67  
[Epoch: 167] [Batch: 0101/0570] Loss: 0.14126  Avg Loss: 0.16304  Avg mIoU:  74.93  
[Epoch: 167] [Batch: 0151/0570] Loss: 0.21381  Avg Loss: 0.16163  Avg mIoU:  74.29  
[Epoch: 167] [Batch: 0201/0570] Loss: 0.15910  Avg Loss: 0.16417  Avg mIoU:  73.93  
[Epoch: 167] [Batch: 0251/0570] Loss: 0.13008  Avg Loss: 0.16401  Avg mIoU:  74.07  
[Epoch: 167] [Batch: 0301/0570] Loss: 0.28043  Avg Loss: 0.16494  Avg mIoU:  73.72  
[Epoch: 167] [Batch: 0351/0570] Loss: 0.14649  Avg Loss: 0.16517  Avg mIoU:  73.55  
[Epoch: 167] [Batch: 0401/0570] Loss: 0.11609  Avg Loss: 0.16683  Avg mIoU:  73.29  
[Epoch: 167] [Batch: 0451/0570] Loss: 0.13748  Avg Loss: 0.16605  Avg mIoU:  73.31  
[Epoch: 167] [Batch: 0501/0570] Loss: 0.19596  Avg Loss: 0.16594  Avg mIoU:  73.13  
[Epoch: 167] [Batch: 0551/0570] Loss: 0.13688  Avg Loss: 0.16495  Avg mIoU:  73.19  

*** Training [@Epoch 167] Avg Loss: 0.16481  Avg mIoU:  73.13  ***

[Epoch: 167] [Batch: 0001/0050] Loss: 0.12596  Avg Loss: 0.12596  Avg mIoU:  66.34  

*** Validation [@Epoch 167] Avg Loss: 0.15865  Avg mIoU:  63.38  ***

[Epoch: 168] [Batch: 0001/0570] Loss: 0.10318  Avg Loss: 0.10318  Avg mIoU:  53.91  
[Epoch: 168] [Batch: 0051/0570] Loss: 0.12992  Avg Loss: 0.15173  Avg mIoU:  74.14  
[Epoch: 168] [Batch: 0101/0570] Loss: 0.12113  Avg Loss: 0.16322  Avg mIoU:  73.32  
[Epoch: 168] [Batch: 0151/0570] Loss: 0.15270  Avg Loss: 0.16363  Avg mIoU:  72.53  
[Epoch: 168] [Batch: 0201/0570] Loss: 0.15519  Avg Loss: 0.16535  Avg mIoU:  72.71  
[Epoch: 168] [Batch: 0251/0570] Loss: 0.16994  Avg Loss: 0.16365  Avg mIoU:  73.02  
[Epoch: 168] [Batch: 0301/0570] Loss: 0.12405  Avg Loss: 0.16245  Avg mIoU:  72.94  
[Epoch: 168] [Batch: 0351/0570] Loss: 0.12491  Avg Loss: 0.16426  Avg mIoU:  73.13  
[Epoch: 168] [Batch: 0401/0570] Loss: 0.13351  Avg Loss: 0.16453  Avg mIoU:  73.16  
[Epoch: 168] [Batch: 0451/0570] Loss: 0.12581  Avg Loss: 0.16442  Avg mIoU:  73.18  
[Epoch: 168] [Batch: 0501/0570] Loss: 0.15685  Avg Loss: 0.16412  Avg mIoU:  73.16  
[Epoch: 168] [Batch: 0551/0570] Loss: 0.17322  Avg Loss: 0.16364  Avg mIoU:  73.17  

*** Training [@Epoch 168] Avg Loss: 0.16336  Avg mIoU:  73.24  ***

[Epoch: 168] [Batch: 0001/0050] Loss: 0.12729  Avg Loss: 0.12729  Avg mIoU:  66.13  

*** Validation [@Epoch 168] Avg Loss: 0.16220  Avg mIoU:  65.70  ***

[Epoch: 169] [Batch: 0001/0570] Loss: 0.12639  Avg Loss: 0.12639  Avg mIoU:  45.85  
[Epoch: 169] [Batch: 0051/0570] Loss: 0.09920  Avg Loss: 0.16142  Avg mIoU:  72.42  
[Epoch: 169] [Batch: 0101/0570] Loss: 0.10193  Avg Loss: 0.15813  Avg mIoU:  74.18  
[Epoch: 169] [Batch: 0151/0570] Loss: 0.21930  Avg Loss: 0.15868  Avg mIoU:  74.17  
[Epoch: 169] [Batch: 0201/0570] Loss: 0.16715  Avg Loss: 0.15932  Avg mIoU:  73.73  
[Epoch: 169] [Batch: 0251/0570] Loss: 0.14176  Avg Loss: 0.16048  Avg mIoU:  73.41  
[Epoch: 169] [Batch: 0301/0570] Loss: 0.14271  Avg Loss: 0.15955  Avg mIoU:  73.18  
[Epoch: 169] [Batch: 0351/0570] Loss: 0.17040  Avg Loss: 0.15952  Avg mIoU:  73.59  
[Epoch: 169] [Batch: 0401/0570] Loss: 0.20173  Avg Loss: 0.15974  Avg mIoU:  73.55  
[Epoch: 169] [Batch: 0451/0570] Loss: 0.15841  Avg Loss: 0.16015  Avg mIoU:  73.73  
[Epoch: 169] [Batch: 0501/0570] Loss: 0.11382  Avg Loss: 0.16069  Avg mIoU:  73.66  
[Epoch: 169] [Batch: 0551/0570] Loss: 0.17584  Avg Loss: 0.16077  Avg mIoU:  73.60  

*** Training [@Epoch 169] Avg Loss: 0.16101  Avg mIoU:  73.68  ***

[Epoch: 169] [Batch: 0001/0050] Loss: 0.14353  Avg Loss: 0.14353  Avg mIoU:  67.07  

*** Validation [@Epoch 169] Avg Loss: 0.19559  Avg mIoU:  64.66  ***

[Epoch: 170] [Batch: 0001/0570] Loss: 0.16195  Avg Loss: 0.16195  Avg mIoU:  46.59  
[Epoch: 170] [Batch: 0051/0570] Loss: 0.16375  Avg Loss: 0.15842  Avg mIoU:  73.77  
[Epoch: 170] [Batch: 0101/0570] Loss: 0.20116  Avg Loss: 0.16312  Avg mIoU:  72.68  
[Epoch: 170] [Batch: 0151/0570] Loss: 0.14668  Avg Loss: 0.16287  Avg mIoU:  73.23  
[Epoch: 170] [Batch: 0201/0570] Loss: 0.15166  Avg Loss: 0.16267  Avg mIoU:  73.74  
[Epoch: 170] [Batch: 0251/0570] Loss: 0.17294  Avg Loss: 0.16164  Avg mIoU:  73.44  
[Epoch: 170] [Batch: 0301/0570] Loss: 0.15898  Avg Loss: 0.16208  Avg mIoU:  73.42  
[Epoch: 170] [Batch: 0351/0570] Loss: 0.13077  Avg Loss: 0.16225  Avg mIoU:  73.65  
[Epoch: 170] [Batch: 0401/0570] Loss: 0.15154  Avg Loss: 0.16175  Avg mIoU:  73.83  
[Epoch: 170] [Batch: 0451/0570] Loss: 0.18496  Avg Loss: 0.16149  Avg mIoU:  73.57  
[Epoch: 170] [Batch: 0501/0570] Loss: 0.12055  Avg Loss: 0.16205  Avg mIoU:  73.39  
[Epoch: 170] [Batch: 0551/0570] Loss: 0.21653  Avg Loss: 0.16163  Avg mIoU:  73.57  

*** Training [@Epoch 170] Avg Loss: 0.16204  Avg mIoU:  73.44  ***

[Epoch: 170] [Batch: 0001/0050] Loss: 0.14916  Avg Loss: 0.14916  Avg mIoU:  65.22  

*** Validation [@Epoch 170] Avg Loss: 0.20130  Avg mIoU:  63.13  ***

[Epoch: 171] [Batch: 0001/0570] Loss: 0.17886  Avg Loss: 0.17886  Avg mIoU:  51.58  
[Epoch: 171] [Batch: 0051/0570] Loss: 0.15207  Avg Loss: 0.15608  Avg mIoU:  73.27  
[Epoch: 171] [Batch: 0101/0570] Loss: 0.15230  Avg Loss: 0.16293  Avg mIoU:  72.96  
[Epoch: 171] [Batch: 0151/0570] Loss: 0.15585  Avg Loss: 0.16478  Avg mIoU:  73.08  
[Epoch: 171] [Batch: 0201/0570] Loss: 0.16587  Avg Loss: 0.16297  Avg mIoU:  73.07  
[Epoch: 171] [Batch: 0251/0570] Loss: 0.11882  Avg Loss: 0.16329  Avg mIoU:  73.35  
[Epoch: 171] [Batch: 0301/0570] Loss: 0.15443  Avg Loss: 0.16302  Avg mIoU:  73.16  
[Epoch: 171] [Batch: 0351/0570] Loss: 0.10889  Avg Loss: 0.16172  Avg mIoU:  73.48  
[Epoch: 171] [Batch: 0401/0570] Loss: 0.14339  Avg Loss: 0.16083  Avg mIoU:  73.38  
[Epoch: 171] [Batch: 0451/0570] Loss: 0.13652  Avg Loss: 0.16163  Avg mIoU:  73.23  
[Epoch: 171] [Batch: 0501/0570] Loss: 0.14461  Avg Loss: 0.16219  Avg mIoU:  73.15  
[Epoch: 171] [Batch: 0551/0570] Loss: 0.17285  Avg Loss: 0.16332  Avg mIoU:  73.00  

*** Training [@Epoch 171] Avg Loss: 0.16292  Avg mIoU:  73.06  ***

[Epoch: 171] [Batch: 0001/0050] Loss: 0.12087  Avg Loss: 0.12087  Avg mIoU:  68.11  

*** Validation [@Epoch 171] Avg Loss: 0.16177  Avg mIoU:  66.75  ***

[Epoch: 172] [Batch: 0001/0570] Loss: 0.21333  Avg Loss: 0.21333  Avg mIoU:  26.05  
[Epoch: 172] [Batch: 0051/0570] Loss: 0.18941  Avg Loss: 0.17057  Avg mIoU:  72.00  
[Epoch: 172] [Batch: 0101/0570] Loss: 0.17734  Avg Loss: 0.16264  Avg mIoU:  73.47  
[Epoch: 172] [Batch: 0151/0570] Loss: 0.17407  Avg Loss: 0.16041  Avg mIoU:  73.81  
[Epoch: 172] [Batch: 0201/0570] Loss: 0.22275  Avg Loss: 0.16139  Avg mIoU:  73.51  
[Epoch: 172] [Batch: 0251/0570] Loss: 0.18411  Avg Loss: 0.16120  Avg mIoU:  73.52  
[Epoch: 172] [Batch: 0301/0570] Loss: 0.13897  Avg Loss: 0.16135  Avg mIoU:  73.33  
[Epoch: 172] [Batch: 0351/0570] Loss: 0.13225  Avg Loss: 0.16130  Avg mIoU:  73.23  
[Epoch: 172] [Batch: 0401/0570] Loss: 0.16906  Avg Loss: 0.16168  Avg mIoU:  73.13  
[Epoch: 172] [Batch: 0451/0570] Loss: 0.15698  Avg Loss: 0.16235  Avg mIoU:  73.17  
[Epoch: 172] [Batch: 0501/0570] Loss: 0.12054  Avg Loss: 0.16214  Avg mIoU:  73.18  
[Epoch: 172] [Batch: 0551/0570] Loss: 0.15510  Avg Loss: 0.16253  Avg mIoU:  73.36  

*** Training [@Epoch 172] Avg Loss: 0.16266  Avg mIoU:  73.29  ***

[Epoch: 172] [Batch: 0001/0050] Loss: 0.12706  Avg Loss: 0.12706  Avg mIoU:  67.94  

*** Validation [@Epoch 172] Avg Loss: 0.15116  Avg mIoU:  66.94  ***

Model saved @172 w/ val. mIoU: 66.94.

[Epoch: 173] [Batch: 0001/0570] Loss: 0.12159  Avg Loss: 0.12159  Avg mIoU:  57.13  
[Epoch: 173] [Batch: 0051/0570] Loss: 0.16819  Avg Loss: 0.15138  Avg mIoU:  75.60  
[Epoch: 173] [Batch: 0101/0570] Loss: 0.14331  Avg Loss: 0.15158  Avg mIoU:  75.20  
[Epoch: 173] [Batch: 0151/0570] Loss: 0.24823  Avg Loss: 0.15517  Avg mIoU:  74.83  
[Epoch: 173] [Batch: 0201/0570] Loss: 0.20804  Avg Loss: 0.15664  Avg mIoU:  74.54  
[Epoch: 173] [Batch: 0251/0570] Loss: 0.14054  Avg Loss: 0.15732  Avg mIoU:  74.33  
[Epoch: 173] [Batch: 0301/0570] Loss: 0.10729  Avg Loss: 0.15912  Avg mIoU:  74.26  
[Epoch: 173] [Batch: 0351/0570] Loss: 0.12501  Avg Loss: 0.15784  Avg mIoU:  74.42  
[Epoch: 173] [Batch: 0401/0570] Loss: 0.13585  Avg Loss: 0.15865  Avg mIoU:  74.19  
[Epoch: 173] [Batch: 0451/0570] Loss: 0.21619  Avg Loss: 0.15938  Avg mIoU:  73.87  
[Epoch: 173] [Batch: 0501/0570] Loss: 0.16695  Avg Loss: 0.15936  Avg mIoU:  74.03  
[Epoch: 173] [Batch: 0551/0570] Loss: 0.15790  Avg Loss: 0.16032  Avg mIoU:  74.03  

*** Training [@Epoch 173] Avg Loss: 0.16047  Avg mIoU:  73.97  ***

[Epoch: 173] [Batch: 0001/0050] Loss: 0.14137  Avg Loss: 0.14137  Avg mIoU:  67.18  

*** Validation [@Epoch 173] Avg Loss: 0.18659  Avg mIoU:  65.57  ***

[Epoch: 174] [Batch: 0001/0570] Loss: 0.17605  Avg Loss: 0.17605  Avg mIoU:  38.49  
[Epoch: 174] [Batch: 0051/0570] Loss: 0.18377  Avg Loss: 0.16266  Avg mIoU:  73.17  
[Epoch: 174] [Batch: 0101/0570] Loss: 0.18352  Avg Loss: 0.16179  Avg mIoU:  73.78  
[Epoch: 174] [Batch: 0151/0570] Loss: 0.20626  Avg Loss: 0.16003  Avg mIoU:  74.52  
[Epoch: 174] [Batch: 0201/0570] Loss: 0.18913  Avg Loss: 0.16215  Avg mIoU:  74.05  
[Epoch: 174] [Batch: 0251/0570] Loss: 0.13988  Avg Loss: 0.16117  Avg mIoU:  73.94  
[Epoch: 174] [Batch: 0301/0570] Loss: 0.19295  Avg Loss: 0.16076  Avg mIoU:  74.03  
[Epoch: 174] [Batch: 0351/0570] Loss: 0.20394  Avg Loss: 0.16083  Avg mIoU:  73.78  
[Epoch: 174] [Batch: 0401/0570] Loss: 0.20212  Avg Loss: 0.16102  Avg mIoU:  73.70  
[Epoch: 174] [Batch: 0451/0570] Loss: 0.17383  Avg Loss: 0.16133  Avg mIoU:  73.57  
[Epoch: 174] [Batch: 0501/0570] Loss: 0.15775  Avg Loss: 0.16148  Avg mIoU:  73.51  
[Epoch: 174] [Batch: 0551/0570] Loss: 0.10758  Avg Loss: 0.16250  Avg mIoU:  73.53  

*** Training [@Epoch 174] Avg Loss: 0.16241  Avg mIoU:  73.57  ***

[Epoch: 174] [Batch: 0001/0050] Loss: 0.15307  Avg Loss: 0.15307  Avg mIoU:  65.31  

*** Validation [@Epoch 174] Avg Loss: 0.20259  Avg mIoU:  63.56  ***

[Epoch: 175] [Batch: 0001/0570] Loss: 0.14689  Avg Loss: 0.14689  Avg mIoU:  42.86  
[Epoch: 175] [Batch: 0051/0570] Loss: 0.16464  Avg Loss: 0.15720  Avg mIoU:  73.96  
[Epoch: 175] [Batch: 0101/0570] Loss: 0.17631  Avg Loss: 0.15692  Avg mIoU:  74.66  
[Epoch: 175] [Batch: 0151/0570] Loss: 0.18030  Avg Loss: 0.15716  Avg mIoU:  73.97  
[Epoch: 175] [Batch: 0201/0570] Loss: 0.11618  Avg Loss: 0.15801  Avg mIoU:  73.68  
[Epoch: 175] [Batch: 0251/0570] Loss: 0.12831  Avg Loss: 0.15763  Avg mIoU:  74.07  
[Epoch: 175] [Batch: 0301/0570] Loss: 0.23275  Avg Loss: 0.15794  Avg mIoU:  73.91  
[Epoch: 175] [Batch: 0351/0570] Loss: 0.12831  Avg Loss: 0.15793  Avg mIoU:  73.87  
[Epoch: 175] [Batch: 0401/0570] Loss: 0.15423  Avg Loss: 0.15887  Avg mIoU:  73.71  
[Epoch: 175] [Batch: 0451/0570] Loss: 0.13040  Avg Loss: 0.16009  Avg mIoU:  73.71  
[Epoch: 175] [Batch: 0501/0570] Loss: 0.14591  Avg Loss: 0.16034  Avg mIoU:  73.69  
[Epoch: 175] [Batch: 0551/0570] Loss: 0.15413  Avg Loss: 0.16029  Avg mIoU:  73.68  

*** Training [@Epoch 175] Avg Loss: 0.16050  Avg mIoU:  73.64  ***

[Epoch: 175] [Batch: 0001/0050] Loss: 0.12414  Avg Loss: 0.12414  Avg mIoU:  64.53  

*** Validation [@Epoch 175] Avg Loss: 0.17103  Avg mIoU:  63.16  ***

[Epoch: 176] [Batch: 0001/0570] Loss: 0.13224  Avg Loss: 0.13224  Avg mIoU:  37.83  
[Epoch: 176] [Batch: 0051/0570] Loss: 0.16847  Avg Loss: 0.15662  Avg mIoU:  75.24  
[Epoch: 176] [Batch: 0101/0570] Loss: 0.19935  Avg Loss: 0.15667  Avg mIoU:  75.18  
[Epoch: 176] [Batch: 0151/0570] Loss: 0.17922  Avg Loss: 0.15629  Avg mIoU:  74.88  
[Epoch: 176] [Batch: 0201/0570] Loss: 0.16439  Avg Loss: 0.15812  Avg mIoU:  74.33  
[Epoch: 176] [Batch: 0251/0570] Loss: 0.19247  Avg Loss: 0.15833  Avg mIoU:  74.35  
[Epoch: 176] [Batch: 0301/0570] Loss: 0.18548  Avg Loss: 0.15778  Avg mIoU:  74.58  
[Epoch: 176] [Batch: 0351/0570] Loss: 0.13541  Avg Loss: 0.15858  Avg mIoU:  74.29  
[Epoch: 176] [Batch: 0401/0570] Loss: 0.11844  Avg Loss: 0.15922  Avg mIoU:  74.11  
[Epoch: 176] [Batch: 0451/0570] Loss: 0.13549  Avg Loss: 0.15960  Avg mIoU:  74.19  
[Epoch: 176] [Batch: 0501/0570] Loss: 0.10096  Avg Loss: 0.15946  Avg mIoU:  74.03  
[Epoch: 176] [Batch: 0551/0570] Loss: 0.19997  Avg Loss: 0.15976  Avg mIoU:  73.96  

*** Training [@Epoch 176] Avg Loss: 0.15966  Avg mIoU:  73.90  ***

[Epoch: 176] [Batch: 0001/0050] Loss: 0.13703  Avg Loss: 0.13703  Avg mIoU:  63.22  

*** Validation [@Epoch 176] Avg Loss: 0.16314  Avg mIoU:  67.33  ***

Model saved @176 w/ val. mIoU: 67.33.

[Epoch: 177] [Batch: 0001/0570] Loss: 0.17174  Avg Loss: 0.17174  Avg mIoU:  47.60  
[Epoch: 177] [Batch: 0051/0570] Loss: 0.15172  Avg Loss: 0.15523  Avg mIoU:  74.14  
[Epoch: 177] [Batch: 0101/0570] Loss: 0.13247  Avg Loss: 0.15363  Avg mIoU:  75.01  
[Epoch: 177] [Batch: 0151/0570] Loss: 0.14407  Avg Loss: 0.15320  Avg mIoU:  74.72  
[Epoch: 177] [Batch: 0201/0570] Loss: 0.14108  Avg Loss: 0.15340  Avg mIoU:  74.56  
[Epoch: 177] [Batch: 0251/0570] Loss: 0.15092  Avg Loss: 0.15402  Avg mIoU:  74.55  
[Epoch: 177] [Batch: 0301/0570] Loss: 0.17585  Avg Loss: 0.15493  Avg mIoU:  74.74  
[Epoch: 177] [Batch: 0351/0570] Loss: 0.15715  Avg Loss: 0.15541  Avg mIoU:  74.72  
[Epoch: 177] [Batch: 0401/0570] Loss: 0.10334  Avg Loss: 0.15670  Avg mIoU:  74.70  
[Epoch: 177] [Batch: 0451/0570] Loss: 0.16679  Avg Loss: 0.15717  Avg mIoU:  74.57  
[Epoch: 177] [Batch: 0501/0570] Loss: 0.15592  Avg Loss: 0.15772  Avg mIoU:  74.34  
[Epoch: 177] [Batch: 0551/0570] Loss: 0.12750  Avg Loss: 0.15815  Avg mIoU:  74.21  

*** Training [@Epoch 177] Avg Loss: 0.15845  Avg mIoU:  74.11  ***

[Epoch: 177] [Batch: 0001/0050] Loss: 0.13660  Avg Loss: 0.13660  Avg mIoU:  66.78  

*** Validation [@Epoch 177] Avg Loss: 0.16888  Avg mIoU:  64.63  ***

[Epoch: 178] [Batch: 0001/0570] Loss: 0.19241  Avg Loss: 0.19241  Avg mIoU:  35.62  
[Epoch: 178] [Batch: 0051/0570] Loss: 0.18147  Avg Loss: 0.15949  Avg mIoU:  75.57  
[Epoch: 178] [Batch: 0101/0570] Loss: 0.12738  Avg Loss: 0.15313  Avg mIoU:  75.57  
[Epoch: 178] [Batch: 0151/0570] Loss: 0.11972  Avg Loss: 0.15761  Avg mIoU:  74.79  
[Epoch: 178] [Batch: 0201/0570] Loss: 0.18954  Avg Loss: 0.15852  Avg mIoU:  74.17  
[Epoch: 178] [Batch: 0251/0570] Loss: 0.14973  Avg Loss: 0.15836  Avg mIoU:  74.22  
[Epoch: 178] [Batch: 0301/0570] Loss: 0.14368  Avg Loss: 0.15837  Avg mIoU:  74.39  
[Epoch: 178] [Batch: 0351/0570] Loss: 0.15886  Avg Loss: 0.15910  Avg mIoU:  74.36  
[Epoch: 178] [Batch: 0401/0570] Loss: 0.19455  Avg Loss: 0.15960  Avg mIoU:  74.47  
[Epoch: 178] [Batch: 0451/0570] Loss: 0.24088  Avg Loss: 0.15982  Avg mIoU:  74.36  
[Epoch: 178] [Batch: 0501/0570] Loss: 0.13169  Avg Loss: 0.15999  Avg mIoU:  74.26  
[Epoch: 178] [Batch: 0551/0570] Loss: 0.27264  Avg Loss: 0.15982  Avg mIoU:  74.17  

*** Training [@Epoch 178] Avg Loss: 0.15942  Avg mIoU:  74.18  ***

[Epoch: 178] [Batch: 0001/0050] Loss: 0.13618  Avg Loss: 0.13618  Avg mIoU:  65.67  

*** Validation [@Epoch 178] Avg Loss: 0.16097  Avg mIoU:  65.66  ***

[Epoch: 179] [Batch: 0001/0570] Loss: 0.24598  Avg Loss: 0.24598  Avg mIoU:  40.09  
[Epoch: 179] [Batch: 0051/0570] Loss: 0.16969  Avg Loss: 0.16606  Avg mIoU:  72.90  
[Epoch: 179] [Batch: 0101/0570] Loss: 0.13308  Avg Loss: 0.16327  Avg mIoU:  73.46  
[Epoch: 179] [Batch: 0151/0570] Loss: 0.12468  Avg Loss: 0.16083  Avg mIoU:  73.87  
[Epoch: 179] [Batch: 0201/0570] Loss: 0.08016  Avg Loss: 0.16078  Avg mIoU:  73.84  
[Epoch: 179] [Batch: 0251/0570] Loss: 0.17850  Avg Loss: 0.16080  Avg mIoU:  73.72  
[Epoch: 179] [Batch: 0301/0570] Loss: 0.15718  Avg Loss: 0.15898  Avg mIoU:  74.06  
[Epoch: 179] [Batch: 0351/0570] Loss: 0.10548  Avg Loss: 0.15985  Avg mIoU:  74.00  
[Epoch: 179] [Batch: 0401/0570] Loss: 0.23039  Avg Loss: 0.16053  Avg mIoU:  73.80  
[Epoch: 179] [Batch: 0451/0570] Loss: 0.14214  Avg Loss: 0.16045  Avg mIoU:  73.59  
[Epoch: 179] [Batch: 0501/0570] Loss: 0.15885  Avg Loss: 0.16051  Avg mIoU:  73.72  
[Epoch: 179] [Batch: 0551/0570] Loss: 0.15585  Avg Loss: 0.16088  Avg mIoU:  73.63  

*** Training [@Epoch 179] Avg Loss: 0.16059  Avg mIoU:  73.63  ***

[Epoch: 179] [Batch: 0001/0050] Loss: 0.15095  Avg Loss: 0.15095  Avg mIoU:  64.60  

*** Validation [@Epoch 179] Avg Loss: 0.18711  Avg mIoU:  65.57  ***

[Epoch: 180] [Batch: 0001/0570] Loss: 0.13822  Avg Loss: 0.13822  Avg mIoU:  33.03  
[Epoch: 180] [Batch: 0051/0570] Loss: 0.12154  Avg Loss: 0.15766  Avg mIoU:  74.18  
[Epoch: 180] [Batch: 0101/0570] Loss: 0.10296  Avg Loss: 0.15616  Avg mIoU:  74.55  
[Epoch: 180] [Batch: 0151/0570] Loss: 0.19474  Avg Loss: 0.15829  Avg mIoU:  74.31  
[Epoch: 180] [Batch: 0201/0570] Loss: 0.15067  Avg Loss: 0.16058  Avg mIoU:  73.92  
[Epoch: 180] [Batch: 0251/0570] Loss: 0.16616  Avg Loss: 0.16087  Avg mIoU:  74.09  
[Epoch: 180] [Batch: 0301/0570] Loss: 0.16175  Avg Loss: 0.15990  Avg mIoU:  74.03  
[Epoch: 180] [Batch: 0351/0570] Loss: 0.12896  Avg Loss: 0.15967  Avg mIoU:  73.99  
[Epoch: 180] [Batch: 0401/0570] Loss: 0.27936  Avg Loss: 0.16007  Avg mIoU:  74.13  
[Epoch: 180] [Batch: 0451/0570] Loss: 0.16118  Avg Loss: 0.16087  Avg mIoU:  74.07  
[Epoch: 180] [Batch: 0501/0570] Loss: 0.12006  Avg Loss: 0.16095  Avg mIoU:  73.97  
[Epoch: 180] [Batch: 0551/0570] Loss: 0.19325  Avg Loss: 0.16086  Avg mIoU:  73.95  

*** Training [@Epoch 180] Avg Loss: 0.16055  Avg mIoU:  73.87  ***

[Epoch: 180] [Batch: 0001/0050] Loss: 0.12689  Avg Loss: 0.12689  Avg mIoU:  66.39  

*** Validation [@Epoch 180] Avg Loss: 0.16785  Avg mIoU:  64.54  ***

[Epoch: 181] [Batch: 0001/0570] Loss: 0.18974  Avg Loss: 0.18974  Avg mIoU:  39.31  
[Epoch: 181] [Batch: 0051/0570] Loss: 0.15386  Avg Loss: 0.15974  Avg mIoU:  74.28  
[Epoch: 181] [Batch: 0101/0570] Loss: 0.13823  Avg Loss: 0.15403  Avg mIoU:  74.91  
[Epoch: 181] [Batch: 0151/0570] Loss: 0.11662  Avg Loss: 0.15647  Avg mIoU:  74.24  
[Epoch: 181] [Batch: 0201/0570] Loss: 0.18351  Avg Loss: 0.15633  Avg mIoU:  74.33  
[Epoch: 181] [Batch: 0251/0570] Loss: 0.11829  Avg Loss: 0.15783  Avg mIoU:  74.29  
[Epoch: 181] [Batch: 0301/0570] Loss: 0.18531  Avg Loss: 0.15793  Avg mIoU:  74.15  
[Epoch: 181] [Batch: 0351/0570] Loss: 0.18360  Avg Loss: 0.15830  Avg mIoU:  74.03  
[Epoch: 181] [Batch: 0401/0570] Loss: 0.22337  Avg Loss: 0.15849  Avg mIoU:  73.90  
[Epoch: 181] [Batch: 0451/0570] Loss: 0.17012  Avg Loss: 0.16012  Avg mIoU:  73.94  
[Epoch: 181] [Batch: 0501/0570] Loss: 0.16104  Avg Loss: 0.16091  Avg mIoU:  73.81  
[Epoch: 181] [Batch: 0551/0570] Loss: 0.21035  Avg Loss: 0.16078  Avg mIoU:  73.87  

*** Training [@Epoch 181] Avg Loss: 0.16050  Avg mIoU:  73.89  ***

[Epoch: 181] [Batch: 0001/0050] Loss: 0.13670  Avg Loss: 0.13670  Avg mIoU:  64.29  

*** Validation [@Epoch 181] Avg Loss: 0.17221  Avg mIoU:  64.96  ***

[Epoch: 182] [Batch: 0001/0570] Loss: 0.22081  Avg Loss: 0.22081  Avg mIoU:  33.15  
[Epoch: 182] [Batch: 0051/0570] Loss: 0.17372  Avg Loss: 0.15677  Avg mIoU:  72.22  
[Epoch: 182] [Batch: 0101/0570] Loss: 0.11070  Avg Loss: 0.15933  Avg mIoU:  72.68  
[Epoch: 182] [Batch: 0151/0570] Loss: 0.24162  Avg Loss: 0.15980  Avg mIoU:  72.79  
[Epoch: 182] [Batch: 0201/0570] Loss: 0.15424  Avg Loss: 0.16035  Avg mIoU:  72.98  
[Epoch: 182] [Batch: 0251/0570] Loss: 0.11814  Avg Loss: 0.16016  Avg mIoU:  73.41  
[Epoch: 182] [Batch: 0301/0570] Loss: 0.17188  Avg Loss: 0.15966  Avg mIoU:  73.60  
[Epoch: 182] [Batch: 0351/0570] Loss: 0.13598  Avg Loss: 0.15990  Avg mIoU:  73.60  
[Epoch: 182] [Batch: 0401/0570] Loss: 0.17362  Avg Loss: 0.16010  Avg mIoU:  73.49  
[Epoch: 182] [Batch: 0451/0570] Loss: 0.17133  Avg Loss: 0.16083  Avg mIoU:  73.40  
[Epoch: 182] [Batch: 0501/0570] Loss: 0.10314  Avg Loss: 0.15998  Avg mIoU:  73.51  
[Epoch: 182] [Batch: 0551/0570] Loss: 0.19964  Avg Loss: 0.16088  Avg mIoU:  73.47  

*** Training [@Epoch 182] Avg Loss: 0.16085  Avg mIoU:  73.51  ***

[Epoch: 182] [Batch: 0001/0050] Loss: 0.14058  Avg Loss: 0.14058  Avg mIoU:  64.79  

*** Validation [@Epoch 182] Avg Loss: 0.16626  Avg mIoU:  65.58  ***

[Epoch: 183] [Batch: 0001/0570] Loss: 0.17129  Avg Loss: 0.17129  Avg mIoU:  41.61  
[Epoch: 183] [Batch: 0051/0570] Loss: 0.18523  Avg Loss: 0.16160  Avg mIoU:  74.92  
[Epoch: 183] [Batch: 0101/0570] Loss: 0.22398  Avg Loss: 0.16297  Avg mIoU:  74.62  
[Epoch: 183] [Batch: 0151/0570] Loss: 0.15984  Avg Loss: 0.16078  Avg mIoU:  74.35  
[Epoch: 183] [Batch: 0201/0570] Loss: 0.17995  Avg Loss: 0.16054  Avg mIoU:  74.39  
[Epoch: 183] [Batch: 0251/0570] Loss: 0.13521  Avg Loss: 0.16076  Avg mIoU:  74.30  
[Epoch: 183] [Batch: 0301/0570] Loss: 0.20292  Avg Loss: 0.16147  Avg mIoU:  74.06  
[Epoch: 183] [Batch: 0351/0570] Loss: 0.14076  Avg Loss: 0.16113  Avg mIoU:  74.15  
[Epoch: 183] [Batch: 0401/0570] Loss: 0.11488  Avg Loss: 0.16030  Avg mIoU:  74.16  
[Epoch: 183] [Batch: 0451/0570] Loss: 0.13240  Avg Loss: 0.15982  Avg mIoU:  74.20  
[Epoch: 183] [Batch: 0501/0570] Loss: 0.12726  Avg Loss: 0.16059  Avg mIoU:  73.97  
[Epoch: 183] [Batch: 0551/0570] Loss: 0.15671  Avg Loss: 0.16077  Avg mIoU:  73.78  

*** Training [@Epoch 183] Avg Loss: 0.16096  Avg mIoU:  73.88  ***

[Epoch: 183] [Batch: 0001/0050] Loss: 0.13830  Avg Loss: 0.13830  Avg mIoU:  64.49  

*** Validation [@Epoch 183] Avg Loss: 0.18540  Avg mIoU:  64.68  ***

[Epoch: 184] [Batch: 0001/0570] Loss: 0.12780  Avg Loss: 0.12780  Avg mIoU:  54.70  
[Epoch: 184] [Batch: 0051/0570] Loss: 0.16782  Avg Loss: 0.15360  Avg mIoU:  75.48  
[Epoch: 184] [Batch: 0101/0570] Loss: 0.15713  Avg Loss: 0.15412  Avg mIoU:  74.88  
[Epoch: 184] [Batch: 0151/0570] Loss: 0.18147  Avg Loss: 0.15530  Avg mIoU:  74.05  
[Epoch: 184] [Batch: 0201/0570] Loss: 0.21021  Avg Loss: 0.15466  Avg mIoU:  74.43  
[Epoch: 184] [Batch: 0251/0570] Loss: 0.17701  Avg Loss: 0.15683  Avg mIoU:  74.07  
[Epoch: 184] [Batch: 0301/0570] Loss: 0.17270  Avg Loss: 0.15792  Avg mIoU:  74.06  
[Epoch: 184] [Batch: 0351/0570] Loss: 0.10190  Avg Loss: 0.15831  Avg mIoU:  74.27  
[Epoch: 184] [Batch: 0401/0570] Loss: 0.19044  Avg Loss: 0.15902  Avg mIoU:  74.14  
[Epoch: 184] [Batch: 0451/0570] Loss: 0.16035  Avg Loss: 0.15940  Avg mIoU:  73.89  
[Epoch: 184] [Batch: 0501/0570] Loss: 0.13370  Avg Loss: 0.15992  Avg mIoU:  73.92  
[Epoch: 184] [Batch: 0551/0570] Loss: 0.08796  Avg Loss: 0.15965  Avg mIoU:  73.96  

*** Training [@Epoch 184] Avg Loss: 0.15971  Avg mIoU:  73.93  ***

[Epoch: 184] [Batch: 0001/0050] Loss: 0.13749  Avg Loss: 0.13749  Avg mIoU:  64.00  

*** Validation [@Epoch 184] Avg Loss: 0.18166  Avg mIoU:  64.46  ***

[Epoch: 185] [Batch: 0001/0570] Loss: 0.20977  Avg Loss: 0.20977  Avg mIoU:  43.38  
[Epoch: 185] [Batch: 0051/0570] Loss: 0.21725  Avg Loss: 0.16079  Avg mIoU:  73.76  
[Epoch: 185] [Batch: 0101/0570] Loss: 0.18670  Avg Loss: 0.15788  Avg mIoU:  74.40  
[Epoch: 185] [Batch: 0151/0570] Loss: 0.18339  Avg Loss: 0.15891  Avg mIoU:  74.02  
[Epoch: 185] [Batch: 0201/0570] Loss: 0.13118  Avg Loss: 0.15720  Avg mIoU:  74.28  
[Epoch: 185] [Batch: 0251/0570] Loss: 0.14960  Avg Loss: 0.15837  Avg mIoU:  74.05  
[Epoch: 185] [Batch: 0301/0570] Loss: 0.14490  Avg Loss: 0.15718  Avg mIoU:  74.22  
[Epoch: 185] [Batch: 0351/0570] Loss: 0.17465  Avg Loss: 0.15726  Avg mIoU:  74.26  
[Epoch: 185] [Batch: 0401/0570] Loss: 0.17165  Avg Loss: 0.15732  Avg mIoU:  74.19  
[Epoch: 185] [Batch: 0451/0570] Loss: 0.14279  Avg Loss: 0.15696  Avg mIoU:  74.26  
[Epoch: 185] [Batch: 0501/0570] Loss: 0.18977  Avg Loss: 0.15619  Avg mIoU:  74.34  
[Epoch: 185] [Batch: 0551/0570] Loss: 0.14367  Avg Loss: 0.15713  Avg mIoU:  74.18  

*** Training [@Epoch 185] Avg Loss: 0.15730  Avg mIoU:  74.22  ***

[Epoch: 185] [Batch: 0001/0050] Loss: 0.12811  Avg Loss: 0.12811  Avg mIoU:  65.98  

*** Validation [@Epoch 185] Avg Loss: 0.16205  Avg mIoU:  65.42  ***

[Epoch: 186] [Batch: 0001/0570] Loss: 0.12845  Avg Loss: 0.12845  Avg mIoU:  46.41  
[Epoch: 186] [Batch: 0051/0570] Loss: 0.12613  Avg Loss: 0.15653  Avg mIoU:  74.21  
[Epoch: 186] [Batch: 0101/0570] Loss: 0.08669  Avg Loss: 0.15370  Avg mIoU:  74.31  
[Epoch: 186] [Batch: 0151/0570] Loss: 0.13996  Avg Loss: 0.15750  Avg mIoU:  74.68  
[Epoch: 186] [Batch: 0201/0570] Loss: 0.12628  Avg Loss: 0.15792  Avg mIoU:  74.12  
[Epoch: 186] [Batch: 0251/0570] Loss: 0.14180  Avg Loss: 0.15661  Avg mIoU:  74.14  
[Epoch: 186] [Batch: 0301/0570] Loss: 0.25442  Avg Loss: 0.15835  Avg mIoU:  73.94  
[Epoch: 186] [Batch: 0351/0570] Loss: 0.13707  Avg Loss: 0.15858  Avg mIoU:  73.79  
[Epoch: 186] [Batch: 0401/0570] Loss: 0.13434  Avg Loss: 0.15900  Avg mIoU:  73.84  
[Epoch: 186] [Batch: 0451/0570] Loss: 0.10801  Avg Loss: 0.15825  Avg mIoU:  74.05  
[Epoch: 186] [Batch: 0501/0570] Loss: 0.12108  Avg Loss: 0.15758  Avg mIoU:  73.99  
[Epoch: 186] [Batch: 0551/0570] Loss: 0.15326  Avg Loss: 0.15813  Avg mIoU:  73.94  

*** Training [@Epoch 186] Avg Loss: 0.15811  Avg mIoU:  73.98  ***

[Epoch: 186] [Batch: 0001/0050] Loss: 0.14237  Avg Loss: 0.14237  Avg mIoU:  63.58  

*** Validation [@Epoch 186] Avg Loss: 0.17666  Avg mIoU:  64.71  ***

[Epoch: 187] [Batch: 0001/0570] Loss: 0.15481  Avg Loss: 0.15481  Avg mIoU:  45.77  
[Epoch: 187] [Batch: 0051/0570] Loss: 0.11380  Avg Loss: 0.14848  Avg mIoU:  75.27  
[Epoch: 187] [Batch: 0101/0570] Loss: 0.19184  Avg Loss: 0.15066  Avg mIoU:  74.33  
[Epoch: 187] [Batch: 0151/0570] Loss: 0.15583  Avg Loss: 0.15433  Avg mIoU:  73.71  
[Epoch: 187] [Batch: 0201/0570] Loss: 0.22949  Avg Loss: 0.15694  Avg mIoU:  73.92  
[Epoch: 187] [Batch: 0251/0570] Loss: 0.12875  Avg Loss: 0.15817  Avg mIoU:  73.82  
[Epoch: 187] [Batch: 0301/0570] Loss: 0.18866  Avg Loss: 0.15931  Avg mIoU:  73.99  
[Epoch: 187] [Batch: 0351/0570] Loss: 0.12373  Avg Loss: 0.15968  Avg mIoU:  74.02  
[Epoch: 187] [Batch: 0401/0570] Loss: 0.14837  Avg Loss: 0.16197  Avg mIoU:  73.97  
[Epoch: 187] [Batch: 0451/0570] Loss: 0.22475  Avg Loss: 0.16112  Avg mIoU:  74.07  
[Epoch: 187] [Batch: 0501/0570] Loss: 0.12273  Avg Loss: 0.16122  Avg mIoU:  73.99  
[Epoch: 187] [Batch: 0551/0570] Loss: 0.15766  Avg Loss: 0.16122  Avg mIoU:  73.90  

*** Training [@Epoch 187] Avg Loss: 0.16062  Avg mIoU:  74.03  ***

[Epoch: 187] [Batch: 0001/0050] Loss: 0.11766  Avg Loss: 0.11766  Avg mIoU:  65.98  

*** Validation [@Epoch 187] Avg Loss: 0.17304  Avg mIoU:  64.21  ***

[Epoch: 188] [Batch: 0001/0570] Loss: 0.13556  Avg Loss: 0.13556  Avg mIoU:  45.83  
[Epoch: 188] [Batch: 0051/0570] Loss: 0.14441  Avg Loss: 0.15563  Avg mIoU:  74.49  
[Epoch: 188] [Batch: 0101/0570] Loss: 0.18744  Avg Loss: 0.15615  Avg mIoU:  74.95  
[Epoch: 188] [Batch: 0151/0570] Loss: 0.14918  Avg Loss: 0.15438  Avg mIoU:  75.13  
[Epoch: 188] [Batch: 0201/0570] Loss: 0.18826  Avg Loss: 0.15606  Avg mIoU:  74.61  
[Epoch: 188] [Batch: 0251/0570] Loss: 0.20342  Avg Loss: 0.15902  Avg mIoU:  74.38  
[Epoch: 188] [Batch: 0301/0570] Loss: 0.20407  Avg Loss: 0.15840  Avg mIoU:  74.11  
[Epoch: 188] [Batch: 0351/0570] Loss: 0.13655  Avg Loss: 0.15888  Avg mIoU:  74.04  
[Epoch: 188] [Batch: 0401/0570] Loss: 0.16347  Avg Loss: 0.15959  Avg mIoU:  74.05  
[Epoch: 188] [Batch: 0451/0570] Loss: 0.15090  Avg Loss: 0.15849  Avg mIoU:  74.08  
[Epoch: 188] [Batch: 0501/0570] Loss: 0.12976  Avg Loss: 0.15823  Avg mIoU:  74.09  
[Epoch: 188] [Batch: 0551/0570] Loss: 0.17354  Avg Loss: 0.15834  Avg mIoU:  74.17  

*** Training [@Epoch 188] Avg Loss: 0.15843  Avg mIoU:  74.15  ***

[Epoch: 188] [Batch: 0001/0050] Loss: 0.12251  Avg Loss: 0.12251  Avg mIoU:  67.79  

*** Validation [@Epoch 188] Avg Loss: 0.15655  Avg mIoU:  63.96  ***

[Epoch: 189] [Batch: 0001/0570] Loss: 0.18900  Avg Loss: 0.18900  Avg mIoU:  41.15  
[Epoch: 189] [Batch: 0051/0570] Loss: 0.12075  Avg Loss: 0.15612  Avg mIoU:  73.34  
[Epoch: 189] [Batch: 0101/0570] Loss: 0.17291  Avg Loss: 0.15645  Avg mIoU:  73.66  
[Epoch: 189] [Batch: 0151/0570] Loss: 0.14139  Avg Loss: 0.15593  Avg mIoU:  73.46  
[Epoch: 189] [Batch: 0201/0570] Loss: 0.20205  Avg Loss: 0.15719  Avg mIoU:  73.51  
[Epoch: 189] [Batch: 0251/0570] Loss: 0.14004  Avg Loss: 0.15859  Avg mIoU:  73.66  
[Epoch: 189] [Batch: 0301/0570] Loss: 0.11532  Avg Loss: 0.15842  Avg mIoU:  73.86  
[Epoch: 189] [Batch: 0351/0570] Loss: 0.23207  Avg Loss: 0.15898  Avg mIoU:  73.95  
[Epoch: 189] [Batch: 0401/0570] Loss: 0.15778  Avg Loss: 0.15796  Avg mIoU:  74.24  
[Epoch: 189] [Batch: 0451/0570] Loss: 0.25862  Avg Loss: 0.15881  Avg mIoU:  74.31  
[Epoch: 189] [Batch: 0501/0570] Loss: 0.13723  Avg Loss: 0.15889  Avg mIoU:  74.26  
[Epoch: 189] [Batch: 0551/0570] Loss: 0.14689  Avg Loss: 0.15798  Avg mIoU:  74.22  

*** Training [@Epoch 189] Avg Loss: 0.15808  Avg mIoU:  74.23  ***

[Epoch: 189] [Batch: 0001/0050] Loss: 0.14121  Avg Loss: 0.14121  Avg mIoU:  67.08  

*** Validation [@Epoch 189] Avg Loss: 0.18085  Avg mIoU:  65.16  ***

[Epoch: 190] [Batch: 0001/0570] Loss: 0.19371  Avg Loss: 0.19371  Avg mIoU:  38.19  
[Epoch: 190] [Batch: 0051/0570] Loss: 0.09544  Avg Loss: 0.15571  Avg mIoU:  75.09  
[Epoch: 190] [Batch: 0101/0570] Loss: 0.16876  Avg Loss: 0.15731  Avg mIoU:  74.51  
[Epoch: 190] [Batch: 0151/0570] Loss: 0.14152  Avg Loss: 0.15711  Avg mIoU:  74.30  
[Epoch: 190] [Batch: 0201/0570] Loss: 0.12172  Avg Loss: 0.15753  Avg mIoU:  74.41  
[Epoch: 190] [Batch: 0251/0570] Loss: 0.18877  Avg Loss: 0.15759  Avg mIoU:  74.34  
[Epoch: 190] [Batch: 0301/0570] Loss: 0.12605  Avg Loss: 0.15802  Avg mIoU:  74.39  
[Epoch: 190] [Batch: 0351/0570] Loss: 0.12440  Avg Loss: 0.15892  Avg mIoU:  74.47  
[Epoch: 190] [Batch: 0401/0570] Loss: 0.17136  Avg Loss: 0.15794  Avg mIoU:  74.47  
[Epoch: 190] [Batch: 0451/0570] Loss: 0.14766  Avg Loss: 0.15825  Avg mIoU:  74.23  
[Epoch: 190] [Batch: 0501/0570] Loss: 0.12687  Avg Loss: 0.15871  Avg mIoU:  74.27  
[Epoch: 190] [Batch: 0551/0570] Loss: 0.11317  Avg Loss: 0.15853  Avg mIoU:  74.24  

*** Training [@Epoch 190] Avg Loss: 0.15881  Avg mIoU:  74.29  ***

[Epoch: 190] [Batch: 0001/0050] Loss: 0.13167  Avg Loss: 0.13167  Avg mIoU:  64.63  

*** Validation [@Epoch 190] Avg Loss: 0.17146  Avg mIoU:  63.37  ***

[Epoch: 191] [Batch: 0001/0570] Loss: 0.16708  Avg Loss: 0.16708  Avg mIoU:  24.21  
[Epoch: 191] [Batch: 0051/0570] Loss: 0.15703  Avg Loss: 0.15622  Avg mIoU:  72.67  
[Epoch: 191] [Batch: 0101/0570] Loss: 0.13169  Avg Loss: 0.15776  Avg mIoU:  74.18  
[Epoch: 191] [Batch: 0151/0570] Loss: 0.17828  Avg Loss: 0.15678  Avg mIoU:  73.58  
[Epoch: 191] [Batch: 0201/0570] Loss: 0.16240  Avg Loss: 0.15749  Avg mIoU:  73.44  
[Epoch: 191] [Batch: 0251/0570] Loss: 0.19382  Avg Loss: 0.15818  Avg mIoU:  73.61  
[Epoch: 191] [Batch: 0301/0570] Loss: 0.15923  Avg Loss: 0.15856  Avg mIoU:  73.93  
[Epoch: 191] [Batch: 0351/0570] Loss: 0.13894  Avg Loss: 0.15971  Avg mIoU:  73.75  
[Epoch: 191] [Batch: 0401/0570] Loss: 0.11475  Avg Loss: 0.15902  Avg mIoU:  73.88  
[Epoch: 191] [Batch: 0451/0570] Loss: 0.14161  Avg Loss: 0.15841  Avg mIoU:  74.01  
[Epoch: 191] [Batch: 0501/0570] Loss: 0.14307  Avg Loss: 0.15803  Avg mIoU:  74.11  
[Epoch: 191] [Batch: 0551/0570] Loss: 0.15234  Avg Loss: 0.15791  Avg mIoU:  74.10  

*** Training [@Epoch 191] Avg Loss: 0.15845  Avg mIoU:  74.09  ***

[Epoch: 191] [Batch: 0001/0050] Loss: 0.14078  Avg Loss: 0.14078  Avg mIoU:  66.02  

*** Validation [@Epoch 191] Avg Loss: 0.16873  Avg mIoU:  64.41  ***

[Epoch: 192] [Batch: 0001/0570] Loss: 0.18425  Avg Loss: 0.18425  Avg mIoU:  52.63  
[Epoch: 192] [Batch: 0051/0570] Loss: 0.19255  Avg Loss: 0.16350  Avg mIoU:  71.25  
[Epoch: 192] [Batch: 0101/0570] Loss: 0.13267  Avg Loss: 0.16372  Avg mIoU:  73.11  
[Epoch: 192] [Batch: 0151/0570] Loss: 0.16550  Avg Loss: 0.16245  Avg mIoU:  73.56  
[Epoch: 192] [Batch: 0201/0570] Loss: 0.14122  Avg Loss: 0.15917  Avg mIoU:  73.64  
[Epoch: 192] [Batch: 0251/0570] Loss: 0.18974  Avg Loss: 0.15915  Avg mIoU:  73.65  
[Epoch: 192] [Batch: 0301/0570] Loss: 0.14583  Avg Loss: 0.16100  Avg mIoU:  73.39  
[Epoch: 192] [Batch: 0351/0570] Loss: 0.16614  Avg Loss: 0.16041  Avg mIoU:  73.79  
[Epoch: 192] [Batch: 0401/0570] Loss: 0.11452  Avg Loss: 0.15949  Avg mIoU:  73.83  
[Epoch: 192] [Batch: 0451/0570] Loss: 0.13066  Avg Loss: 0.15888  Avg mIoU:  73.93  
[Epoch: 192] [Batch: 0501/0570] Loss: 0.12220  Avg Loss: 0.15962  Avg mIoU:  73.84  
[Epoch: 192] [Batch: 0551/0570] Loss: 0.14436  Avg Loss: 0.16019  Avg mIoU:  73.76  

*** Training [@Epoch 192] Avg Loss: 0.16028  Avg mIoU:  73.75  ***

[Epoch: 192] [Batch: 0001/0050] Loss: 0.13735  Avg Loss: 0.13735  Avg mIoU:  66.09  

*** Validation [@Epoch 192] Avg Loss: 0.18081  Avg mIoU:  64.72  ***

[Epoch: 193] [Batch: 0001/0570] Loss: 0.19513  Avg Loss: 0.19513  Avg mIoU:  29.27  
[Epoch: 193] [Batch: 0051/0570] Loss: 0.15970  Avg Loss: 0.15522  Avg mIoU:  73.73  
[Epoch: 193] [Batch: 0101/0570] Loss: 0.18579  Avg Loss: 0.15485  Avg mIoU:  73.86  
[Epoch: 193] [Batch: 0151/0570] Loss: 0.08010  Avg Loss: 0.15432  Avg mIoU:  73.62  
[Epoch: 193] [Batch: 0201/0570] Loss: 0.17093  Avg Loss: 0.15936  Avg mIoU:  73.43  
[Epoch: 193] [Batch: 0251/0570] Loss: 0.13433  Avg Loss: 0.15942  Avg mIoU:  73.48  
[Epoch: 193] [Batch: 0301/0570] Loss: 0.08118  Avg Loss: 0.15909  Avg mIoU:  73.51  
[Epoch: 193] [Batch: 0351/0570] Loss: 0.14666  Avg Loss: 0.15961  Avg mIoU:  73.71  
[Epoch: 193] [Batch: 0401/0570] Loss: 0.20650  Avg Loss: 0.15889  Avg mIoU:  73.89  
[Epoch: 193] [Batch: 0451/0570] Loss: 0.11084  Avg Loss: 0.15855  Avg mIoU:  74.02  
[Epoch: 193] [Batch: 0501/0570] Loss: 0.10796  Avg Loss: 0.15879  Avg mIoU:  74.08  
[Epoch: 193] [Batch: 0551/0570] Loss: 0.19019  Avg Loss: 0.15871  Avg mIoU:  73.97  

*** Training [@Epoch 193] Avg Loss: 0.15841  Avg mIoU:  74.01  ***

[Epoch: 193] [Batch: 0001/0050] Loss: 0.12471  Avg Loss: 0.12471  Avg mIoU:  65.70  

*** Validation [@Epoch 193] Avg Loss: 0.15718  Avg mIoU:  64.90  ***

[Epoch: 194] [Batch: 0001/0570] Loss: 0.11675  Avg Loss: 0.11675  Avg mIoU:  51.74  
[Epoch: 194] [Batch: 0051/0570] Loss: 0.14436  Avg Loss: 0.14991  Avg mIoU:  75.44  
[Epoch: 194] [Batch: 0101/0570] Loss: 0.16247  Avg Loss: 0.15245  Avg mIoU:  74.83  
[Epoch: 194] [Batch: 0151/0570] Loss: 0.11834  Avg Loss: 0.15489  Avg mIoU:  74.47  
[Epoch: 194] [Batch: 0201/0570] Loss: 0.12349  Avg Loss: 0.15470  Avg mIoU:  74.28  
[Epoch: 194] [Batch: 0251/0570] Loss: 0.31685  Avg Loss: 0.15615  Avg mIoU:  74.12  
[Epoch: 194] [Batch: 0301/0570] Loss: 0.15130  Avg Loss: 0.15743  Avg mIoU:  73.70  
[Epoch: 194] [Batch: 0351/0570] Loss: 0.11824  Avg Loss: 0.15634  Avg mIoU:  74.11  
[Epoch: 194] [Batch: 0401/0570] Loss: 0.14131  Avg Loss: 0.15708  Avg mIoU:  74.21  
[Epoch: 194] [Batch: 0451/0570] Loss: 0.15930  Avg Loss: 0.15759  Avg mIoU:  74.16  
[Epoch: 194] [Batch: 0501/0570] Loss: 0.24116  Avg Loss: 0.15766  Avg mIoU:  74.18  
[Epoch: 194] [Batch: 0551/0570] Loss: 0.20549  Avg Loss: 0.15767  Avg mIoU:  74.18  

*** Training [@Epoch 194] Avg Loss: 0.15756  Avg mIoU:  74.28  ***

[Epoch: 194] [Batch: 0001/0050] Loss: 0.12985  Avg Loss: 0.12985  Avg mIoU:  66.11  

*** Validation [@Epoch 194] Avg Loss: 0.16796  Avg mIoU:  66.07  ***

[Epoch: 195] [Batch: 0001/0570] Loss: 0.15877  Avg Loss: 0.15877  Avg mIoU:  35.63  
[Epoch: 195] [Batch: 0051/0570] Loss: 0.17195  Avg Loss: 0.15989  Avg mIoU:  73.49  
[Epoch: 195] [Batch: 0101/0570] Loss: 0.14594  Avg Loss: 0.15606  Avg mIoU:  73.93  
[Epoch: 195] [Batch: 0151/0570] Loss: 0.11453  Avg Loss: 0.15808  Avg mIoU:  74.05  
[Epoch: 195] [Batch: 0201/0570] Loss: 0.21004  Avg Loss: 0.15599  Avg mIoU:  74.51  
[Epoch: 195] [Batch: 0251/0570] Loss: 0.12529  Avg Loss: 0.15426  Avg mIoU:  74.34  
[Epoch: 195] [Batch: 0301/0570] Loss: 0.14847  Avg Loss: 0.15528  Avg mIoU:  74.64  
[Epoch: 195] [Batch: 0351/0570] Loss: 0.17464  Avg Loss: 0.15453  Avg mIoU:  74.61  
[Epoch: 195] [Batch: 0401/0570] Loss: 0.13942  Avg Loss: 0.15545  Avg mIoU:  74.40  
[Epoch: 195] [Batch: 0451/0570] Loss: 0.17283  Avg Loss: 0.15624  Avg mIoU:  74.34  
[Epoch: 195] [Batch: 0501/0570] Loss: 0.16665  Avg Loss: 0.15696  Avg mIoU:  74.35  
[Epoch: 195] [Batch: 0551/0570] Loss: 0.10708  Avg Loss: 0.15651  Avg mIoU:  74.32  

*** Training [@Epoch 195] Avg Loss: 0.15637  Avg mIoU:  74.37  ***

[Epoch: 195] [Batch: 0001/0050] Loss: 0.14015  Avg Loss: 0.14015  Avg mIoU:  64.86  

*** Validation [@Epoch 195] Avg Loss: 0.17237  Avg mIoU:  65.57  ***

[Epoch: 196] [Batch: 0001/0570] Loss: 0.18393  Avg Loss: 0.18393  Avg mIoU:  29.93  
[Epoch: 196] [Batch: 0051/0570] Loss: 0.16598  Avg Loss: 0.15025  Avg mIoU:  76.97  
[Epoch: 196] [Batch: 0101/0570] Loss: 0.13773  Avg Loss: 0.15491  Avg mIoU:  76.24  
[Epoch: 196] [Batch: 0151/0570] Loss: 0.18939  Avg Loss: 0.15446  Avg mIoU:  75.45  
[Epoch: 196] [Batch: 0201/0570] Loss: 0.10778  Avg Loss: 0.15488  Avg mIoU:  74.88  
[Epoch: 196] [Batch: 0251/0570] Loss: 0.14437  Avg Loss: 0.15538  Avg mIoU:  74.95  
[Epoch: 196] [Batch: 0301/0570] Loss: 0.12259  Avg Loss: 0.15657  Avg mIoU:  74.53  
[Epoch: 196] [Batch: 0351/0570] Loss: 0.09659  Avg Loss: 0.15507  Avg mIoU:  74.62  
[Epoch: 196] [Batch: 0401/0570] Loss: 0.12303  Avg Loss: 0.15416  Avg mIoU:  74.73  
[Epoch: 196] [Batch: 0451/0570] Loss: 0.18182  Avg Loss: 0.15474  Avg mIoU:  74.52  
[Epoch: 196] [Batch: 0501/0570] Loss: 0.13146  Avg Loss: 0.15439  Avg mIoU:  74.61  
[Epoch: 196] [Batch: 0551/0570] Loss: 0.20823  Avg Loss: 0.15501  Avg mIoU:  74.51  

*** Training [@Epoch 196] Avg Loss: 0.15503  Avg mIoU:  74.46  ***

[Epoch: 196] [Batch: 0001/0050] Loss: 0.14086  Avg Loss: 0.14086  Avg mIoU:  64.85  

*** Validation [@Epoch 196] Avg Loss: 0.17920  Avg mIoU:  65.60  ***

[Epoch: 197] [Batch: 0001/0570] Loss: 0.14309  Avg Loss: 0.14309  Avg mIoU:  44.13  
[Epoch: 197] [Batch: 0051/0570] Loss: 0.21203  Avg Loss: 0.14710  Avg mIoU:  75.19  
[Epoch: 197] [Batch: 0101/0570] Loss: 0.17464  Avg Loss: 0.14878  Avg mIoU:  75.81  
[Epoch: 197] [Batch: 0151/0570] Loss: 0.15247  Avg Loss: 0.15645  Avg mIoU:  74.86  
[Epoch: 197] [Batch: 0201/0570] Loss: 0.12410  Avg Loss: 0.15560  Avg mIoU:  74.85  
[Epoch: 197] [Batch: 0251/0570] Loss: 0.18240  Avg Loss: 0.15546  Avg mIoU:  74.60  
[Epoch: 197] [Batch: 0301/0570] Loss: 0.15131  Avg Loss: 0.15765  Avg mIoU:  74.38  
[Epoch: 197] [Batch: 0351/0570] Loss: 0.12099  Avg Loss: 0.15680  Avg mIoU:  74.50  
[Epoch: 197] [Batch: 0401/0570] Loss: 0.13928  Avg Loss: 0.15603  Avg mIoU:  74.59  
[Epoch: 197] [Batch: 0451/0570] Loss: 0.18088  Avg Loss: 0.15680  Avg mIoU:  74.31  
[Epoch: 197] [Batch: 0501/0570] Loss: 0.14689  Avg Loss: 0.15643  Avg mIoU:  74.34  
[Epoch: 197] [Batch: 0551/0570] Loss: 0.20126  Avg Loss: 0.15673  Avg mIoU:  74.25  

*** Training [@Epoch 197] Avg Loss: 0.15717  Avg mIoU:  74.12  ***

[Epoch: 197] [Batch: 0001/0050] Loss: 0.13810  Avg Loss: 0.13810  Avg mIoU:  65.25  

*** Validation [@Epoch 197] Avg Loss: 0.16844  Avg mIoU:  65.93  ***

[Epoch: 198] [Batch: 0001/0570] Loss: 0.17539  Avg Loss: 0.17539  Avg mIoU:  51.65  
[Epoch: 198] [Batch: 0051/0570] Loss: 0.15316  Avg Loss: 0.16469  Avg mIoU:  72.70  
[Epoch: 198] [Batch: 0101/0570] Loss: 0.21200  Avg Loss: 0.16367  Avg mIoU:  73.30  
[Epoch: 198] [Batch: 0151/0570] Loss: 0.13021  Avg Loss: 0.15911  Avg mIoU:  73.94  
[Epoch: 198] [Batch: 0201/0570] Loss: 0.15955  Avg Loss: 0.16009  Avg mIoU:  74.15  
[Epoch: 198] [Batch: 0251/0570] Loss: 0.15007  Avg Loss: 0.16012  Avg mIoU:  73.99  
[Epoch: 198] [Batch: 0301/0570] Loss: 0.19920  Avg Loss: 0.15975  Avg mIoU:  73.86  
[Epoch: 198] [Batch: 0351/0570] Loss: 0.12955  Avg Loss: 0.15865  Avg mIoU:  74.00  
[Epoch: 198] [Batch: 0401/0570] Loss: 0.15359  Avg Loss: 0.15907  Avg mIoU:  74.01  
[Epoch: 198] [Batch: 0451/0570] Loss: 0.16535  Avg Loss: 0.15920  Avg mIoU:  74.04  
[Epoch: 198] [Batch: 0501/0570] Loss: 0.22255  Avg Loss: 0.15897  Avg mIoU:  74.05  
[Epoch: 198] [Batch: 0551/0570] Loss: 0.26926  Avg Loss: 0.15919  Avg mIoU:  74.03  

*** Training [@Epoch 198] Avg Loss: 0.15913  Avg mIoU:  74.05  ***

[Epoch: 198] [Batch: 0001/0050] Loss: 0.13416  Avg Loss: 0.13416  Avg mIoU:  66.04  

*** Validation [@Epoch 198] Avg Loss: 0.15981  Avg mIoU:  65.16  ***

[Epoch: 199] [Batch: 0001/0570] Loss: 0.17720  Avg Loss: 0.17720  Avg mIoU:  45.79  
[Epoch: 199] [Batch: 0051/0570] Loss: 0.14975  Avg Loss: 0.14980  Avg mIoU:  74.22  
[Epoch: 199] [Batch: 0101/0570] Loss: 0.20116  Avg Loss: 0.14948  Avg mIoU:  73.90  
[Epoch: 199] [Batch: 0151/0570] Loss: 0.13599  Avg Loss: 0.15193  Avg mIoU:  73.75  
[Epoch: 199] [Batch: 0201/0570] Loss: 0.19221  Avg Loss: 0.15343  Avg mIoU:  74.01  
[Epoch: 199] [Batch: 0251/0570] Loss: 0.13778  Avg Loss: 0.15320  Avg mIoU:  74.39  
[Epoch: 199] [Batch: 0301/0570] Loss: 0.15186  Avg Loss: 0.15699  Avg mIoU:  73.99  
[Epoch: 199] [Batch: 0351/0570] Loss: 0.15350  Avg Loss: 0.15698  Avg mIoU:  74.02  
[Epoch: 199] [Batch: 0401/0570] Loss: 0.18164  Avg Loss: 0.15623  Avg mIoU:  74.20  
[Epoch: 199] [Batch: 0451/0570] Loss: 0.21776  Avg Loss: 0.15741  Avg mIoU:  73.92  
[Epoch: 199] [Batch: 0501/0570] Loss: 0.17034  Avg Loss: 0.15738  Avg mIoU:  74.17  
[Epoch: 199] [Batch: 0551/0570] Loss: 0.13569  Avg Loss: 0.15616  Avg mIoU:  74.33  

*** Training [@Epoch 199] Avg Loss: 0.15584  Avg mIoU:  74.43  ***

[Epoch: 199] [Batch: 0001/0050] Loss: 0.12887  Avg Loss: 0.12887  Avg mIoU:  65.76  

*** Validation [@Epoch 199] Avg Loss: 0.17279  Avg mIoU:  64.79  ***

[Epoch: 200] [Batch: 0001/0570] Loss: 0.16881  Avg Loss: 0.16881  Avg mIoU:  29.54  
[Epoch: 200] [Batch: 0051/0570] Loss: 0.16439  Avg Loss: 0.14852  Avg mIoU:  74.36  
[Epoch: 200] [Batch: 0101/0570] Loss: 0.10095  Avg Loss: 0.15238  Avg mIoU:  74.94  
[Epoch: 200] [Batch: 0151/0570] Loss: 0.11421  Avg Loss: 0.15169  Avg mIoU:  75.39  
[Epoch: 200] [Batch: 0201/0570] Loss: 0.21648  Avg Loss: 0.15217  Avg mIoU:  75.38  
[Epoch: 200] [Batch: 0251/0570] Loss: 0.12862  Avg Loss: 0.15272  Avg mIoU:  75.42  
[Epoch: 200] [Batch: 0301/0570] Loss: 0.11919  Avg Loss: 0.15436  Avg mIoU:  75.24  
[Epoch: 200] [Batch: 0351/0570] Loss: 0.14474  Avg Loss: 0.15558  Avg mIoU:  74.90  
[Epoch: 200] [Batch: 0401/0570] Loss: 0.20136  Avg Loss: 0.15593  Avg mIoU:  74.76  
[Epoch: 200] [Batch: 0451/0570] Loss: 0.11183  Avg Loss: 0.15582  Avg mIoU:  74.85  
[Epoch: 200] [Batch: 0501/0570] Loss: 0.16636  Avg Loss: 0.15574  Avg mIoU:  74.88  
[Epoch: 200] [Batch: 0551/0570] Loss: 0.17022  Avg Loss: 0.15622  Avg mIoU:  74.64  

*** Training [@Epoch 200] Avg Loss: 0.15630  Avg mIoU:  74.50  ***

[Epoch: 200] [Batch: 0001/0050] Loss: 0.13072  Avg Loss: 0.13072  Avg mIoU:  64.38  

*** Validation [@Epoch 200] Avg Loss: 0.17000  Avg mIoU:  63.66  ***

[Epoch: 201] [Batch: 0001/0570] Loss: 0.17132  Avg Loss: 0.17132  Avg mIoU:  41.42  
[Epoch: 201] [Batch: 0051/0570] Loss: 0.17015  Avg Loss: 0.15562  Avg mIoU:  73.54  
[Epoch: 201] [Batch: 0101/0570] Loss: 0.11622  Avg Loss: 0.15707  Avg mIoU:  74.32  
[Epoch: 201] [Batch: 0151/0570] Loss: 0.14736  Avg Loss: 0.15585  Avg mIoU:  74.48  
[Epoch: 201] [Batch: 0201/0570] Loss: 0.26867  Avg Loss: 0.15518  Avg mIoU:  74.63  
[Epoch: 201] [Batch: 0251/0570] Loss: 0.22589  Avg Loss: 0.15449  Avg mIoU:  74.51  
[Epoch: 201] [Batch: 0301/0570] Loss: 0.13345  Avg Loss: 0.15491  Avg mIoU:  74.47  
[Epoch: 201] [Batch: 0351/0570] Loss: 0.16883  Avg Loss: 0.15493  Avg mIoU:  74.27  
[Epoch: 201] [Batch: 0401/0570] Loss: 0.15837  Avg Loss: 0.15553  Avg mIoU:  74.10  
[Epoch: 201] [Batch: 0451/0570] Loss: 0.13027  Avg Loss: 0.15569  Avg mIoU:  74.15  
[Epoch: 201] [Batch: 0501/0570] Loss: 0.19405  Avg Loss: 0.15659  Avg mIoU:  74.12  
[Epoch: 201] [Batch: 0551/0570] Loss: 0.17342  Avg Loss: 0.15591  Avg mIoU:  74.16  

*** Training [@Epoch 201] Avg Loss: 0.15596  Avg mIoU:  74.24  ***

[Epoch: 201] [Batch: 0001/0050] Loss: 0.13961  Avg Loss: 0.13961  Avg mIoU:  64.91  

*** Validation [@Epoch 201] Avg Loss: 0.18834  Avg mIoU:  64.69  ***

[Epoch: 202] [Batch: 0001/0570] Loss: 0.16194  Avg Loss: 0.16194  Avg mIoU:  34.14  
[Epoch: 202] [Batch: 0051/0570] Loss: 0.12587  Avg Loss: 0.14688  Avg mIoU:  76.60  
[Epoch: 202] [Batch: 0101/0570] Loss: 0.17666  Avg Loss: 0.15155  Avg mIoU:  74.81  
[Epoch: 202] [Batch: 0151/0570] Loss: 0.12055  Avg Loss: 0.15459  Avg mIoU:  74.30  
[Epoch: 202] [Batch: 0201/0570] Loss: 0.12479  Avg Loss: 0.15623  Avg mIoU:  74.24  
[Epoch: 202] [Batch: 0251/0570] Loss: 0.12130  Avg Loss: 0.15589  Avg mIoU:  74.21  
[Epoch: 202] [Batch: 0301/0570] Loss: 0.17771  Avg Loss: 0.15519  Avg mIoU:  74.34  
[Epoch: 202] [Batch: 0351/0570] Loss: 0.10532  Avg Loss: 0.15457  Avg mIoU:  74.69  
[Epoch: 202] [Batch: 0401/0570] Loss: 0.11185  Avg Loss: 0.15478  Avg mIoU:  74.71  
[Epoch: 202] [Batch: 0451/0570] Loss: 0.12842  Avg Loss: 0.15496  Avg mIoU:  74.68  
[Epoch: 202] [Batch: 0501/0570] Loss: 0.14212  Avg Loss: 0.15497  Avg mIoU:  74.69  
[Epoch: 202] [Batch: 0551/0570] Loss: 0.23418  Avg Loss: 0.15524  Avg mIoU:  74.54  

*** Training [@Epoch 202] Avg Loss: 0.15555  Avg mIoU:  74.48  ***

[Epoch: 202] [Batch: 0001/0050] Loss: 0.14399  Avg Loss: 0.14399  Avg mIoU:  65.51  

*** Validation [@Epoch 202] Avg Loss: 0.17376  Avg mIoU:  64.82  ***

[Epoch: 203] [Batch: 0001/0570] Loss: 0.16315  Avg Loss: 0.16315  Avg mIoU:  40.01  
[Epoch: 203] [Batch: 0051/0570] Loss: 0.14802  Avg Loss: 0.15707  Avg mIoU:  74.74  
[Epoch: 203] [Batch: 0101/0570] Loss: 0.11266  Avg Loss: 0.15496  Avg mIoU:  74.89  
[Epoch: 203] [Batch: 0151/0570] Loss: 0.19069  Avg Loss: 0.15514  Avg mIoU:  74.61  
[Epoch: 203] [Batch: 0201/0570] Loss: 0.11280  Avg Loss: 0.15661  Avg mIoU:  74.10  
[Epoch: 203] [Batch: 0251/0570] Loss: 0.21958  Avg Loss: 0.15496  Avg mIoU:  74.60  
[Epoch: 203] [Batch: 0301/0570] Loss: 0.14642  Avg Loss: 0.15437  Avg mIoU:  74.68  
[Epoch: 203] [Batch: 0351/0570] Loss: 0.10234  Avg Loss: 0.15364  Avg mIoU:  74.81  
[Epoch: 203] [Batch: 0401/0570] Loss: 0.11589  Avg Loss: 0.15253  Avg mIoU:  74.68  
[Epoch: 203] [Batch: 0451/0570] Loss: 0.11236  Avg Loss: 0.15352  Avg mIoU:  74.79  
[Epoch: 203] [Batch: 0501/0570] Loss: 0.12001  Avg Loss: 0.15447  Avg mIoU:  74.67  
[Epoch: 203] [Batch: 0551/0570] Loss: 0.24204  Avg Loss: 0.15526  Avg mIoU:  74.63  

*** Training [@Epoch 203] Avg Loss: 0.15461  Avg mIoU:  74.70  ***

[Epoch: 203] [Batch: 0001/0050] Loss: 0.12909  Avg Loss: 0.12909  Avg mIoU:  67.55  

*** Validation [@Epoch 203] Avg Loss: 0.16805  Avg mIoU:  66.80  ***

[Epoch: 204] [Batch: 0001/0570] Loss: 0.18632  Avg Loss: 0.18632  Avg mIoU:  49.62  
[Epoch: 204] [Batch: 0051/0570] Loss: 0.12877  Avg Loss: 0.15627  Avg mIoU:  73.07  
[Epoch: 204] [Batch: 0101/0570] Loss: 0.18067  Avg Loss: 0.15605  Avg mIoU:  72.88  
[Epoch: 204] [Batch: 0151/0570] Loss: 0.16626  Avg Loss: 0.15701  Avg mIoU:  73.77  
[Epoch: 204] [Batch: 0201/0570] Loss: 0.12468  Avg Loss: 0.15618  Avg mIoU:  74.02  
[Epoch: 204] [Batch: 0251/0570] Loss: 0.17815  Avg Loss: 0.15708  Avg mIoU:  74.03  
[Epoch: 204] [Batch: 0301/0570] Loss: 0.13001  Avg Loss: 0.15612  Avg mIoU:  74.37  
[Epoch: 204] [Batch: 0351/0570] Loss: 0.12705  Avg Loss: 0.15590  Avg mIoU:  74.59  
[Epoch: 204] [Batch: 0401/0570] Loss: 0.11801  Avg Loss: 0.15568  Avg mIoU:  74.68  
[Epoch: 204] [Batch: 0451/0570] Loss: 0.17046  Avg Loss: 0.15557  Avg mIoU:  74.74  
[Epoch: 204] [Batch: 0501/0570] Loss: 0.10844  Avg Loss: 0.15539  Avg mIoU:  74.68  
[Epoch: 204] [Batch: 0551/0570] Loss: 0.13953  Avg Loss: 0.15479  Avg mIoU:  74.75  

*** Training [@Epoch 204] Avg Loss: 0.15489  Avg mIoU:  74.88  ***

[Epoch: 204] [Batch: 0001/0050] Loss: 0.12407  Avg Loss: 0.12407  Avg mIoU:  64.58  

*** Validation [@Epoch 204] Avg Loss: 0.15650  Avg mIoU:  66.27  ***

[Epoch: 205] [Batch: 0001/0570] Loss: 0.15777  Avg Loss: 0.15777  Avg mIoU:  59.08  
[Epoch: 205] [Batch: 0051/0570] Loss: 0.15487  Avg Loss: 0.15200  Avg mIoU:  74.23  
[Epoch: 205] [Batch: 0101/0570] Loss: 0.18383  Avg Loss: 0.15376  Avg mIoU:  74.95  
[Epoch: 205] [Batch: 0151/0570] Loss: 0.11016  Avg Loss: 0.15195  Avg mIoU:  75.16  
[Epoch: 205] [Batch: 0201/0570] Loss: 0.14899  Avg Loss: 0.15334  Avg mIoU:  74.63  
[Epoch: 205] [Batch: 0251/0570] Loss: 0.12223  Avg Loss: 0.15209  Avg mIoU:  74.67  
[Epoch: 205] [Batch: 0301/0570] Loss: 0.18808  Avg Loss: 0.15309  Avg mIoU:  74.71  
[Epoch: 205] [Batch: 0351/0570] Loss: 0.16009  Avg Loss: 0.15321  Avg mIoU:  74.62  
[Epoch: 205] [Batch: 0401/0570] Loss: 0.15170  Avg Loss: 0.15436  Avg mIoU:  74.77  
[Epoch: 205] [Batch: 0451/0570] Loss: 0.18328  Avg Loss: 0.15571  Avg mIoU:  74.61  
[Epoch: 205] [Batch: 0501/0570] Loss: 0.12783  Avg Loss: 0.15544  Avg mIoU:  74.72  
[Epoch: 205] [Batch: 0551/0570] Loss: 0.19266  Avg Loss: 0.15539  Avg mIoU:  74.54  

*** Training [@Epoch 205] Avg Loss: 0.15507  Avg mIoU:  74.58  ***

[Epoch: 205] [Batch: 0001/0050] Loss: 0.12888  Avg Loss: 0.12888  Avg mIoU:  65.34  

*** Validation [@Epoch 205] Avg Loss: 0.18274  Avg mIoU:  65.38  ***

[Epoch: 206] [Batch: 0001/0570] Loss: 0.13946  Avg Loss: 0.13946  Avg mIoU:  51.76  
[Epoch: 206] [Batch: 0051/0570] Loss: 0.12236  Avg Loss: 0.15108  Avg mIoU:  74.83  
[Epoch: 206] [Batch: 0101/0570] Loss: 0.15023  Avg Loss: 0.15176  Avg mIoU:  74.20  
[Epoch: 206] [Batch: 0151/0570] Loss: 0.11674  Avg Loss: 0.15445  Avg mIoU:  74.23  
[Epoch: 206] [Batch: 0201/0570] Loss: 0.19707  Avg Loss: 0.15588  Avg mIoU:  74.00  
[Epoch: 206] [Batch: 0251/0570] Loss: 0.19623  Avg Loss: 0.15461  Avg mIoU:  74.20  
[Epoch: 206] [Batch: 0301/0570] Loss: 0.22230  Avg Loss: 0.15401  Avg mIoU:  74.46  
[Epoch: 206] [Batch: 0351/0570] Loss: 0.15739  Avg Loss: 0.15509  Avg mIoU:  74.36  
[Epoch: 206] [Batch: 0401/0570] Loss: 0.20430  Avg Loss: 0.15572  Avg mIoU:  74.22  
[Epoch: 206] [Batch: 0451/0570] Loss: 0.12686  Avg Loss: 0.15593  Avg mIoU:  73.97  
[Epoch: 206] [Batch: 0501/0570] Loss: 0.10743  Avg Loss: 0.15566  Avg mIoU:  74.06  
[Epoch: 206] [Batch: 0551/0570] Loss: 0.17771  Avg Loss: 0.15530  Avg mIoU:  74.19  

*** Training [@Epoch 206] Avg Loss: 0.15544  Avg mIoU:  74.32  ***

[Epoch: 206] [Batch: 0001/0050] Loss: 0.11708  Avg Loss: 0.11708  Avg mIoU:  67.90  

*** Validation [@Epoch 206] Avg Loss: 0.16322  Avg mIoU:  65.25  ***

[Epoch: 207] [Batch: 0001/0570] Loss: 0.15146  Avg Loss: 0.15146  Avg mIoU:  41.49  
[Epoch: 207] [Batch: 0051/0570] Loss: 0.16168  Avg Loss: 0.14506  Avg mIoU:  76.12  
[Epoch: 207] [Batch: 0101/0570] Loss: 0.15679  Avg Loss: 0.15069  Avg mIoU:  76.15  
[Epoch: 207] [Batch: 0151/0570] Loss: 0.19663  Avg Loss: 0.15234  Avg mIoU:  76.07  
[Epoch: 207] [Batch: 0201/0570] Loss: 0.16484  Avg Loss: 0.15255  Avg mIoU:  75.31  
[Epoch: 207] [Batch: 0251/0570] Loss: 0.19659  Avg Loss: 0.15225  Avg mIoU:  75.61  
[Epoch: 207] [Batch: 0301/0570] Loss: 0.13658  Avg Loss: 0.15283  Avg mIoU:  75.09  
[Epoch: 207] [Batch: 0351/0570] Loss: 0.18640  Avg Loss: 0.15252  Avg mIoU:  75.29  
[Epoch: 207] [Batch: 0401/0570] Loss: 0.17775  Avg Loss: 0.15261  Avg mIoU:  75.07  
[Epoch: 207] [Batch: 0451/0570] Loss: 0.11441  Avg Loss: 0.15211  Avg mIoU:  75.07  
[Epoch: 207] [Batch: 0501/0570] Loss: 0.12436  Avg Loss: 0.15113  Avg mIoU:  75.08  
[Epoch: 207] [Batch: 0551/0570] Loss: 0.19883  Avg Loss: 0.15273  Avg mIoU:  74.82  

*** Training [@Epoch 207] Avg Loss: 0.15306  Avg mIoU:  74.88  ***

[Epoch: 207] [Batch: 0001/0050] Loss: 0.12385  Avg Loss: 0.12385  Avg mIoU:  67.34  

*** Validation [@Epoch 207] Avg Loss: 0.17785  Avg mIoU:  64.09  ***

[Epoch: 208] [Batch: 0001/0570] Loss: 0.14566  Avg Loss: 0.14566  Avg mIoU:  57.40  
[Epoch: 208] [Batch: 0051/0570] Loss: 0.19234  Avg Loss: 0.14638  Avg mIoU:  74.71  
[Epoch: 208] [Batch: 0101/0570] Loss: 0.11635  Avg Loss: 0.15170  Avg mIoU:  74.37  
[Epoch: 208] [Batch: 0151/0570] Loss: 0.17005  Avg Loss: 0.14955  Avg mIoU:  74.64  
[Epoch: 208] [Batch: 0201/0570] Loss: 0.18359  Avg Loss: 0.15106  Avg mIoU:  74.54  
[Epoch: 208] [Batch: 0251/0570] Loss: 0.16782  Avg Loss: 0.15142  Avg mIoU:  74.47  
[Epoch: 208] [Batch: 0301/0570] Loss: 0.19404  Avg Loss: 0.15236  Avg mIoU:  75.03  
[Epoch: 208] [Batch: 0351/0570] Loss: 0.19712  Avg Loss: 0.15168  Avg mIoU:  75.14  
[Epoch: 208] [Batch: 0401/0570] Loss: 0.13796  Avg Loss: 0.15260  Avg mIoU:  74.96  
[Epoch: 208] [Batch: 0451/0570] Loss: 0.15825  Avg Loss: 0.15444  Avg mIoU:  74.76  
[Epoch: 208] [Batch: 0501/0570] Loss: 0.13128  Avg Loss: 0.15483  Avg mIoU:  74.83  
[Epoch: 208] [Batch: 0551/0570] Loss: 0.13693  Avg Loss: 0.15407  Avg mIoU:  74.85  

*** Training [@Epoch 208] Avg Loss: 0.15402  Avg mIoU:  74.94  ***

[Epoch: 208] [Batch: 0001/0050] Loss: 0.13640  Avg Loss: 0.13640  Avg mIoU:  66.50  

*** Validation [@Epoch 208] Avg Loss: 0.18374  Avg mIoU:  64.62  ***

[Epoch: 209] [Batch: 0001/0570] Loss: 0.18635  Avg Loss: 0.18635  Avg mIoU:  57.22  
[Epoch: 209] [Batch: 0051/0570] Loss: 0.20214  Avg Loss: 0.15929  Avg mIoU:  74.04  
[Epoch: 209] [Batch: 0101/0570] Loss: 0.11978  Avg Loss: 0.15285  Avg mIoU:  74.77  
[Epoch: 209] [Batch: 0151/0570] Loss: 0.23905  Avg Loss: 0.15395  Avg mIoU:  74.46  
[Epoch: 209] [Batch: 0201/0570] Loss: 0.13993  Avg Loss: 0.15559  Avg mIoU:  74.06  
[Epoch: 209] [Batch: 0251/0570] Loss: 0.20564  Avg Loss: 0.15609  Avg mIoU:  74.20  
[Epoch: 209] [Batch: 0301/0570] Loss: 0.15021  Avg Loss: 0.15711  Avg mIoU:  74.02  
[Epoch: 209] [Batch: 0351/0570] Loss: 0.13776  Avg Loss: 0.15765  Avg mIoU:  73.91  
[Epoch: 209] [Batch: 0401/0570] Loss: 0.13802  Avg Loss: 0.15612  Avg mIoU:  74.19  
[Epoch: 209] [Batch: 0451/0570] Loss: 0.18863  Avg Loss: 0.15641  Avg mIoU:  74.06  
[Epoch: 209] [Batch: 0501/0570] Loss: 0.19124  Avg Loss: 0.15573  Avg mIoU:  74.16  
[Epoch: 209] [Batch: 0551/0570] Loss: 0.12482  Avg Loss: 0.15585  Avg mIoU:  74.14  

*** Training [@Epoch 209] Avg Loss: 0.15580  Avg mIoU:  74.18  ***

[Epoch: 209] [Batch: 0001/0050] Loss: 0.12366  Avg Loss: 0.12366  Avg mIoU:  68.47  

*** Validation [@Epoch 209] Avg Loss: 0.18407  Avg mIoU:  65.39  ***

[Epoch: 210] [Batch: 0001/0570] Loss: 0.19467  Avg Loss: 0.19467  Avg mIoU:  41.37  
[Epoch: 210] [Batch: 0051/0570] Loss: 0.09770  Avg Loss: 0.15620  Avg mIoU:  77.10  
[Epoch: 210] [Batch: 0101/0570] Loss: 0.16742  Avg Loss: 0.15312  Avg mIoU:  75.47  
[Epoch: 210] [Batch: 0151/0570] Loss: 0.12996  Avg Loss: 0.15646  Avg mIoU:  75.66  
[Epoch: 210] [Batch: 0201/0570] Loss: 0.15099  Avg Loss: 0.15712  Avg mIoU:  74.95  
[Epoch: 210] [Batch: 0251/0570] Loss: 0.11545  Avg Loss: 0.15516  Avg mIoU:  74.86  
[Epoch: 210] [Batch: 0301/0570] Loss: 0.13442  Avg Loss: 0.15307  Avg mIoU:  75.19  
[Epoch: 210] [Batch: 0351/0570] Loss: 0.13321  Avg Loss: 0.15277  Avg mIoU:  75.33  
[Epoch: 210] [Batch: 0401/0570] Loss: 0.21965  Avg Loss: 0.15217  Avg mIoU:  75.41  
[Epoch: 210] [Batch: 0451/0570] Loss: 0.15771  Avg Loss: 0.15271  Avg mIoU:  75.39  
[Epoch: 210] [Batch: 0501/0570] Loss: 0.15371  Avg Loss: 0.15363  Avg mIoU:  75.15  
[Epoch: 210] [Batch: 0551/0570] Loss: 0.15083  Avg Loss: 0.15387  Avg mIoU:  74.93  

*** Training [@Epoch 210] Avg Loss: 0.15339  Avg mIoU:  75.02  ***

[Epoch: 210] [Batch: 0001/0050] Loss: 0.12874  Avg Loss: 0.12874  Avg mIoU:  66.59  

*** Validation [@Epoch 210] Avg Loss: 0.17961  Avg mIoU:  65.56  ***

[Epoch: 211] [Batch: 0001/0570] Loss: 0.15577  Avg Loss: 0.15577  Avg mIoU:  51.36  
[Epoch: 211] [Batch: 0051/0570] Loss: 0.14846  Avg Loss: 0.15232  Avg mIoU:  76.90  
[Epoch: 211] [Batch: 0101/0570] Loss: 0.15531  Avg Loss: 0.15043  Avg mIoU:  75.73  
[Epoch: 211] [Batch: 0151/0570] Loss: 0.28197  Avg Loss: 0.14998  Avg mIoU:  75.43  
[Epoch: 211] [Batch: 0201/0570] Loss: 0.17977  Avg Loss: 0.14946  Avg mIoU:  75.11  
[Epoch: 211] [Batch: 0251/0570] Loss: 0.16857  Avg Loss: 0.15165  Avg mIoU:  74.73  
[Epoch: 211] [Batch: 0301/0570] Loss: 0.14888  Avg Loss: 0.15154  Avg mIoU:  74.77  
[Epoch: 211] [Batch: 0351/0570] Loss: 0.18535  Avg Loss: 0.15159  Avg mIoU:  74.86  
[Epoch: 211] [Batch: 0401/0570] Loss: 0.17869  Avg Loss: 0.15163  Avg mIoU:  74.92  
[Epoch: 211] [Batch: 0451/0570] Loss: 0.15119  Avg Loss: 0.15183  Avg mIoU:  75.01  
[Epoch: 211] [Batch: 0501/0570] Loss: 0.11921  Avg Loss: 0.15240  Avg mIoU:  74.97  
[Epoch: 211] [Batch: 0551/0570] Loss: 0.11908  Avg Loss: 0.15230  Avg mIoU:  75.00  

*** Training [@Epoch 211] Avg Loss: 0.15263  Avg mIoU:  75.02  ***

[Epoch: 211] [Batch: 0001/0050] Loss: 0.14627  Avg Loss: 0.14627  Avg mIoU:  62.79  

*** Validation [@Epoch 211] Avg Loss: 0.18376  Avg mIoU:  64.49  ***

[Epoch: 212] [Batch: 0001/0570] Loss: 0.10327  Avg Loss: 0.10327  Avg mIoU:  31.56  
[Epoch: 212] [Batch: 0051/0570] Loss: 0.13192  Avg Loss: 0.14790  Avg mIoU:  75.43  
[Epoch: 212] [Batch: 0101/0570] Loss: 0.10825  Avg Loss: 0.15042  Avg mIoU:  75.20  
[Epoch: 212] [Batch: 0151/0570] Loss: 0.15243  Avg Loss: 0.14933  Avg mIoU:  76.13  
[Epoch: 212] [Batch: 0201/0570] Loss: 0.11561  Avg Loss: 0.14943  Avg mIoU:  75.84  
[Epoch: 212] [Batch: 0251/0570] Loss: 0.11695  Avg Loss: 0.14804  Avg mIoU:  75.87  
[Epoch: 212] [Batch: 0301/0570] Loss: 0.22597  Avg Loss: 0.14908  Avg mIoU:  75.75  
[Epoch: 212] [Batch: 0351/0570] Loss: 0.10553  Avg Loss: 0.14875  Avg mIoU:  75.68  
[Epoch: 212] [Batch: 0401/0570] Loss: 0.11530  Avg Loss: 0.14941  Avg mIoU:  75.49  
[Epoch: 212] [Batch: 0451/0570] Loss: 0.22421  Avg Loss: 0.14970  Avg mIoU:  75.36  
[Epoch: 212] [Batch: 0501/0570] Loss: 0.10089  Avg Loss: 0.15062  Avg mIoU:  75.28  
[Epoch: 212] [Batch: 0551/0570] Loss: 0.18404  Avg Loss: 0.15121  Avg mIoU:  75.27  

*** Training [@Epoch 212] Avg Loss: 0.15147  Avg mIoU:  75.21  ***

[Epoch: 212] [Batch: 0001/0050] Loss: 0.11501  Avg Loss: 0.11501  Avg mIoU:  63.92  

*** Validation [@Epoch 212] Avg Loss: 0.15352  Avg mIoU:  63.42  ***

[Epoch: 213] [Batch: 0001/0570] Loss: 0.18597  Avg Loss: 0.18597  Avg mIoU:  55.26  
[Epoch: 213] [Batch: 0051/0570] Loss: 0.20934  Avg Loss: 0.16550  Avg mIoU:  72.98  
[Epoch: 213] [Batch: 0101/0570] Loss: 0.11476  Avg Loss: 0.15698  Avg mIoU:  74.43  
[Epoch: 213] [Batch: 0151/0570] Loss: 0.13887  Avg Loss: 0.15718  Avg mIoU:  74.64  
[Epoch: 213] [Batch: 0201/0570] Loss: 0.21025  Avg Loss: 0.15596  Avg mIoU:  74.43  
[Epoch: 213] [Batch: 0251/0570] Loss: 0.16377  Avg Loss: 0.15476  Avg mIoU:  74.66  
[Epoch: 213] [Batch: 0301/0570] Loss: 0.13725  Avg Loss: 0.15494  Avg mIoU:  74.47  
[Epoch: 213] [Batch: 0351/0570] Loss: 0.17175  Avg Loss: 0.15320  Avg mIoU:  74.81  
[Epoch: 213] [Batch: 0401/0570] Loss: 0.17333  Avg Loss: 0.15364  Avg mIoU:  74.71  
[Epoch: 213] [Batch: 0451/0570] Loss: 0.13278  Avg Loss: 0.15296  Avg mIoU:  74.64  
[Epoch: 213] [Batch: 0501/0570] Loss: 0.16199  Avg Loss: 0.15318  Avg mIoU:  74.63  
[Epoch: 213] [Batch: 0551/0570] Loss: 0.22615  Avg Loss: 0.15262  Avg mIoU:  74.80  

*** Training [@Epoch 213] Avg Loss: 0.15264  Avg mIoU:  74.84  ***

[Epoch: 213] [Batch: 0001/0050] Loss: 0.12288  Avg Loss: 0.12288  Avg mIoU:  68.22  

*** Validation [@Epoch 213] Avg Loss: 0.17779  Avg mIoU:  63.77  ***

[Epoch: 214] [Batch: 0001/0570] Loss: 0.20306  Avg Loss: 0.20306  Avg mIoU:  35.21  
[Epoch: 214] [Batch: 0051/0570] Loss: 0.17189  Avg Loss: 0.15633  Avg mIoU:  73.72  
[Epoch: 214] [Batch: 0101/0570] Loss: 0.15578  Avg Loss: 0.15006  Avg mIoU:  74.69  
[Epoch: 214] [Batch: 0151/0570] Loss: 0.14967  Avg Loss: 0.14863  Avg mIoU:  74.53  
[Epoch: 214] [Batch: 0201/0570] Loss: 0.15007  Avg Loss: 0.15101  Avg mIoU:  74.35  
[Epoch: 214] [Batch: 0251/0570] Loss: 0.17003  Avg Loss: 0.15250  Avg mIoU:  74.44  
[Epoch: 214] [Batch: 0301/0570] Loss: 0.14709  Avg Loss: 0.15299  Avg mIoU:  74.28  
[Epoch: 214] [Batch: 0351/0570] Loss: 0.12422  Avg Loss: 0.15350  Avg mIoU:  74.32  
[Epoch: 214] [Batch: 0401/0570] Loss: 0.22632  Avg Loss: 0.15354  Avg mIoU:  74.30  
[Epoch: 214] [Batch: 0451/0570] Loss: 0.22649  Avg Loss: 0.15261  Avg mIoU:  74.49  
[Epoch: 214] [Batch: 0501/0570] Loss: 0.11476  Avg Loss: 0.15242  Avg mIoU:  74.69  
[Epoch: 214] [Batch: 0551/0570] Loss: 0.18248  Avg Loss: 0.15236  Avg mIoU:  74.72  

*** Training [@Epoch 214] Avg Loss: 0.15243  Avg mIoU:  74.81  ***

[Epoch: 214] [Batch: 0001/0050] Loss: 0.14068  Avg Loss: 0.14068  Avg mIoU:  64.25  

*** Validation [@Epoch 214] Avg Loss: 0.18268  Avg mIoU:  62.86  ***

[Epoch: 215] [Batch: 0001/0570] Loss: 0.17364  Avg Loss: 0.17364  Avg mIoU:  38.88  
[Epoch: 215] [Batch: 0051/0570] Loss: 0.14368  Avg Loss: 0.14682  Avg mIoU:  76.01  
[Epoch: 215] [Batch: 0101/0570] Loss: 0.09306  Avg Loss: 0.14769  Avg mIoU:  76.38  
[Epoch: 215] [Batch: 0151/0570] Loss: 0.11221  Avg Loss: 0.14793  Avg mIoU:  75.48  
[Epoch: 215] [Batch: 0201/0570] Loss: 0.12860  Avg Loss: 0.14825  Avg mIoU:  75.52  
[Epoch: 215] [Batch: 0251/0570] Loss: 0.14564  Avg Loss: 0.14971  Avg mIoU:  75.11  
[Epoch: 215] [Batch: 0301/0570] Loss: 0.13372  Avg Loss: 0.15107  Avg mIoU:  75.32  
[Epoch: 215] [Batch: 0351/0570] Loss: 0.19959  Avg Loss: 0.15347  Avg mIoU:  74.88  
[Epoch: 215] [Batch: 0401/0570] Loss: 0.13225  Avg Loss: 0.15450  Avg mIoU:  74.70  
[Epoch: 215] [Batch: 0451/0570] Loss: 0.11519  Avg Loss: 0.15429  Avg mIoU:  74.71  
[Epoch: 215] [Batch: 0501/0570] Loss: 0.14802  Avg Loss: 0.15349  Avg mIoU:  74.83  
[Epoch: 215] [Batch: 0551/0570] Loss: 0.20256  Avg Loss: 0.15349  Avg mIoU:  74.83  

*** Training [@Epoch 215] Avg Loss: 0.15332  Avg mIoU:  74.90  ***

[Epoch: 215] [Batch: 0001/0050] Loss: 0.12717  Avg Loss: 0.12717  Avg mIoU:  68.58  

*** Validation [@Epoch 215] Avg Loss: 0.18400  Avg mIoU:  65.06  ***

[Epoch: 216] [Batch: 0001/0570] Loss: 0.16661  Avg Loss: 0.16661  Avg mIoU:  38.45  
[Epoch: 216] [Batch: 0051/0570] Loss: 0.17249  Avg Loss: 0.14651  Avg mIoU:  75.43  
[Epoch: 216] [Batch: 0101/0570] Loss: 0.12846  Avg Loss: 0.14454  Avg mIoU:  75.69  
[Epoch: 216] [Batch: 0151/0570] Loss: 0.25855  Avg Loss: 0.15014  Avg mIoU:  75.21  
[Epoch: 216] [Batch: 0201/0570] Loss: 0.15304  Avg Loss: 0.15200  Avg mIoU:  74.73  
[Epoch: 216] [Batch: 0251/0570] Loss: 0.14774  Avg Loss: 0.15175  Avg mIoU:  75.13  
[Epoch: 216] [Batch: 0301/0570] Loss: 0.13662  Avg Loss: 0.15242  Avg mIoU:  75.19  
[Epoch: 216] [Batch: 0351/0570] Loss: 0.09686  Avg Loss: 0.15238  Avg mIoU:  75.09  
[Epoch: 216] [Batch: 0401/0570] Loss: 0.11008  Avg Loss: 0.15204  Avg mIoU:  75.15  
[Epoch: 216] [Batch: 0451/0570] Loss: 0.13592  Avg Loss: 0.15277  Avg mIoU:  75.17  
[Epoch: 216] [Batch: 0501/0570] Loss: 0.14158  Avg Loss: 0.15197  Avg mIoU:  75.18  
[Epoch: 216] [Batch: 0551/0570] Loss: 0.10820  Avg Loss: 0.15169  Avg mIoU:  75.26  

*** Training [@Epoch 216] Avg Loss: 0.15161  Avg mIoU:  75.24  ***

[Epoch: 216] [Batch: 0001/0050] Loss: 0.12087  Avg Loss: 0.12087  Avg mIoU:  62.93  

*** Validation [@Epoch 216] Avg Loss: 0.15999  Avg mIoU:  64.36  ***

[Epoch: 217] [Batch: 0001/0570] Loss: 0.19058  Avg Loss: 0.19058  Avg mIoU:  52.16  
[Epoch: 217] [Batch: 0051/0570] Loss: 0.15109  Avg Loss: 0.14162  Avg mIoU:  75.05  
[Epoch: 217] [Batch: 0101/0570] Loss: 0.16244  Avg Loss: 0.14419  Avg mIoU:  75.29  
[Epoch: 217] [Batch: 0151/0570] Loss: 0.12524  Avg Loss: 0.14861  Avg mIoU:  74.80  
[Epoch: 217] [Batch: 0201/0570] Loss: 0.11706  Avg Loss: 0.15007  Avg mIoU:  74.86  
[Epoch: 217] [Batch: 0251/0570] Loss: 0.13318  Avg Loss: 0.15047  Avg mIoU:  74.84  
[Epoch: 217] [Batch: 0301/0570] Loss: 0.15370  Avg Loss: 0.15140  Avg mIoU:  74.90  
[Epoch: 217] [Batch: 0351/0570] Loss: 0.16877  Avg Loss: 0.15071  Avg mIoU:  75.10  
[Epoch: 217] [Batch: 0401/0570] Loss: 0.15851  Avg Loss: 0.15089  Avg mIoU:  75.15  
[Epoch: 217] [Batch: 0451/0570] Loss: 0.12969  Avg Loss: 0.15170  Avg mIoU:  75.02  
[Epoch: 217] [Batch: 0501/0570] Loss: 0.10917  Avg Loss: 0.15137  Avg mIoU:  75.16  
[Epoch: 217] [Batch: 0551/0570] Loss: 0.20262  Avg Loss: 0.15149  Avg mIoU:  75.08  

*** Training [@Epoch 217] Avg Loss: 0.15152  Avg mIoU:  75.09  ***

[Epoch: 217] [Batch: 0001/0050] Loss: 0.12481  Avg Loss: 0.12481  Avg mIoU:  67.23  

*** Validation [@Epoch 217] Avg Loss: 0.18001  Avg mIoU:  64.18  ***

[Epoch: 218] [Batch: 0001/0570] Loss: 0.14188  Avg Loss: 0.14188  Avg mIoU:  44.76  
[Epoch: 218] [Batch: 0051/0570] Loss: 0.15755  Avg Loss: 0.15113  Avg mIoU:  73.30  
[Epoch: 218] [Batch: 0101/0570] Loss: 0.17824  Avg Loss: 0.15393  Avg mIoU:  74.00  
[Epoch: 218] [Batch: 0151/0570] Loss: 0.12836  Avg Loss: 0.15254  Avg mIoU:  74.30  
[Epoch: 218] [Batch: 0201/0570] Loss: 0.10229  Avg Loss: 0.15292  Avg mIoU:  74.58  
[Epoch: 218] [Batch: 0251/0570] Loss: 0.16184  Avg Loss: 0.15376  Avg mIoU:  74.70  
[Epoch: 218] [Batch: 0301/0570] Loss: 0.14054  Avg Loss: 0.15367  Avg mIoU:  74.74  
[Epoch: 218] [Batch: 0351/0570] Loss: 0.12963  Avg Loss: 0.15482  Avg mIoU:  74.87  
[Epoch: 218] [Batch: 0401/0570] Loss: 0.12091  Avg Loss: 0.15464  Avg mIoU:  74.78  
[Epoch: 218] [Batch: 0451/0570] Loss: 0.16144  Avg Loss: 0.15357  Avg mIoU:  74.73  
[Epoch: 218] [Batch: 0501/0570] Loss: 0.16753  Avg Loss: 0.15465  Avg mIoU:  74.55  
[Epoch: 218] [Batch: 0551/0570] Loss: 0.13178  Avg Loss: 0.15439  Avg mIoU:  74.64  

*** Training [@Epoch 218] Avg Loss: 0.15414  Avg mIoU:  74.60  ***

[Epoch: 218] [Batch: 0001/0050] Loss: 0.12689  Avg Loss: 0.12689  Avg mIoU:  65.83  

*** Validation [@Epoch 218] Avg Loss: 0.16675  Avg mIoU:  65.83  ***

[Epoch: 219] [Batch: 0001/0570] Loss: 0.14244  Avg Loss: 0.14244  Avg mIoU:  48.86  
[Epoch: 219] [Batch: 0051/0570] Loss: 0.17684  Avg Loss: 0.15724  Avg mIoU:  71.14  
[Epoch: 219] [Batch: 0101/0570] Loss: 0.17121  Avg Loss: 0.15435  Avg mIoU:  73.28  
[Epoch: 219] [Batch: 0151/0570] Loss: 0.11300  Avg Loss: 0.15366  Avg mIoU:  73.96  
[Epoch: 219] [Batch: 0201/0570] Loss: 0.11736  Avg Loss: 0.15091  Avg mIoU:  74.36  
[Epoch: 219] [Batch: 0251/0570] Loss: 0.14369  Avg Loss: 0.15233  Avg mIoU:  74.50  
[Epoch: 219] [Batch: 0301/0570] Loss: 0.13136  Avg Loss: 0.15247  Avg mIoU:  74.54  
[Epoch: 219] [Batch: 0351/0570] Loss: 0.20852  Avg Loss: 0.15242  Avg mIoU:  74.65  
[Epoch: 219] [Batch: 0401/0570] Loss: 0.11868  Avg Loss: 0.15282  Avg mIoU:  74.72  
[Epoch: 219] [Batch: 0451/0570] Loss: 0.17309  Avg Loss: 0.15240  Avg mIoU:  74.72  
[Epoch: 219] [Batch: 0501/0570] Loss: 0.18479  Avg Loss: 0.15238  Avg mIoU:  74.94  
[Epoch: 219] [Batch: 0551/0570] Loss: 0.14721  Avg Loss: 0.15211  Avg mIoU:  75.08  

*** Training [@Epoch 219] Avg Loss: 0.15207  Avg mIoU:  75.07  ***

[Epoch: 219] [Batch: 0001/0050] Loss: 0.12337  Avg Loss: 0.12337  Avg mIoU:  65.71  

*** Validation [@Epoch 219] Avg Loss: 0.16263  Avg mIoU:  63.51  ***

[Epoch: 220] [Batch: 0001/0570] Loss: 0.13817  Avg Loss: 0.13817  Avg mIoU:  36.31  
[Epoch: 220] [Batch: 0051/0570] Loss: 0.12493  Avg Loss: 0.15521  Avg mIoU:  73.42  
[Epoch: 220] [Batch: 0101/0570] Loss: 0.11587  Avg Loss: 0.15678  Avg mIoU:  72.73  
[Epoch: 220] [Batch: 0151/0570] Loss: 0.15874  Avg Loss: 0.15426  Avg mIoU:  74.35  
[Epoch: 220] [Batch: 0201/0570] Loss: 0.12059  Avg Loss: 0.15416  Avg mIoU:  74.45  
[Epoch: 220] [Batch: 0251/0570] Loss: 0.14921  Avg Loss: 0.15387  Avg mIoU:  74.40  
[Epoch: 220] [Batch: 0301/0570] Loss: 0.19246  Avg Loss: 0.15607  Avg mIoU:  74.38  
[Epoch: 220] [Batch: 0351/0570] Loss: 0.17748  Avg Loss: 0.15575  Avg mIoU:  74.48  
[Epoch: 220] [Batch: 0401/0570] Loss: 0.10125  Avg Loss: 0.15451  Avg mIoU:  74.58  
[Epoch: 220] [Batch: 0451/0570] Loss: 0.17199  Avg Loss: 0.15322  Avg mIoU:  74.81  
[Epoch: 220] [Batch: 0501/0570] Loss: 0.15675  Avg Loss: 0.15282  Avg mIoU:  74.91  
[Epoch: 220] [Batch: 0551/0570] Loss: 0.11848  Avg Loss: 0.15243  Avg mIoU:  74.99  

*** Training [@Epoch 220] Avg Loss: 0.15236  Avg mIoU:  74.96  ***

[Epoch: 220] [Batch: 0001/0050] Loss: 0.12629  Avg Loss: 0.12629  Avg mIoU:  63.06  

*** Validation [@Epoch 220] Avg Loss: 0.15708  Avg mIoU:  63.38  ***

[Epoch: 221] [Batch: 0001/0570] Loss: 0.19397  Avg Loss: 0.19397  Avg mIoU:  42.62  
[Epoch: 221] [Batch: 0051/0570] Loss: 0.16047  Avg Loss: 0.15027  Avg mIoU:  73.86  
[Epoch: 221] [Batch: 0101/0570] Loss: 0.15460  Avg Loss: 0.15036  Avg mIoU:  75.79  
[Epoch: 221] [Batch: 0151/0570] Loss: 0.15270  Avg Loss: 0.15115  Avg mIoU:  75.08  
[Epoch: 221] [Batch: 0201/0570] Loss: 0.17463  Avg Loss: 0.15280  Avg mIoU:  74.60  
[Epoch: 221] [Batch: 0251/0570] Loss: 0.12450  Avg Loss: 0.15257  Avg mIoU:  74.77  
[Epoch: 221] [Batch: 0301/0570] Loss: 0.12374  Avg Loss: 0.15230  Avg mIoU:  74.96  
[Epoch: 221] [Batch: 0351/0570] Loss: 0.14759  Avg Loss: 0.15236  Avg mIoU:  75.02  
[Epoch: 221] [Batch: 0401/0570] Loss: 0.13135  Avg Loss: 0.15245  Avg mIoU:  75.02  
[Epoch: 221] [Batch: 0451/0570] Loss: 0.13174  Avg Loss: 0.15189  Avg mIoU:  75.00  
[Epoch: 221] [Batch: 0501/0570] Loss: 0.17190  Avg Loss: 0.15286  Avg mIoU:  75.12  
[Epoch: 221] [Batch: 0551/0570] Loss: 0.12655  Avg Loss: 0.15338  Avg mIoU:  75.04  

*** Training [@Epoch 221] Avg Loss: 0.15354  Avg mIoU:  75.03  ***

[Epoch: 221] [Batch: 0001/0050] Loss: 0.13030  Avg Loss: 0.13030  Avg mIoU:  68.40  

*** Validation [@Epoch 221] Avg Loss: 0.17799  Avg mIoU:  64.00  ***

[Epoch: 222] [Batch: 0001/0570] Loss: 0.14116  Avg Loss: 0.14116  Avg mIoU:  50.64  
[Epoch: 222] [Batch: 0051/0570] Loss: 0.13383  Avg Loss: 0.15714  Avg mIoU:  74.80  
[Epoch: 222] [Batch: 0101/0570] Loss: 0.24038  Avg Loss: 0.15771  Avg mIoU:  74.61  
[Epoch: 222] [Batch: 0151/0570] Loss: 0.11581  Avg Loss: 0.15403  Avg mIoU:  74.36  
[Epoch: 222] [Batch: 0201/0570] Loss: 0.20129  Avg Loss: 0.15553  Avg mIoU:  74.77  
[Epoch: 222] [Batch: 0251/0570] Loss: 0.21318  Avg Loss: 0.15316  Avg mIoU:  75.05  
[Epoch: 222] [Batch: 0301/0570] Loss: 0.18050  Avg Loss: 0.15222  Avg mIoU:  74.98  
[Epoch: 222] [Batch: 0351/0570] Loss: 0.19458  Avg Loss: 0.15151  Avg mIoU:  75.24  
[Epoch: 222] [Batch: 0401/0570] Loss: 0.21438  Avg Loss: 0.15165  Avg mIoU:  75.32  
[Epoch: 222] [Batch: 0451/0570] Loss: 0.20189  Avg Loss: 0.15195  Avg mIoU:  75.13  
[Epoch: 222] [Batch: 0501/0570] Loss: 0.18629  Avg Loss: 0.15182  Avg mIoU:  75.44  
[Epoch: 222] [Batch: 0551/0570] Loss: 0.14795  Avg Loss: 0.15170  Avg mIoU:  75.34  

*** Training [@Epoch 222] Avg Loss: 0.15137  Avg mIoU:  75.37  ***

[Epoch: 222] [Batch: 0001/0050] Loss: 0.11764  Avg Loss: 0.11764  Avg mIoU:  67.11  

*** Validation [@Epoch 222] Avg Loss: 0.16351  Avg mIoU:  64.96  ***

[Epoch: 223] [Batch: 0001/0570] Loss: 0.11715  Avg Loss: 0.11715  Avg mIoU:  42.59  
[Epoch: 223] [Batch: 0051/0570] Loss: 0.15094  Avg Loss: 0.15054  Avg mIoU:  74.52  
[Epoch: 223] [Batch: 0101/0570] Loss: 0.13192  Avg Loss: 0.14883  Avg mIoU:  76.05  
[Epoch: 223] [Batch: 0151/0570] Loss: 0.13749  Avg Loss: 0.15289  Avg mIoU:  75.48  
[Epoch: 223] [Batch: 0201/0570] Loss: 0.14462  Avg Loss: 0.15224  Avg mIoU:  75.52  
[Epoch: 223] [Batch: 0251/0570] Loss: 0.15005  Avg Loss: 0.15187  Avg mIoU:  75.49  
[Epoch: 223] [Batch: 0301/0570] Loss: 0.16297  Avg Loss: 0.15313  Avg mIoU:  75.19  
[Epoch: 223] [Batch: 0351/0570] Loss: 0.20000  Avg Loss: 0.15370  Avg mIoU:  75.23  
[Epoch: 223] [Batch: 0401/0570] Loss: 0.22383  Avg Loss: 0.15311  Avg mIoU:  75.20  
[Epoch: 223] [Batch: 0451/0570] Loss: 0.33180  Avg Loss: 0.15179  Avg mIoU:  75.12  
[Epoch: 223] [Batch: 0501/0570] Loss: 0.15114  Avg Loss: 0.15188  Avg mIoU:  75.04  
[Epoch: 223] [Batch: 0551/0570] Loss: 0.19608  Avg Loss: 0.15139  Avg mIoU:  75.21  

*** Training [@Epoch 223] Avg Loss: 0.15126  Avg mIoU:  75.24  ***

[Epoch: 223] [Batch: 0001/0050] Loss: 0.15538  Avg Loss: 0.15538  Avg mIoU:  65.46  

*** Validation [@Epoch 223] Avg Loss: 0.20886  Avg mIoU:  64.59  ***

[Epoch: 224] [Batch: 0001/0570] Loss: 0.16933  Avg Loss: 0.16933  Avg mIoU:  60.81  
[Epoch: 224] [Batch: 0051/0570] Loss: 0.21825  Avg Loss: 0.14814  Avg mIoU:  76.93  
[Epoch: 224] [Batch: 0101/0570] Loss: 0.13487  Avg Loss: 0.14845  Avg mIoU:  76.07  
[Epoch: 224] [Batch: 0151/0570] Loss: 0.18751  Avg Loss: 0.15275  Avg mIoU:  75.44  
[Epoch: 224] [Batch: 0201/0570] Loss: 0.17788  Avg Loss: 0.15217  Avg mIoU:  75.26  
[Epoch: 224] [Batch: 0251/0570] Loss: 0.14544  Avg Loss: 0.15126  Avg mIoU:  75.41  
[Epoch: 224] [Batch: 0301/0570] Loss: 0.13722  Avg Loss: 0.15082  Avg mIoU:  75.34  
[Epoch: 224] [Batch: 0351/0570] Loss: 0.14721  Avg Loss: 0.14993  Avg mIoU:  75.38  
[Epoch: 224] [Batch: 0401/0570] Loss: 0.12317  Avg Loss: 0.15057  Avg mIoU:  75.26  
[Epoch: 224] [Batch: 0451/0570] Loss: 0.15130  Avg Loss: 0.14975  Avg mIoU:  75.27  
[Epoch: 224] [Batch: 0501/0570] Loss: 0.12899  Avg Loss: 0.14939  Avg mIoU:  75.31  
[Epoch: 224] [Batch: 0551/0570] Loss: 0.14200  Avg Loss: 0.15026  Avg mIoU:  75.22  

*** Training [@Epoch 224] Avg Loss: 0.15004  Avg mIoU:  75.30  ***

[Epoch: 224] [Batch: 0001/0050] Loss: 0.11986  Avg Loss: 0.11986  Avg mIoU:  65.43  

*** Validation [@Epoch 224] Avg Loss: 0.16261  Avg mIoU:  63.39  ***

[Epoch: 225] [Batch: 0001/0570] Loss: 0.09525  Avg Loss: 0.09525  Avg mIoU:  59.35  
[Epoch: 225] [Batch: 0051/0570] Loss: 0.19464  Avg Loss: 0.14618  Avg mIoU:  74.54  
[Epoch: 225] [Batch: 0101/0570] Loss: 0.15615  Avg Loss: 0.15104  Avg mIoU:  74.27  
[Epoch: 225] [Batch: 0151/0570] Loss: 0.15206  Avg Loss: 0.15516  Avg mIoU:  74.31  
[Epoch: 225] [Batch: 0201/0570] Loss: 0.14313  Avg Loss: 0.15398  Avg mIoU:  74.76  
[Epoch: 225] [Batch: 0251/0570] Loss: 0.16114  Avg Loss: 0.15267  Avg mIoU:  74.87  
[Epoch: 225] [Batch: 0301/0570] Loss: 0.16050  Avg Loss: 0.15180  Avg mIoU:  74.88  
[Epoch: 225] [Batch: 0351/0570] Loss: 0.16942  Avg Loss: 0.15178  Avg mIoU:  74.65  
[Epoch: 225] [Batch: 0401/0570] Loss: 0.27342  Avg Loss: 0.15178  Avg mIoU:  74.99  
[Epoch: 225] [Batch: 0451/0570] Loss: 0.11130  Avg Loss: 0.15060  Avg mIoU:  75.36  
[Epoch: 225] [Batch: 0501/0570] Loss: 0.16796  Avg Loss: 0.15061  Avg mIoU:  75.48  
[Epoch: 225] [Batch: 0551/0570] Loss: 0.17363  Avg Loss: 0.15056  Avg mIoU:  75.48  

*** Training [@Epoch 225] Avg Loss: 0.15076  Avg mIoU:  75.36  ***

[Epoch: 225] [Batch: 0001/0050] Loss: 0.13709  Avg Loss: 0.13709  Avg mIoU:  65.81  

*** Validation [@Epoch 225] Avg Loss: 0.16931  Avg mIoU:  65.97  ***

[Epoch: 226] [Batch: 0001/0570] Loss: 0.13551  Avg Loss: 0.13551  Avg mIoU:  48.10  
[Epoch: 226] [Batch: 0051/0570] Loss: 0.19818  Avg Loss: 0.15377  Avg mIoU:  75.29  
[Epoch: 226] [Batch: 0101/0570] Loss: 0.11056  Avg Loss: 0.14953  Avg mIoU:  75.67  
[Epoch: 226] [Batch: 0151/0570] Loss: 0.28015  Avg Loss: 0.15122  Avg mIoU:  75.20  
[Epoch: 226] [Batch: 0201/0570] Loss: 0.21742  Avg Loss: 0.15089  Avg mIoU:  75.12  
[Epoch: 226] [Batch: 0251/0570] Loss: 0.15213  Avg Loss: 0.15080  Avg mIoU:  75.21  
[Epoch: 226] [Batch: 0301/0570] Loss: 0.21301  Avg Loss: 0.15089  Avg mIoU:  75.24  
[Epoch: 226] [Batch: 0351/0570] Loss: 0.21619  Avg Loss: 0.15064  Avg mIoU:  75.24  
[Epoch: 226] [Batch: 0401/0570] Loss: 0.12034  Avg Loss: 0.15099  Avg mIoU:  75.30  
[Epoch: 226] [Batch: 0451/0570] Loss: 0.14619  Avg Loss: 0.15120  Avg mIoU:  75.31  
[Epoch: 226] [Batch: 0501/0570] Loss: 0.11083  Avg Loss: 0.15147  Avg mIoU:  75.16  
[Epoch: 226] [Batch: 0551/0570] Loss: 0.16714  Avg Loss: 0.15091  Avg mIoU:  75.05  

*** Training [@Epoch 226] Avg Loss: 0.15070  Avg mIoU:  75.12  ***

[Epoch: 226] [Batch: 0001/0050] Loss: 0.13927  Avg Loss: 0.13927  Avg mIoU:  65.99  

*** Validation [@Epoch 226] Avg Loss: 0.17838  Avg mIoU:  64.57  ***

[Epoch: 227] [Batch: 0001/0570] Loss: 0.16111  Avg Loss: 0.16111  Avg mIoU:  40.54  
[Epoch: 227] [Batch: 0051/0570] Loss: 0.08486  Avg Loss: 0.13980  Avg mIoU:  75.53  
[Epoch: 227] [Batch: 0101/0570] Loss: 0.14656  Avg Loss: 0.14718  Avg mIoU:  75.72  
[Epoch: 227] [Batch: 0151/0570] Loss: 0.15049  Avg Loss: 0.14784  Avg mIoU:  75.16  
[Epoch: 227] [Batch: 0201/0570] Loss: 0.10178  Avg Loss: 0.14839  Avg mIoU:  75.40  
[Epoch: 227] [Batch: 0251/0570] Loss: 0.12610  Avg Loss: 0.15043  Avg mIoU:  75.23  
[Epoch: 227] [Batch: 0301/0570] Loss: 0.16972  Avg Loss: 0.15130  Avg mIoU:  75.02  
[Epoch: 227] [Batch: 0351/0570] Loss: 0.21280  Avg Loss: 0.15177  Avg mIoU:  75.10  
[Epoch: 227] [Batch: 0401/0570] Loss: 0.13847  Avg Loss: 0.15115  Avg mIoU:  75.43  
[Epoch: 227] [Batch: 0451/0570] Loss: 0.14037  Avg Loss: 0.15107  Avg mIoU:  75.48  
[Epoch: 227] [Batch: 0501/0570] Loss: 0.18780  Avg Loss: 0.15034  Avg mIoU:  75.54  
[Epoch: 227] [Batch: 0551/0570] Loss: 0.11592  Avg Loss: 0.14998  Avg mIoU:  75.40  

*** Training [@Epoch 227] Avg Loss: 0.14975  Avg mIoU:  75.44  ***

[Epoch: 227] [Batch: 0001/0050] Loss: 0.16680  Avg Loss: 0.16680  Avg mIoU:  63.30  

*** Validation [@Epoch 227] Avg Loss: 0.20283  Avg mIoU:  65.72  ***

[Epoch: 228] [Batch: 0001/0570] Loss: 0.15881  Avg Loss: 0.15881  Avg mIoU:  42.70  
[Epoch: 228] [Batch: 0051/0570] Loss: 0.12892  Avg Loss: 0.15055  Avg mIoU:  76.21  
[Epoch: 228] [Batch: 0101/0570] Loss: 0.12669  Avg Loss: 0.15004  Avg mIoU:  75.23  
[Epoch: 228] [Batch: 0151/0570] Loss: 0.15416  Avg Loss: 0.15050  Avg mIoU:  75.91  
[Epoch: 228] [Batch: 0201/0570] Loss: 0.15711  Avg Loss: 0.14857  Avg mIoU:  75.89  
[Epoch: 228] [Batch: 0251/0570] Loss: 0.17108  Avg Loss: 0.14859  Avg mIoU:  75.58  
[Epoch: 228] [Batch: 0301/0570] Loss: 0.13925  Avg Loss: 0.14865  Avg mIoU:  75.45  
[Epoch: 228] [Batch: 0351/0570] Loss: 0.14008  Avg Loss: 0.14778  Avg mIoU:  75.48  
[Epoch: 228] [Batch: 0401/0570] Loss: 0.16257  Avg Loss: 0.14831  Avg mIoU:  75.30  
[Epoch: 228] [Batch: 0451/0570] Loss: 0.15676  Avg Loss: 0.14872  Avg mIoU:  75.44  
[Epoch: 228] [Batch: 0501/0570] Loss: 0.16569  Avg Loss: 0.14845  Avg mIoU:  75.52  
[Epoch: 228] [Batch: 0551/0570] Loss: 0.13117  Avg Loss: 0.14829  Avg mIoU:  75.52  

*** Training [@Epoch 228] Avg Loss: 0.14840  Avg mIoU:  75.57  ***

[Epoch: 228] [Batch: 0001/0050] Loss: 0.11407  Avg Loss: 0.11407  Avg mIoU:  65.53  

*** Validation [@Epoch 228] Avg Loss: 0.15418  Avg mIoU:  65.33  ***

[Epoch: 229] [Batch: 0001/0570] Loss: 0.16772  Avg Loss: 0.16772  Avg mIoU:  59.21  
[Epoch: 229] [Batch: 0051/0570] Loss: 0.16912  Avg Loss: 0.14235  Avg mIoU:  77.27  
[Epoch: 229] [Batch: 0101/0570] Loss: 0.22425  Avg Loss: 0.14959  Avg mIoU:  76.65  
[Epoch: 229] [Batch: 0151/0570] Loss: 0.17997  Avg Loss: 0.15161  Avg mIoU:  76.67  
[Epoch: 229] [Batch: 0201/0570] Loss: 0.14704  Avg Loss: 0.15140  Avg mIoU:  76.39  
[Epoch: 229] [Batch: 0251/0570] Loss: 0.13314  Avg Loss: 0.15109  Avg mIoU:  76.14  
[Epoch: 229] [Batch: 0301/0570] Loss: 0.08391  Avg Loss: 0.14923  Avg mIoU:  76.20  
[Epoch: 229] [Batch: 0351/0570] Loss: 0.17250  Avg Loss: 0.15050  Avg mIoU:  75.76  
[Epoch: 229] [Batch: 0401/0570] Loss: 0.13863  Avg Loss: 0.14849  Avg mIoU:  76.08  
[Epoch: 229] [Batch: 0451/0570] Loss: 0.11048  Avg Loss: 0.14830  Avg mIoU:  76.05  
[Epoch: 229] [Batch: 0501/0570] Loss: 0.13965  Avg Loss: 0.14871  Avg mIoU:  75.86  
[Epoch: 229] [Batch: 0551/0570] Loss: 0.17747  Avg Loss: 0.14941  Avg mIoU:  75.50  

*** Training [@Epoch 229] Avg Loss: 0.14964  Avg mIoU:  75.50  ***

[Epoch: 229] [Batch: 0001/0050] Loss: 0.14371  Avg Loss: 0.14371  Avg mIoU:  63.06  

*** Validation [@Epoch 229] Avg Loss: 0.17557  Avg mIoU:  64.02  ***

[Epoch: 230] [Batch: 0001/0570] Loss: 0.20153  Avg Loss: 0.20153  Avg mIoU:  37.52  
[Epoch: 230] [Batch: 0051/0570] Loss: 0.17678  Avg Loss: 0.15532  Avg mIoU:  75.25  
[Epoch: 230] [Batch: 0101/0570] Loss: 0.12066  Avg Loss: 0.15134  Avg mIoU:  74.99  
[Epoch: 230] [Batch: 0151/0570] Loss: 0.13322  Avg Loss: 0.15276  Avg mIoU:  74.55  
[Epoch: 230] [Batch: 0201/0570] Loss: 0.16569  Avg Loss: 0.15159  Avg mIoU:  75.04  
[Epoch: 230] [Batch: 0251/0570] Loss: 0.12599  Avg Loss: 0.15141  Avg mIoU:  74.89  
[Epoch: 230] [Batch: 0301/0570] Loss: 0.19690  Avg Loss: 0.15212  Avg mIoU:  75.00  
[Epoch: 230] [Batch: 0351/0570] Loss: 0.10203  Avg Loss: 0.15102  Avg mIoU:  75.26  
[Epoch: 230] [Batch: 0401/0570] Loss: 0.19265  Avg Loss: 0.15076  Avg mIoU:  75.34  
[Epoch: 230] [Batch: 0451/0570] Loss: 0.16120  Avg Loss: 0.15041  Avg mIoU:  75.43  
[Epoch: 230] [Batch: 0501/0570] Loss: 0.15485  Avg Loss: 0.14978  Avg mIoU:  75.34  
[Epoch: 230] [Batch: 0551/0570] Loss: 0.16033  Avg Loss: 0.15067  Avg mIoU:  75.41  

*** Training [@Epoch 230] Avg Loss: 0.15076  Avg mIoU:  75.46  ***

[Epoch: 230] [Batch: 0001/0050] Loss: 0.12299  Avg Loss: 0.12299  Avg mIoU:  63.88  

*** Validation [@Epoch 230] Avg Loss: 0.16762  Avg mIoU:  65.12  ***

[Epoch: 231] [Batch: 0001/0570] Loss: 0.12363  Avg Loss: 0.12363  Avg mIoU:  42.87  
[Epoch: 231] [Batch: 0051/0570] Loss: 0.10107  Avg Loss: 0.15001  Avg mIoU:  75.21  
[Epoch: 231] [Batch: 0101/0570] Loss: 0.11644  Avg Loss: 0.14774  Avg mIoU:  74.82  
[Epoch: 231] [Batch: 0151/0570] Loss: 0.13398  Avg Loss: 0.14799  Avg mIoU:  74.94  
[Epoch: 231] [Batch: 0201/0570] Loss: 0.14342  Avg Loss: 0.14922  Avg mIoU:  74.99  
[Epoch: 231] [Batch: 0251/0570] Loss: 0.18007  Avg Loss: 0.14842  Avg mIoU:  75.68  
[Epoch: 231] [Batch: 0301/0570] Loss: 0.16109  Avg Loss: 0.14846  Avg mIoU:  75.70  
[Epoch: 231] [Batch: 0351/0570] Loss: 0.23083  Avg Loss: 0.14884  Avg mIoU:  75.49  
[Epoch: 231] [Batch: 0401/0570] Loss: 0.14974  Avg Loss: 0.14941  Avg mIoU:  75.37  
[Epoch: 231] [Batch: 0451/0570] Loss: 0.18798  Avg Loss: 0.14995  Avg mIoU:  75.33  
[Epoch: 231] [Batch: 0501/0570] Loss: 0.12531  Avg Loss: 0.14969  Avg mIoU:  75.23  
[Epoch: 231] [Batch: 0551/0570] Loss: 0.11083  Avg Loss: 0.15113  Avg mIoU:  75.27  

*** Training [@Epoch 231] Avg Loss: 0.15077  Avg mIoU:  75.29  ***

[Epoch: 231] [Batch: 0001/0050] Loss: 0.11219  Avg Loss: 0.11219  Avg mIoU:  65.86  

*** Validation [@Epoch 231] Avg Loss: 0.14951  Avg mIoU:  64.96  ***

[Epoch: 232] [Batch: 0001/0570] Loss: 0.18516  Avg Loss: 0.18516  Avg mIoU:  27.25  
[Epoch: 232] [Batch: 0051/0570] Loss: 0.19665  Avg Loss: 0.14870  Avg mIoU:  75.91  
[Epoch: 232] [Batch: 0101/0570] Loss: 0.12581  Avg Loss: 0.14882  Avg mIoU:  75.49  
[Epoch: 232] [Batch: 0151/0570] Loss: 0.19852  Avg Loss: 0.14946  Avg mIoU:  74.97  
[Epoch: 232] [Batch: 0201/0570] Loss: 0.11852  Avg Loss: 0.14823  Avg mIoU:  75.67  
[Epoch: 232] [Batch: 0251/0570] Loss: 0.13241  Avg Loss: 0.14828  Avg mIoU:  75.56  
[Epoch: 232] [Batch: 0301/0570] Loss: 0.14032  Avg Loss: 0.14955  Avg mIoU:  75.38  
[Epoch: 232] [Batch: 0351/0570] Loss: 0.16243  Avg Loss: 0.14918  Avg mIoU:  75.41  
[Epoch: 232] [Batch: 0401/0570] Loss: 0.14143  Avg Loss: 0.14946  Avg mIoU:  75.40  
[Epoch: 232] [Batch: 0451/0570] Loss: 0.15353  Avg Loss: 0.14886  Avg mIoU:  75.56  
[Epoch: 232] [Batch: 0501/0570] Loss: 0.20155  Avg Loss: 0.14855  Avg mIoU:  75.61  
[Epoch: 232] [Batch: 0551/0570] Loss: 0.20328  Avg Loss: 0.14866  Avg mIoU:  75.57  

*** Training [@Epoch 232] Avg Loss: 0.14884  Avg mIoU:  75.56  ***

[Epoch: 232] [Batch: 0001/0050] Loss: 0.12361  Avg Loss: 0.12361  Avg mIoU:  68.68  

*** Validation [@Epoch 232] Avg Loss: 0.17437  Avg mIoU:  64.69  ***

[Epoch: 233] [Batch: 0001/0570] Loss: 0.15384  Avg Loss: 0.15384  Avg mIoU:  48.94  
[Epoch: 233] [Batch: 0051/0570] Loss: 0.12170  Avg Loss: 0.14357  Avg mIoU:  77.24  
[Epoch: 233] [Batch: 0101/0570] Loss: 0.12777  Avg Loss: 0.14540  Avg mIoU:  76.44  
[Epoch: 233] [Batch: 0151/0570] Loss: 0.19003  Avg Loss: 0.14552  Avg mIoU:  76.30  
[Epoch: 233] [Batch: 0201/0570] Loss: 0.14206  Avg Loss: 0.14428  Avg mIoU:  76.11  
[Epoch: 233] [Batch: 0251/0570] Loss: 0.14305  Avg Loss: 0.14562  Avg mIoU:  75.60  
[Epoch: 233] [Batch: 0301/0570] Loss: 0.12906  Avg Loss: 0.14629  Avg mIoU:  75.57  
[Epoch: 233] [Batch: 0351/0570] Loss: 0.16015  Avg Loss: 0.14720  Avg mIoU:  75.32  
[Epoch: 233] [Batch: 0401/0570] Loss: 0.19725  Avg Loss: 0.14763  Avg mIoU:  75.42  
[Epoch: 233] [Batch: 0451/0570] Loss: 0.21100  Avg Loss: 0.14720  Avg mIoU:  75.69  
[Epoch: 233] [Batch: 0501/0570] Loss: 0.13734  Avg Loss: 0.14806  Avg mIoU:  75.43  
[Epoch: 233] [Batch: 0551/0570] Loss: 0.17062  Avg Loss: 0.14902  Avg mIoU:  75.60  

*** Training [@Epoch 233] Avg Loss: 0.14877  Avg mIoU:  75.65  ***

[Epoch: 233] [Batch: 0001/0050] Loss: 0.12657  Avg Loss: 0.12657  Avg mIoU:  66.91  

*** Validation [@Epoch 233] Avg Loss: 0.18892  Avg mIoU:  65.33  ***

[Epoch: 234] [Batch: 0001/0570] Loss: 0.18252  Avg Loss: 0.18252  Avg mIoU:  53.62  
[Epoch: 234] [Batch: 0051/0570] Loss: 0.13403  Avg Loss: 0.14589  Avg mIoU:  75.82  
[Epoch: 234] [Batch: 0101/0570] Loss: 0.15142  Avg Loss: 0.15078  Avg mIoU:  74.76  
[Epoch: 234] [Batch: 0151/0570] Loss: 0.25534  Avg Loss: 0.15212  Avg mIoU:  75.03  
[Epoch: 234] [Batch: 0201/0570] Loss: 0.15846  Avg Loss: 0.15152  Avg mIoU:  75.03  
[Epoch: 234] [Batch: 0251/0570] Loss: 0.13088  Avg Loss: 0.15109  Avg mIoU:  74.86  
[Epoch: 234] [Batch: 0301/0570] Loss: 0.18951  Avg Loss: 0.15010  Avg mIoU:  74.94  
[Epoch: 234] [Batch: 0351/0570] Loss: 0.17358  Avg Loss: 0.15019  Avg mIoU:  75.20  
[Epoch: 234] [Batch: 0401/0570] Loss: 0.15366  Avg Loss: 0.15041  Avg mIoU:  75.16  
[Epoch: 234] [Batch: 0451/0570] Loss: 0.14005  Avg Loss: 0.15012  Avg mIoU:  75.24  
[Epoch: 234] [Batch: 0501/0570] Loss: 0.17569  Avg Loss: 0.15078  Avg mIoU:  75.21  
[Epoch: 234] [Batch: 0551/0570] Loss: 0.10757  Avg Loss: 0.15015  Avg mIoU:  75.42  

*** Training [@Epoch 234] Avg Loss: 0.14986  Avg mIoU:  75.44  ***

[Epoch: 234] [Batch: 0001/0050] Loss: 0.12879  Avg Loss: 0.12879  Avg mIoU:  66.10  

*** Validation [@Epoch 234] Avg Loss: 0.16720  Avg mIoU:  65.42  ***

[Epoch: 235] [Batch: 0001/0570] Loss: 0.11486  Avg Loss: 0.11486  Avg mIoU:  53.64  
[Epoch: 235] [Batch: 0051/0570] Loss: 0.11519  Avg Loss: 0.14261  Avg mIoU:  76.12  
[Epoch: 235] [Batch: 0101/0570] Loss: 0.15816  Avg Loss: 0.14677  Avg mIoU:  75.55  
[Epoch: 235] [Batch: 0151/0570] Loss: 0.14780  Avg Loss: 0.14731  Avg mIoU:  76.34  
[Epoch: 235] [Batch: 0201/0570] Loss: 0.11394  Avg Loss: 0.14514  Avg mIoU:  76.61  
[Epoch: 235] [Batch: 0251/0570] Loss: 0.15349  Avg Loss: 0.14700  Avg mIoU:  76.15  
[Epoch: 235] [Batch: 0301/0570] Loss: 0.13786  Avg Loss: 0.14950  Avg mIoU:  76.03  
[Epoch: 235] [Batch: 0351/0570] Loss: 0.18838  Avg Loss: 0.15003  Avg mIoU:  75.59  
[Epoch: 235] [Batch: 0401/0570] Loss: 0.14239  Avg Loss: 0.14971  Avg mIoU:  75.62  
[Epoch: 235] [Batch: 0451/0570] Loss: 0.19792  Avg Loss: 0.14946  Avg mIoU:  75.48  
[Epoch: 235] [Batch: 0501/0570] Loss: 0.10844  Avg Loss: 0.14914  Avg mIoU:  75.44  
[Epoch: 235] [Batch: 0551/0570] Loss: 0.16140  Avg Loss: 0.14895  Avg mIoU:  75.37  

*** Training [@Epoch 235] Avg Loss: 0.14912  Avg mIoU:  75.37  ***

[Epoch: 235] [Batch: 0001/0050] Loss: 0.12823  Avg Loss: 0.12823  Avg mIoU:  68.35  

*** Validation [@Epoch 235] Avg Loss: 0.16680  Avg mIoU:  64.75  ***

[Epoch: 236] [Batch: 0001/0570] Loss: 0.14789  Avg Loss: 0.14789  Avg mIoU:  42.55  
[Epoch: 236] [Batch: 0051/0570] Loss: 0.14228  Avg Loss: 0.14647  Avg mIoU:  76.76  
[Epoch: 236] [Batch: 0101/0570] Loss: 0.16978  Avg Loss: 0.14494  Avg mIoU:  77.22  
[Epoch: 236] [Batch: 0151/0570] Loss: 0.20247  Avg Loss: 0.14958  Avg mIoU:  75.98  
[Epoch: 236] [Batch: 0201/0570] Loss: 0.18776  Avg Loss: 0.15025  Avg mIoU:  75.62  
[Epoch: 236] [Batch: 0251/0570] Loss: 0.14072  Avg Loss: 0.15073  Avg mIoU:  75.26  
[Epoch: 236] [Batch: 0301/0570] Loss: 0.15491  Avg Loss: 0.14950  Avg mIoU:  75.33  
[Epoch: 236] [Batch: 0351/0570] Loss: 0.20280  Avg Loss: 0.14884  Avg mIoU:  75.08  
[Epoch: 236] [Batch: 0401/0570] Loss: 0.12871  Avg Loss: 0.14844  Avg mIoU:  75.23  
[Epoch: 236] [Batch: 0451/0570] Loss: 0.15778  Avg Loss: 0.14877  Avg mIoU:  75.30  
[Epoch: 236] [Batch: 0501/0570] Loss: 0.13060  Avg Loss: 0.14945  Avg mIoU:  75.19  
[Epoch: 236] [Batch: 0551/0570] Loss: 0.13743  Avg Loss: 0.14972  Avg mIoU:  75.24  

*** Training [@Epoch 236] Avg Loss: 0.14960  Avg mIoU:  75.26  ***

[Epoch: 236] [Batch: 0001/0050] Loss: 0.12750  Avg Loss: 0.12750  Avg mIoU:  67.55  

*** Validation [@Epoch 236] Avg Loss: 0.17616  Avg mIoU:  64.57  ***

[Epoch: 237] [Batch: 0001/0570] Loss: 0.11255  Avg Loss: 0.11255  Avg mIoU:  49.64  
[Epoch: 237] [Batch: 0051/0570] Loss: 0.10522  Avg Loss: 0.14892  Avg mIoU:  76.67  
[Epoch: 237] [Batch: 0101/0570] Loss: 0.11714  Avg Loss: 0.14804  Avg mIoU:  75.91  
[Epoch: 237] [Batch: 0151/0570] Loss: 0.09917  Avg Loss: 0.14696  Avg mIoU:  76.20  
[Epoch: 237] [Batch: 0201/0570] Loss: 0.18541  Avg Loss: 0.14939  Avg mIoU:  75.90  
[Epoch: 237] [Batch: 0251/0570] Loss: 0.15167  Avg Loss: 0.14868  Avg mIoU:  75.99  
[Epoch: 237] [Batch: 0301/0570] Loss: 0.15651  Avg Loss: 0.14839  Avg mIoU:  76.01  
[Epoch: 237] [Batch: 0351/0570] Loss: 0.13419  Avg Loss: 0.14718  Avg mIoU:  76.10  
[Epoch: 237] [Batch: 0401/0570] Loss: 0.14107  Avg Loss: 0.14840  Avg mIoU:  75.74  
[Epoch: 237] [Batch: 0451/0570] Loss: 0.15521  Avg Loss: 0.14861  Avg mIoU:  75.66  
[Epoch: 237] [Batch: 0501/0570] Loss: 0.14474  Avg Loss: 0.14860  Avg mIoU:  75.79  
[Epoch: 237] [Batch: 0551/0570] Loss: 0.11519  Avg Loss: 0.14868  Avg mIoU:  75.69  

*** Training [@Epoch 237] Avg Loss: 0.14896  Avg mIoU:  75.65  ***

[Epoch: 237] [Batch: 0001/0050] Loss: 0.14562  Avg Loss: 0.14562  Avg mIoU:  65.73  

*** Validation [@Epoch 237] Avg Loss: 0.19113  Avg mIoU:  65.45  ***

[Epoch: 238] [Batch: 0001/0570] Loss: 0.18106  Avg Loss: 0.18106  Avg mIoU:  42.74  
[Epoch: 238] [Batch: 0051/0570] Loss: 0.10194  Avg Loss: 0.13170  Avg mIoU:  76.22  
[Epoch: 238] [Batch: 0101/0570] Loss: 0.10895  Avg Loss: 0.13731  Avg mIoU:  76.83  
[Epoch: 238] [Batch: 0151/0570] Loss: 0.14122  Avg Loss: 0.14036  Avg mIoU:  76.62  
[Epoch: 238] [Batch: 0201/0570] Loss: 0.19325  Avg Loss: 0.14235  Avg mIoU:  76.19  
[Epoch: 238] [Batch: 0251/0570] Loss: 0.22029  Avg Loss: 0.14406  Avg mIoU:  76.03  
[Epoch: 238] [Batch: 0301/0570] Loss: 0.19395  Avg Loss: 0.14617  Avg mIoU:  75.73  
[Epoch: 238] [Batch: 0351/0570] Loss: 0.10307  Avg Loss: 0.14643  Avg mIoU:  75.84  
[Epoch: 238] [Batch: 0401/0570] Loss: 0.16927  Avg Loss: 0.14665  Avg mIoU:  75.88  
[Epoch: 238] [Batch: 0451/0570] Loss: 0.13065  Avg Loss: 0.14711  Avg mIoU:  76.11  
[Epoch: 238] [Batch: 0501/0570] Loss: 0.16272  Avg Loss: 0.14700  Avg mIoU:  76.15  
[Epoch: 238] [Batch: 0551/0570] Loss: 0.18847  Avg Loss: 0.14724  Avg mIoU:  75.89  

*** Training [@Epoch 238] Avg Loss: 0.14743  Avg mIoU:  75.92  ***

[Epoch: 238] [Batch: 0001/0050] Loss: 0.14226  Avg Loss: 0.14226  Avg mIoU:  65.69  

*** Validation [@Epoch 238] Avg Loss: 0.17350  Avg mIoU:  63.00  ***

[Epoch: 239] [Batch: 0001/0570] Loss: 0.11260  Avg Loss: 0.11260  Avg mIoU:  64.17  
[Epoch: 239] [Batch: 0051/0570] Loss: 0.15479  Avg Loss: 0.14393  Avg mIoU:  75.99  
[Epoch: 239] [Batch: 0101/0570] Loss: 0.09586  Avg Loss: 0.14611  Avg mIoU:  75.64  
[Epoch: 239] [Batch: 0151/0570] Loss: 0.19023  Avg Loss: 0.14640  Avg mIoU:  75.81  
[Epoch: 239] [Batch: 0201/0570] Loss: 0.18716  Avg Loss: 0.14640  Avg mIoU:  76.09  
[Epoch: 239] [Batch: 0251/0570] Loss: 0.14090  Avg Loss: 0.14505  Avg mIoU:  76.13  
[Epoch: 239] [Batch: 0301/0570] Loss: 0.16849  Avg Loss: 0.14536  Avg mIoU:  76.12  
[Epoch: 239] [Batch: 0351/0570] Loss: 0.16575  Avg Loss: 0.14639  Avg mIoU:  75.97  
[Epoch: 239] [Batch: 0401/0570] Loss: 0.11747  Avg Loss: 0.14739  Avg mIoU:  75.81  
[Epoch: 239] [Batch: 0451/0570] Loss: 0.14608  Avg Loss: 0.14777  Avg mIoU:  75.77  
[Epoch: 239] [Batch: 0501/0570] Loss: 0.11251  Avg Loss: 0.14752  Avg mIoU:  75.86  
[Epoch: 239] [Batch: 0551/0570] Loss: 0.11968  Avg Loss: 0.14835  Avg mIoU:  75.70  

*** Training [@Epoch 239] Avg Loss: 0.14879  Avg mIoU:  75.65  ***

[Epoch: 239] [Batch: 0001/0050] Loss: 0.12083  Avg Loss: 0.12083  Avg mIoU:  66.17  

*** Validation [@Epoch 239] Avg Loss: 0.15812  Avg mIoU:  63.68  ***

[Epoch: 240] [Batch: 0001/0570] Loss: 0.11576  Avg Loss: 0.11576  Avg mIoU:  46.17  
[Epoch: 240] [Batch: 0051/0570] Loss: 0.15078  Avg Loss: 0.14257  Avg mIoU:  75.31  
[Epoch: 240] [Batch: 0101/0570] Loss: 0.18558  Avg Loss: 0.14878  Avg mIoU:  75.70  
[Epoch: 240] [Batch: 0151/0570] Loss: 0.18363  Avg Loss: 0.14697  Avg mIoU:  75.83  
[Epoch: 240] [Batch: 0201/0570] Loss: 0.12583  Avg Loss: 0.14447  Avg mIoU:  76.39  
[Epoch: 240] [Batch: 0251/0570] Loss: 0.15309  Avg Loss: 0.14627  Avg mIoU:  75.95  
[Epoch: 240] [Batch: 0301/0570] Loss: 0.18128  Avg Loss: 0.14654  Avg mIoU:  76.01  
[Epoch: 240] [Batch: 0351/0570] Loss: 0.09672  Avg Loss: 0.14673  Avg mIoU:  75.88  
[Epoch: 240] [Batch: 0401/0570] Loss: 0.15825  Avg Loss: 0.14666  Avg mIoU:  75.86  
[Epoch: 240] [Batch: 0451/0570] Loss: 0.15922  Avg Loss: 0.14596  Avg mIoU:  75.94  
[Epoch: 240] [Batch: 0501/0570] Loss: 0.10947  Avg Loss: 0.14636  Avg mIoU:  75.81  
[Epoch: 240] [Batch: 0551/0570] Loss: 0.17616  Avg Loss: 0.14649  Avg mIoU:  75.86  

*** Training [@Epoch 240] Avg Loss: 0.14715  Avg mIoU:  75.75  ***

[Epoch: 240] [Batch: 0001/0050] Loss: 0.16796  Avg Loss: 0.16796  Avg mIoU:  64.23  

*** Validation [@Epoch 240] Avg Loss: 0.19928  Avg mIoU:  64.51  ***

[Epoch: 241] [Batch: 0001/0570] Loss: 0.17541  Avg Loss: 0.17541  Avg mIoU:  39.58  
[Epoch: 241] [Batch: 0051/0570] Loss: 0.12376  Avg Loss: 0.15076  Avg mIoU:  74.91  
[Epoch: 241] [Batch: 0101/0570] Loss: 0.15477  Avg Loss: 0.14722  Avg mIoU:  76.56  
[Epoch: 241] [Batch: 0151/0570] Loss: 0.10184  Avg Loss: 0.14576  Avg mIoU:  76.06  
[Epoch: 241] [Batch: 0201/0570] Loss: 0.16979  Avg Loss: 0.14706  Avg mIoU:  75.87  
[Epoch: 241] [Batch: 0251/0570] Loss: 0.19673  Avg Loss: 0.14452  Avg mIoU:  76.14  
[Epoch: 241] [Batch: 0301/0570] Loss: 0.20092  Avg Loss: 0.14507  Avg mIoU:  76.22  
[Epoch: 241] [Batch: 0351/0570] Loss: 0.12310  Avg Loss: 0.14525  Avg mIoU:  76.16  
[Epoch: 241] [Batch: 0401/0570] Loss: 0.14381  Avg Loss: 0.14678  Avg mIoU:  76.04  
[Epoch: 241] [Batch: 0451/0570] Loss: 0.26824  Avg Loss: 0.14657  Avg mIoU:  75.98  
[Epoch: 241] [Batch: 0501/0570] Loss: 0.19710  Avg Loss: 0.14668  Avg mIoU:  76.09  
[Epoch: 241] [Batch: 0551/0570] Loss: 0.22173  Avg Loss: 0.14720  Avg mIoU:  75.96  

*** Training [@Epoch 241] Avg Loss: 0.14670  Avg mIoU:  75.91  ***

[Epoch: 241] [Batch: 0001/0050] Loss: 0.12571  Avg Loss: 0.12571  Avg mIoU:  66.70  

*** Validation [@Epoch 241] Avg Loss: 0.17213  Avg mIoU:  65.36  ***

[Epoch: 242] [Batch: 0001/0570] Loss: 0.12008  Avg Loss: 0.12008  Avg mIoU:  50.34  
[Epoch: 242] [Batch: 0051/0570] Loss: 0.17476  Avg Loss: 0.15194  Avg mIoU:  74.16  
[Epoch: 242] [Batch: 0101/0570] Loss: 0.13211  Avg Loss: 0.14868  Avg mIoU:  75.32  
[Epoch: 242] [Batch: 0151/0570] Loss: 0.12221  Avg Loss: 0.14713  Avg mIoU:  75.60  
[Epoch: 242] [Batch: 0201/0570] Loss: 0.17277  Avg Loss: 0.14809  Avg mIoU:  75.73  
[Epoch: 242] [Batch: 0251/0570] Loss: 0.18615  Avg Loss: 0.14895  Avg mIoU:  75.91  
[Epoch: 242] [Batch: 0301/0570] Loss: 0.16315  Avg Loss: 0.14775  Avg mIoU:  75.96  
[Epoch: 242] [Batch: 0351/0570] Loss: 0.17043  Avg Loss: 0.14865  Avg mIoU:  75.77  
[Epoch: 242] [Batch: 0401/0570] Loss: 0.15592  Avg Loss: 0.14803  Avg mIoU:  75.75  
[Epoch: 242] [Batch: 0451/0570] Loss: 0.22448  Avg Loss: 0.14829  Avg mIoU:  75.67  
[Epoch: 242] [Batch: 0501/0570] Loss: 0.17675  Avg Loss: 0.14839  Avg mIoU:  75.73  
[Epoch: 242] [Batch: 0551/0570] Loss: 0.12770  Avg Loss: 0.14780  Avg mIoU:  75.73  

*** Training [@Epoch 242] Avg Loss: 0.14803  Avg mIoU:  75.76  ***

[Epoch: 242] [Batch: 0001/0050] Loss: 0.18608  Avg Loss: 0.18608  Avg mIoU:  63.08  

*** Validation [@Epoch 242] Avg Loss: 0.20026  Avg mIoU:  64.38  ***

[Epoch: 243] [Batch: 0001/0570] Loss: 0.08972  Avg Loss: 0.08972  Avg mIoU:  46.98  
[Epoch: 243] [Batch: 0051/0570] Loss: 0.15500  Avg Loss: 0.14915  Avg mIoU:  75.74  
[Epoch: 243] [Batch: 0101/0570] Loss: 0.13360  Avg Loss: 0.14349  Avg mIoU:  76.08  
[Epoch: 243] [Batch: 0151/0570] Loss: 0.15549  Avg Loss: 0.14442  Avg mIoU:  75.95  
[Epoch: 243] [Batch: 0201/0570] Loss: 0.15731  Avg Loss: 0.14655  Avg mIoU:  75.62  
[Epoch: 243] [Batch: 0251/0570] Loss: 0.11530  Avg Loss: 0.14571  Avg mIoU:  75.57  
[Epoch: 243] [Batch: 0301/0570] Loss: 0.13150  Avg Loss: 0.14570  Avg mIoU:  75.84  
[Epoch: 243] [Batch: 0351/0570] Loss: 0.12768  Avg Loss: 0.14629  Avg mIoU:  75.62  
[Epoch: 243] [Batch: 0401/0570] Loss: 0.13582  Avg Loss: 0.14665  Avg mIoU:  75.37  
[Epoch: 243] [Batch: 0451/0570] Loss: 0.22656  Avg Loss: 0.14746  Avg mIoU:  75.46  
[Epoch: 243] [Batch: 0501/0570] Loss: 0.12703  Avg Loss: 0.14819  Avg mIoU:  75.53  
[Epoch: 243] [Batch: 0551/0570] Loss: 0.19856  Avg Loss: 0.14742  Avg mIoU:  75.77  

*** Training [@Epoch 243] Avg Loss: 0.14735  Avg mIoU:  75.79  ***

[Epoch: 243] [Batch: 0001/0050] Loss: 0.16717  Avg Loss: 0.16717  Avg mIoU:  64.63  

*** Validation [@Epoch 243] Avg Loss: 0.19281  Avg mIoU:  64.82  ***

[Epoch: 244] [Batch: 0001/0570] Loss: 0.13881  Avg Loss: 0.13881  Avg mIoU:  42.70  
[Epoch: 244] [Batch: 0051/0570] Loss: 0.09222  Avg Loss: 0.13768  Avg mIoU:  77.24  
[Epoch: 244] [Batch: 0101/0570] Loss: 0.08985  Avg Loss: 0.14007  Avg mIoU:  76.56  
[Epoch: 244] [Batch: 0151/0570] Loss: 0.12083  Avg Loss: 0.14562  Avg mIoU:  75.69  
[Epoch: 244] [Batch: 0201/0570] Loss: 0.18336  Avg Loss: 0.14507  Avg mIoU:  75.58  
[Epoch: 244] [Batch: 0251/0570] Loss: 0.13808  Avg Loss: 0.14613  Avg mIoU:  75.45  
[Epoch: 244] [Batch: 0301/0570] Loss: 0.18975  Avg Loss: 0.14540  Avg mIoU:  75.87  
[Epoch: 244] [Batch: 0351/0570] Loss: 0.16890  Avg Loss: 0.14594  Avg mIoU:  75.80  
[Epoch: 244] [Batch: 0401/0570] Loss: 0.13060  Avg Loss: 0.14679  Avg mIoU:  75.77  
[Epoch: 244] [Batch: 0451/0570] Loss: 0.09149  Avg Loss: 0.14662  Avg mIoU:  75.73  
[Epoch: 244] [Batch: 0501/0570] Loss: 0.20209  Avg Loss: 0.14681  Avg mIoU:  75.76  
[Epoch: 244] [Batch: 0551/0570] Loss: 0.10670  Avg Loss: 0.14748  Avg mIoU:  75.73  

*** Training [@Epoch 244] Avg Loss: 0.14740  Avg mIoU:  75.71  ***

[Epoch: 244] [Batch: 0001/0050] Loss: 0.15514  Avg Loss: 0.15514  Avg mIoU:  64.19  

*** Validation [@Epoch 244] Avg Loss: 0.18958  Avg mIoU:  65.38  ***

[Epoch: 245] [Batch: 0001/0570] Loss: 0.10153  Avg Loss: 0.10153  Avg mIoU:  48.36  
[Epoch: 245] [Batch: 0051/0570] Loss: 0.16924  Avg Loss: 0.14026  Avg mIoU:  75.65  
[Epoch: 245] [Batch: 0101/0570] Loss: 0.11525  Avg Loss: 0.14388  Avg mIoU:  75.81  
[Epoch: 245] [Batch: 0151/0570] Loss: 0.16285  Avg Loss: 0.14664  Avg mIoU:  75.84  
[Epoch: 245] [Batch: 0201/0570] Loss: 0.11774  Avg Loss: 0.14791  Avg mIoU:  76.04  
[Epoch: 245] [Batch: 0251/0570] Loss: 0.16364  Avg Loss: 0.14837  Avg mIoU:  76.15  
[Epoch: 245] [Batch: 0301/0570] Loss: 0.17933  Avg Loss: 0.14755  Avg mIoU:  75.98  
[Epoch: 245] [Batch: 0351/0570] Loss: 0.19763  Avg Loss: 0.14799  Avg mIoU:  76.02  
[Epoch: 245] [Batch: 0401/0570] Loss: 0.14033  Avg Loss: 0.14871  Avg mIoU:  75.99  
[Epoch: 245] [Batch: 0451/0570] Loss: 0.12924  Avg Loss: 0.14801  Avg mIoU:  76.00  
[Epoch: 245] [Batch: 0501/0570] Loss: 0.10258  Avg Loss: 0.14763  Avg mIoU:  76.10  
[Epoch: 245] [Batch: 0551/0570] Loss: 0.13353  Avg Loss: 0.14693  Avg mIoU:  76.12  

*** Training [@Epoch 245] Avg Loss: 0.14691  Avg mIoU:  76.08  ***

[Epoch: 245] [Batch: 0001/0050] Loss: 0.11609  Avg Loss: 0.11609  Avg mIoU:  67.13  

*** Validation [@Epoch 245] Avg Loss: 0.16297  Avg mIoU:  64.46  ***

[Epoch: 246] [Batch: 0001/0570] Loss: 0.10560  Avg Loss: 0.10560  Avg mIoU:  46.54  
[Epoch: 246] [Batch: 0051/0570] Loss: 0.15828  Avg Loss: 0.14203  Avg mIoU:  76.07  
[Epoch: 246] [Batch: 0101/0570] Loss: 0.11813  Avg Loss: 0.14741  Avg mIoU:  75.62  
[Epoch: 246] [Batch: 0151/0570] Loss: 0.14361  Avg Loss: 0.14665  Avg mIoU:  75.73  
[Epoch: 246] [Batch: 0201/0570] Loss: 0.09918  Avg Loss: 0.14468  Avg mIoU:  76.08  
[Epoch: 246] [Batch: 0251/0570] Loss: 0.11095  Avg Loss: 0.14448  Avg mIoU:  76.07  
[Epoch: 246] [Batch: 0301/0570] Loss: 0.12568  Avg Loss: 0.14522  Avg mIoU:  76.21  
[Epoch: 246] [Batch: 0351/0570] Loss: 0.12885  Avg Loss: 0.14531  Avg mIoU:  76.00  
[Epoch: 246] [Batch: 0401/0570] Loss: 0.15034  Avg Loss: 0.14663  Avg mIoU:  75.80  
[Epoch: 246] [Batch: 0451/0570] Loss: 0.10560  Avg Loss: 0.14599  Avg mIoU:  75.99  
[Epoch: 246] [Batch: 0501/0570] Loss: 0.14040  Avg Loss: 0.14684  Avg mIoU:  75.81  
[Epoch: 246] [Batch: 0551/0570] Loss: 0.15921  Avg Loss: 0.14697  Avg mIoU:  75.81  

*** Training [@Epoch 246] Avg Loss: 0.14703  Avg mIoU:  75.85  ***

[Epoch: 246] [Batch: 0001/0050] Loss: 0.13340  Avg Loss: 0.13340  Avg mIoU:  67.13  

*** Validation [@Epoch 246] Avg Loss: 0.17211  Avg mIoU:  65.85  ***

[Epoch: 247] [Batch: 0001/0570] Loss: 0.13008  Avg Loss: 0.13008  Avg mIoU:  49.31  
[Epoch: 247] [Batch: 0051/0570] Loss: 0.09944  Avg Loss: 0.14326  Avg mIoU:  74.55  
[Epoch: 247] [Batch: 0101/0570] Loss: 0.16592  Avg Loss: 0.14761  Avg mIoU:  75.36  
[Epoch: 247] [Batch: 0151/0570] Loss: 0.14067  Avg Loss: 0.14685  Avg mIoU:  75.64  
[Epoch: 247] [Batch: 0201/0570] Loss: 0.12061  Avg Loss: 0.14735  Avg mIoU:  76.03  
[Epoch: 247] [Batch: 0251/0570] Loss: 0.15693  Avg Loss: 0.14677  Avg mIoU:  76.08  
[Epoch: 247] [Batch: 0301/0570] Loss: 0.11125  Avg Loss: 0.14587  Avg mIoU:  76.20  
[Epoch: 247] [Batch: 0351/0570] Loss: 0.20115  Avg Loss: 0.14685  Avg mIoU:  76.02  
[Epoch: 247] [Batch: 0401/0570] Loss: 0.15649  Avg Loss: 0.14710  Avg mIoU:  75.97  
[Epoch: 247] [Batch: 0451/0570] Loss: 0.16058  Avg Loss: 0.14750  Avg mIoU:  76.04  
[Epoch: 247] [Batch: 0501/0570] Loss: 0.10751  Avg Loss: 0.14740  Avg mIoU:  76.03  
[Epoch: 247] [Batch: 0551/0570] Loss: 0.12755  Avg Loss: 0.14638  Avg mIoU:  76.02  

*** Training [@Epoch 247] Avg Loss: 0.14642  Avg mIoU:  75.94  ***

[Epoch: 247] [Batch: 0001/0050] Loss: 0.16257  Avg Loss: 0.16257  Avg mIoU:  63.51  

*** Validation [@Epoch 247] Avg Loss: 0.16765  Avg mIoU:  66.04  ***

[Epoch: 248] [Batch: 0001/0570] Loss: 0.16190  Avg Loss: 0.16190  Avg mIoU:  52.14  
[Epoch: 248] [Batch: 0051/0570] Loss: 0.12397  Avg Loss: 0.14174  Avg mIoU:  76.23  
[Epoch: 248] [Batch: 0101/0570] Loss: 0.14601  Avg Loss: 0.14029  Avg mIoU:  76.35  
[Epoch: 248] [Batch: 0151/0570] Loss: 0.15796  Avg Loss: 0.14023  Avg mIoU:  76.22  
[Epoch: 248] [Batch: 0201/0570] Loss: 0.12720  Avg Loss: 0.14339  Avg mIoU:  76.18  
[Epoch: 248] [Batch: 0251/0570] Loss: 0.20454  Avg Loss: 0.14383  Avg mIoU:  76.40  
[Epoch: 248] [Batch: 0301/0570] Loss: 0.13126  Avg Loss: 0.14532  Avg mIoU:  76.12  
[Epoch: 248] [Batch: 0351/0570] Loss: 0.16509  Avg Loss: 0.14616  Avg mIoU:  75.50  
[Epoch: 248] [Batch: 0401/0570] Loss: 0.25417  Avg Loss: 0.14759  Avg mIoU:  75.38  
[Epoch: 248] [Batch: 0451/0570] Loss: 0.19821  Avg Loss: 0.14770  Avg mIoU:  75.66  
[Epoch: 248] [Batch: 0501/0570] Loss: 0.12599  Avg Loss: 0.14699  Avg mIoU:  75.77  
[Epoch: 248] [Batch: 0551/0570] Loss: 0.16805  Avg Loss: 0.14650  Avg mIoU:  75.95  

*** Training [@Epoch 248] Avg Loss: 0.14643  Avg mIoU:  75.98  ***

[Epoch: 248] [Batch: 0001/0050] Loss: 0.13054  Avg Loss: 0.13054  Avg mIoU:  69.84  

*** Validation [@Epoch 248] Avg Loss: 0.18900  Avg mIoU:  65.87  ***

[Epoch: 249] [Batch: 0001/0570] Loss: 0.15895  Avg Loss: 0.15895  Avg mIoU:  52.10  
[Epoch: 249] [Batch: 0051/0570] Loss: 0.11188  Avg Loss: 0.13742  Avg mIoU:  75.28  
[Epoch: 249] [Batch: 0101/0570] Loss: 0.16633  Avg Loss: 0.14474  Avg mIoU:  75.82  
[Epoch: 249] [Batch: 0151/0570] Loss: 0.11635  Avg Loss: 0.14397  Avg mIoU:  75.90  
[Epoch: 249] [Batch: 0201/0570] Loss: 0.22218  Avg Loss: 0.14616  Avg mIoU:  75.72  
[Epoch: 249] [Batch: 0251/0570] Loss: 0.12483  Avg Loss: 0.14802  Avg mIoU:  75.75  
[Epoch: 249] [Batch: 0301/0570] Loss: 0.17306  Avg Loss: 0.14899  Avg mIoU:  75.79  
[Epoch: 249] [Batch: 0351/0570] Loss: 0.10747  Avg Loss: 0.14872  Avg mIoU:  75.94  
[Epoch: 249] [Batch: 0401/0570] Loss: 0.08478  Avg Loss: 0.14792  Avg mIoU:  75.94  
[Epoch: 249] [Batch: 0451/0570] Loss: 0.17111  Avg Loss: 0.14820  Avg mIoU:  75.85  
[Epoch: 249] [Batch: 0501/0570] Loss: 0.14153  Avg Loss: 0.14763  Avg mIoU:  75.88  
[Epoch: 249] [Batch: 0551/0570] Loss: 0.12985  Avg Loss: 0.14705  Avg mIoU:  75.70  

*** Training [@Epoch 249] Avg Loss: 0.14701  Avg mIoU:  75.66  ***

[Epoch: 249] [Batch: 0001/0050] Loss: 0.13922  Avg Loss: 0.13922  Avg mIoU:  66.08  

*** Validation [@Epoch 249] Avg Loss: 0.19141  Avg mIoU:  64.87  ***

[Epoch: 250] [Batch: 0001/0570] Loss: 0.17188  Avg Loss: 0.17188  Avg mIoU:  22.33  
[Epoch: 250] [Batch: 0051/0570] Loss: 0.13333  Avg Loss: 0.15043  Avg mIoU:  75.75  
[Epoch: 250] [Batch: 0101/0570] Loss: 0.14170  Avg Loss: 0.15043  Avg mIoU:  75.83  
[Epoch: 250] [Batch: 0151/0570] Loss: 0.16673  Avg Loss: 0.14986  Avg mIoU:  75.73  
[Epoch: 250] [Batch: 0201/0570] Loss: 0.11409  Avg Loss: 0.14752  Avg mIoU:  75.99  
[Epoch: 250] [Batch: 0251/0570] Loss: 0.25118  Avg Loss: 0.14743  Avg mIoU:  75.83  
[Epoch: 250] [Batch: 0301/0570] Loss: 0.16072  Avg Loss: 0.14621  Avg mIoU:  76.02  
[Epoch: 250] [Batch: 0351/0570] Loss: 0.12643  Avg Loss: 0.14493  Avg mIoU:  76.00  
[Epoch: 250] [Batch: 0401/0570] Loss: 0.14648  Avg Loss: 0.14442  Avg mIoU:  75.89  
[Epoch: 250] [Batch: 0451/0570] Loss: 0.12774  Avg Loss: 0.14575  Avg mIoU:  75.88  
[Epoch: 250] [Batch: 0501/0570] Loss: 0.12041  Avg Loss: 0.14586  Avg mIoU:  75.92  
[Epoch: 250] [Batch: 0551/0570] Loss: 0.13641  Avg Loss: 0.14590  Avg mIoU:  76.01  

*** Training [@Epoch 250] Avg Loss: 0.14614  Avg mIoU:  76.12  ***

[Epoch: 250] [Batch: 0001/0050] Loss: 0.13408  Avg Loss: 0.13408  Avg mIoU:  66.12  

*** Validation [@Epoch 250] Avg Loss: 0.16829  Avg mIoU:  63.32  ***

[Epoch: 251] [Batch: 0001/0570] Loss: 0.13752  Avg Loss: 0.13752  Avg mIoU:  36.74  
[Epoch: 251] [Batch: 0051/0570] Loss: 0.11182  Avg Loss: 0.13890  Avg mIoU:  76.31  
[Epoch: 251] [Batch: 0101/0570] Loss: 0.14485  Avg Loss: 0.14512  Avg mIoU:  75.91  
[Epoch: 251] [Batch: 0151/0570] Loss: 0.18171  Avg Loss: 0.14501  Avg mIoU:  75.87  
[Epoch: 251] [Batch: 0201/0570] Loss: 0.12375  Avg Loss: 0.14459  Avg mIoU:  76.10  
[Epoch: 251] [Batch: 0251/0570] Loss: 0.11553  Avg Loss: 0.14506  Avg mIoU:  75.81  
[Epoch: 251] [Batch: 0301/0570] Loss: 0.12146  Avg Loss: 0.14442  Avg mIoU:  75.87  
[Epoch: 251] [Batch: 0351/0570] Loss: 0.17508  Avg Loss: 0.14598  Avg mIoU:  75.90  
[Epoch: 251] [Batch: 0401/0570] Loss: 0.11963  Avg Loss: 0.14592  Avg mIoU:  75.93  
[Epoch: 251] [Batch: 0451/0570] Loss: 0.12389  Avg Loss: 0.14652  Avg mIoU:  75.94  
[Epoch: 251] [Batch: 0501/0570] Loss: 0.11532  Avg Loss: 0.14655  Avg mIoU:  75.84  
[Epoch: 251] [Batch: 0551/0570] Loss: 0.15884  Avg Loss: 0.14686  Avg mIoU:  75.85  

*** Training [@Epoch 251] Avg Loss: 0.14688  Avg mIoU:  75.82  ***

[Epoch: 251] [Batch: 0001/0050] Loss: 0.12770  Avg Loss: 0.12770  Avg mIoU:  66.74  

*** Validation [@Epoch 251] Avg Loss: 0.16762  Avg mIoU:  63.20  ***

[Epoch: 252] [Batch: 0001/0570] Loss: 0.08035  Avg Loss: 0.08035  Avg mIoU:  58.12  
[Epoch: 252] [Batch: 0051/0570] Loss: 0.11019  Avg Loss: 0.13826  Avg mIoU:  75.22  
[Epoch: 252] [Batch: 0101/0570] Loss: 0.11883  Avg Loss: 0.14027  Avg mIoU:  75.25  
[Epoch: 252] [Batch: 0151/0570] Loss: 0.10381  Avg Loss: 0.14267  Avg mIoU:  76.17  
[Epoch: 252] [Batch: 0201/0570] Loss: 0.16645  Avg Loss: 0.14327  Avg mIoU:  75.95  
[Epoch: 252] [Batch: 0251/0570] Loss: 0.11130  Avg Loss: 0.14349  Avg mIoU:  76.22  
[Epoch: 252] [Batch: 0301/0570] Loss: 0.16768  Avg Loss: 0.14419  Avg mIoU:  76.26  
[Epoch: 252] [Batch: 0351/0570] Loss: 0.12302  Avg Loss: 0.14378  Avg mIoU:  76.47  
[Epoch: 252] [Batch: 0401/0570] Loss: 0.11584  Avg Loss: 0.14438  Avg mIoU:  76.34  
[Epoch: 252] [Batch: 0451/0570] Loss: 0.11698  Avg Loss: 0.14398  Avg mIoU:  76.36  
[Epoch: 252] [Batch: 0501/0570] Loss: 0.11161  Avg Loss: 0.14387  Avg mIoU:  76.45  
[Epoch: 252] [Batch: 0551/0570] Loss: 0.11859  Avg Loss: 0.14345  Avg mIoU:  76.38  

*** Training [@Epoch 252] Avg Loss: 0.14348  Avg mIoU:  76.39  ***

[Epoch: 252] [Batch: 0001/0050] Loss: 0.12945  Avg Loss: 0.12945  Avg mIoU:  65.60  

*** Validation [@Epoch 252] Avg Loss: 0.16611  Avg mIoU:  66.22  ***

[Epoch: 253] [Batch: 0001/0570] Loss: 0.16713  Avg Loss: 0.16713  Avg mIoU:  40.90  
[Epoch: 253] [Batch: 0051/0570] Loss: 0.14948  Avg Loss: 0.15086  Avg mIoU:  76.82  
[Epoch: 253] [Batch: 0101/0570] Loss: 0.14612  Avg Loss: 0.14853  Avg mIoU:  76.67  
[Epoch: 253] [Batch: 0151/0570] Loss: 0.15490  Avg Loss: 0.14730  Avg mIoU:  76.81  
[Epoch: 253] [Batch: 0201/0570] Loss: 0.18390  Avg Loss: 0.14832  Avg mIoU:  76.30  
[Epoch: 253] [Batch: 0251/0570] Loss: 0.15976  Avg Loss: 0.14706  Avg mIoU:  76.17  
[Epoch: 253] [Batch: 0301/0570] Loss: 0.17243  Avg Loss: 0.14921  Avg mIoU:  75.78  
[Epoch: 253] [Batch: 0351/0570] Loss: 0.15356  Avg Loss: 0.14963  Avg mIoU:  75.75  
[Epoch: 253] [Batch: 0401/0570] Loss: 0.15554  Avg Loss: 0.14869  Avg mIoU:  75.87  
[Epoch: 253] [Batch: 0451/0570] Loss: 0.17516  Avg Loss: 0.14882  Avg mIoU:  75.50  
[Epoch: 253] [Batch: 0501/0570] Loss: 0.11678  Avg Loss: 0.14851  Avg mIoU:  75.46  
[Epoch: 253] [Batch: 0551/0570] Loss: 0.14870  Avg Loss: 0.14746  Avg mIoU:  75.60  

*** Training [@Epoch 253] Avg Loss: 0.14794  Avg mIoU:  75.49  ***

[Epoch: 253] [Batch: 0001/0050] Loss: 0.13005  Avg Loss: 0.13005  Avg mIoU:  66.04  

*** Validation [@Epoch 253] Avg Loss: 0.15744  Avg mIoU:  65.33  ***

[Epoch: 254] [Batch: 0001/0570] Loss: 0.16624  Avg Loss: 0.16624  Avg mIoU:  51.10  
[Epoch: 254] [Batch: 0051/0570] Loss: 0.11319  Avg Loss: 0.14002  Avg mIoU:  76.13  
[Epoch: 254] [Batch: 0101/0570] Loss: 0.11221  Avg Loss: 0.14219  Avg mIoU:  76.52  
[Epoch: 254] [Batch: 0151/0570] Loss: 0.14551  Avg Loss: 0.14346  Avg mIoU:  76.66  
[Epoch: 254] [Batch: 0201/0570] Loss: 0.19741  Avg Loss: 0.14225  Avg mIoU:  76.70  
[Epoch: 254] [Batch: 0251/0570] Loss: 0.12086  Avg Loss: 0.14257  Avg mIoU:  76.78  
[Epoch: 254] [Batch: 0301/0570] Loss: 0.17415  Avg Loss: 0.14268  Avg mIoU:  76.66  
[Epoch: 254] [Batch: 0351/0570] Loss: 0.13892  Avg Loss: 0.14420  Avg mIoU:  76.34  
[Epoch: 254] [Batch: 0401/0570] Loss: 0.09068  Avg Loss: 0.14430  Avg mIoU:  76.22  
[Epoch: 254] [Batch: 0451/0570] Loss: 0.13989  Avg Loss: 0.14496  Avg mIoU:  76.21  
[Epoch: 254] [Batch: 0501/0570] Loss: 0.18167  Avg Loss: 0.14493  Avg mIoU:  76.23  
[Epoch: 254] [Batch: 0551/0570] Loss: 0.15417  Avg Loss: 0.14594  Avg mIoU:  76.12  

*** Training [@Epoch 254] Avg Loss: 0.14587  Avg mIoU:  76.09  ***

[Epoch: 254] [Batch: 0001/0050] Loss: 0.12404  Avg Loss: 0.12404  Avg mIoU:  63.57  

*** Validation [@Epoch 254] Avg Loss: 0.14855  Avg mIoU:  63.93  ***

[Epoch: 255] [Batch: 0001/0570] Loss: 0.14586  Avg Loss: 0.14586  Avg mIoU:  47.59  
[Epoch: 255] [Batch: 0051/0570] Loss: 0.16462  Avg Loss: 0.14572  Avg mIoU:  76.86  
[Epoch: 255] [Batch: 0101/0570] Loss: 0.19189  Avg Loss: 0.14365  Avg mIoU:  77.46  
[Epoch: 255] [Batch: 0151/0570] Loss: 0.10902  Avg Loss: 0.14445  Avg mIoU:  76.86  
[Epoch: 255] [Batch: 0201/0570] Loss: 0.15555  Avg Loss: 0.14439  Avg mIoU:  76.79  
[Epoch: 255] [Batch: 0251/0570] Loss: 0.17923  Avg Loss: 0.14471  Avg mIoU:  76.67  
[Epoch: 255] [Batch: 0301/0570] Loss: 0.21294  Avg Loss: 0.14380  Avg mIoU:  76.60  
[Epoch: 255] [Batch: 0351/0570] Loss: 0.13554  Avg Loss: 0.14331  Avg mIoU:  76.50  
[Epoch: 255] [Batch: 0401/0570] Loss: 0.11116  Avg Loss: 0.14370  Avg mIoU:  76.41  
[Epoch: 255] [Batch: 0451/0570] Loss: 0.13914  Avg Loss: 0.14390  Avg mIoU:  76.23  
[Epoch: 255] [Batch: 0501/0570] Loss: 0.15649  Avg Loss: 0.14414  Avg mIoU:  76.15  
[Epoch: 255] [Batch: 0551/0570] Loss: 0.14766  Avg Loss: 0.14449  Avg mIoU:  76.17  

*** Training [@Epoch 255] Avg Loss: 0.14462  Avg mIoU:  76.15  ***

[Epoch: 255] [Batch: 0001/0050] Loss: 0.12149  Avg Loss: 0.12149  Avg mIoU:  65.76  

*** Validation [@Epoch 255] Avg Loss: 0.18180  Avg mIoU:  63.45  ***

[Epoch: 256] [Batch: 0001/0570] Loss: 0.17153  Avg Loss: 0.17153  Avg mIoU:  45.95  
[Epoch: 256] [Batch: 0051/0570] Loss: 0.13952  Avg Loss: 0.15989  Avg mIoU:  75.98  
[Epoch: 256] [Batch: 0101/0570] Loss: 0.19730  Avg Loss: 0.15570  Avg mIoU:  75.52  
[Epoch: 256] [Batch: 0151/0570] Loss: 0.15116  Avg Loss: 0.15373  Avg mIoU:  75.77  
[Epoch: 256] [Batch: 0201/0570] Loss: 0.15037  Avg Loss: 0.15272  Avg mIoU:  75.62  
[Epoch: 256] [Batch: 0251/0570] Loss: 0.11865  Avg Loss: 0.15104  Avg mIoU:  75.59  
[Epoch: 256] [Batch: 0301/0570] Loss: 0.19629  Avg Loss: 0.15148  Avg mIoU:  75.70  
[Epoch: 256] [Batch: 0351/0570] Loss: 0.15880  Avg Loss: 0.15024  Avg mIoU:  75.70  
[Epoch: 256] [Batch: 0401/0570] Loss: 0.10090  Avg Loss: 0.14917  Avg mIoU:  75.58  
[Epoch: 256] [Batch: 0451/0570] Loss: 0.15196  Avg Loss: 0.14871  Avg mIoU:  75.71  
[Epoch: 256] [Batch: 0501/0570] Loss: 0.09811  Avg Loss: 0.14793  Avg mIoU:  75.81  
[Epoch: 256] [Batch: 0551/0570] Loss: 0.15464  Avg Loss: 0.14747  Avg mIoU:  75.97  

*** Training [@Epoch 256] Avg Loss: 0.14758  Avg mIoU:  75.92  ***

[Epoch: 256] [Batch: 0001/0050] Loss: 0.12721  Avg Loss: 0.12721  Avg mIoU:  67.87  

*** Validation [@Epoch 256] Avg Loss: 0.16402  Avg mIoU:  66.43  ***

[Epoch: 257] [Batch: 0001/0570] Loss: 0.17064  Avg Loss: 0.17064  Avg mIoU:  38.97  
[Epoch: 257] [Batch: 0051/0570] Loss: 0.14283  Avg Loss: 0.14951  Avg mIoU:  75.50  
[Epoch: 257] [Batch: 0101/0570] Loss: 0.17675  Avg Loss: 0.14467  Avg mIoU:  76.21  
[Epoch: 257] [Batch: 0151/0570] Loss: 0.15288  Avg Loss: 0.14520  Avg mIoU:  76.10  
[Epoch: 257] [Batch: 0201/0570] Loss: 0.11322  Avg Loss: 0.14535  Avg mIoU:  76.29  
[Epoch: 257] [Batch: 0251/0570] Loss: 0.12657  Avg Loss: 0.14510  Avg mIoU:  76.30  
[Epoch: 257] [Batch: 0301/0570] Loss: 0.17011  Avg Loss: 0.14371  Avg mIoU:  76.42  
[Epoch: 257] [Batch: 0351/0570] Loss: 0.11440  Avg Loss: 0.14359  Avg mIoU:  76.35  
[Epoch: 257] [Batch: 0401/0570] Loss: 0.16920  Avg Loss: 0.14308  Avg mIoU:  76.28  
[Epoch: 257] [Batch: 0451/0570] Loss: 0.18404  Avg Loss: 0.14300  Avg mIoU:  76.44  
[Epoch: 257] [Batch: 0501/0570] Loss: 0.12696  Avg Loss: 0.14288  Avg mIoU:  76.45  
[Epoch: 257] [Batch: 0551/0570] Loss: 0.12540  Avg Loss: 0.14407  Avg mIoU:  76.36  

*** Training [@Epoch 257] Avg Loss: 0.14389  Avg mIoU:  76.35  ***

[Epoch: 257] [Batch: 0001/0050] Loss: 0.11880  Avg Loss: 0.11880  Avg mIoU:  66.77  

*** Validation [@Epoch 257] Avg Loss: 0.17056  Avg mIoU:  64.40  ***

[Epoch: 258] [Batch: 0001/0570] Loss: 0.17106  Avg Loss: 0.17106  Avg mIoU:  54.22  
[Epoch: 258] [Batch: 0051/0570] Loss: 0.10226  Avg Loss: 0.14743  Avg mIoU:  74.42  
[Epoch: 258] [Batch: 0101/0570] Loss: 0.14479  Avg Loss: 0.14569  Avg mIoU:  76.40  
[Epoch: 258] [Batch: 0151/0570] Loss: 0.19938  Avg Loss: 0.14740  Avg mIoU:  76.08  
[Epoch: 258] [Batch: 0201/0570] Loss: 0.16259  Avg Loss: 0.14689  Avg mIoU:  76.26  
[Epoch: 258] [Batch: 0251/0570] Loss: 0.17796  Avg Loss: 0.14709  Avg mIoU:  76.16  
[Epoch: 258] [Batch: 0301/0570] Loss: 0.12818  Avg Loss: 0.14674  Avg mIoU:  76.08  
[Epoch: 258] [Batch: 0351/0570] Loss: 0.15013  Avg Loss: 0.14606  Avg mIoU:  75.83  
[Epoch: 258] [Batch: 0401/0570] Loss: 0.11812  Avg Loss: 0.14586  Avg mIoU:  75.89  
[Epoch: 258] [Batch: 0451/0570] Loss: 0.10818  Avg Loss: 0.14561  Avg mIoU:  75.94  
[Epoch: 258] [Batch: 0501/0570] Loss: 0.15654  Avg Loss: 0.14570  Avg mIoU:  75.92  
[Epoch: 258] [Batch: 0551/0570] Loss: 0.16088  Avg Loss: 0.14566  Avg mIoU:  75.95  

*** Training [@Epoch 258] Avg Loss: 0.14542  Avg mIoU:  75.97  ***

[Epoch: 258] [Batch: 0001/0050] Loss: 0.12916  Avg Loss: 0.12916  Avg mIoU:  68.79  

*** Validation [@Epoch 258] Avg Loss: 0.18505  Avg mIoU:  65.50  ***

[Epoch: 259] [Batch: 0001/0570] Loss: 0.14370  Avg Loss: 0.14370  Avg mIoU:  31.81  
[Epoch: 259] [Batch: 0051/0570] Loss: 0.15602  Avg Loss: 0.14634  Avg mIoU:  76.80  
[Epoch: 259] [Batch: 0101/0570] Loss: 0.16460  Avg Loss: 0.14719  Avg mIoU:  76.42  
[Epoch: 259] [Batch: 0151/0570] Loss: 0.12517  Avg Loss: 0.14595  Avg mIoU:  77.37  
[Epoch: 259] [Batch: 0201/0570] Loss: 0.16688  Avg Loss: 0.14569  Avg mIoU:  76.96  
[Epoch: 259] [Batch: 0251/0570] Loss: 0.12564  Avg Loss: 0.14446  Avg mIoU:  76.95  
[Epoch: 259] [Batch: 0301/0570] Loss: 0.11676  Avg Loss: 0.14268  Avg mIoU:  76.97  
[Epoch: 259] [Batch: 0351/0570] Loss: 0.10069  Avg Loss: 0.14295  Avg mIoU:  76.96  
[Epoch: 259] [Batch: 0401/0570] Loss: 0.17490  Avg Loss: 0.14343  Avg mIoU:  76.79  
[Epoch: 259] [Batch: 0451/0570] Loss: 0.11773  Avg Loss: 0.14426  Avg mIoU:  76.64  
[Epoch: 259] [Batch: 0501/0570] Loss: 0.20068  Avg Loss: 0.14505  Avg mIoU:  76.51  
[Epoch: 259] [Batch: 0551/0570] Loss: 0.18389  Avg Loss: 0.14562  Avg mIoU:  76.28  

*** Training [@Epoch 259] Avg Loss: 0.14541  Avg mIoU:  76.21  ***

[Epoch: 259] [Batch: 0001/0050] Loss: 0.12350  Avg Loss: 0.12350  Avg mIoU:  68.32  

*** Validation [@Epoch 259] Avg Loss: 0.16876  Avg mIoU:  66.60  ***

[Epoch: 260] [Batch: 0001/0570] Loss: 0.09474  Avg Loss: 0.09474  Avg mIoU:  60.95  
[Epoch: 260] [Batch: 0051/0570] Loss: 0.15385  Avg Loss: 0.13735  Avg mIoU:  76.99  
[Epoch: 260] [Batch: 0101/0570] Loss: 0.16713  Avg Loss: 0.14054  Avg mIoU:  76.88  
[Epoch: 260] [Batch: 0151/0570] Loss: 0.15338  Avg Loss: 0.14220  Avg mIoU:  76.76  
[Epoch: 260] [Batch: 0201/0570] Loss: 0.13455  Avg Loss: 0.14137  Avg mIoU:  77.00  
[Epoch: 260] [Batch: 0251/0570] Loss: 0.14687  Avg Loss: 0.14193  Avg mIoU:  76.99  
[Epoch: 260] [Batch: 0301/0570] Loss: 0.18522  Avg Loss: 0.14335  Avg mIoU:  76.92  
[Epoch: 260] [Batch: 0351/0570] Loss: 0.15852  Avg Loss: 0.14264  Avg mIoU:  76.87  
[Epoch: 260] [Batch: 0401/0570] Loss: 0.15058  Avg Loss: 0.14204  Avg mIoU:  76.82  
[Epoch: 260] [Batch: 0451/0570] Loss: 0.11554  Avg Loss: 0.14222  Avg mIoU:  76.63  
[Epoch: 260] [Batch: 0501/0570] Loss: 0.14470  Avg Loss: 0.14270  Avg mIoU:  76.64  
[Epoch: 260] [Batch: 0551/0570] Loss: 0.12454  Avg Loss: 0.14355  Avg mIoU:  76.47  

*** Training [@Epoch 260] Avg Loss: 0.14361  Avg mIoU:  76.49  ***

[Epoch: 260] [Batch: 0001/0050] Loss: 0.12988  Avg Loss: 0.12988  Avg mIoU:  68.46  

*** Validation [@Epoch 260] Avg Loss: 0.17751  Avg mIoU:  66.10  ***

[Epoch: 261] [Batch: 0001/0570] Loss: 0.13748  Avg Loss: 0.13748  Avg mIoU:  49.15  
[Epoch: 261] [Batch: 0051/0570] Loss: 0.08629  Avg Loss: 0.13716  Avg mIoU:  75.84  
[Epoch: 261] [Batch: 0101/0570] Loss: 0.17641  Avg Loss: 0.14031  Avg mIoU:  76.39  
[Epoch: 261] [Batch: 0151/0570] Loss: 0.11090  Avg Loss: 0.14218  Avg mIoU:  76.08  
[Epoch: 261] [Batch: 0201/0570] Loss: 0.17969  Avg Loss: 0.14483  Avg mIoU:  75.98  
[Epoch: 261] [Batch: 0251/0570] Loss: 0.13745  Avg Loss: 0.14492  Avg mIoU:  76.40  
[Epoch: 261] [Batch: 0301/0570] Loss: 0.14438  Avg Loss: 0.14409  Avg mIoU:  76.66  
[Epoch: 261] [Batch: 0351/0570] Loss: 0.18373  Avg Loss: 0.14385  Avg mIoU:  76.62  
[Epoch: 261] [Batch: 0401/0570] Loss: 0.17055  Avg Loss: 0.14420  Avg mIoU:  76.51  
[Epoch: 261] [Batch: 0451/0570] Loss: 0.12364  Avg Loss: 0.14316  Avg mIoU:  76.55  
[Epoch: 261] [Batch: 0501/0570] Loss: 0.16357  Avg Loss: 0.14343  Avg mIoU:  76.49  
[Epoch: 261] [Batch: 0551/0570] Loss: 0.19746  Avg Loss: 0.14350  Avg mIoU:  76.56  

*** Training [@Epoch 261] Avg Loss: 0.14398  Avg mIoU:  76.54  ***

[Epoch: 261] [Batch: 0001/0050] Loss: 0.13118  Avg Loss: 0.13118  Avg mIoU:  66.43  

*** Validation [@Epoch 261] Avg Loss: 0.17374  Avg mIoU:  66.00  ***

[Epoch: 262] [Batch: 0001/0570] Loss: 0.14412  Avg Loss: 0.14412  Avg mIoU:  41.20  
[Epoch: 262] [Batch: 0051/0570] Loss: 0.15284  Avg Loss: 0.14144  Avg mIoU:  76.28  
[Epoch: 262] [Batch: 0101/0570] Loss: 0.14286  Avg Loss: 0.14409  Avg mIoU:  75.48  
[Epoch: 262] [Batch: 0151/0570] Loss: 0.13066  Avg Loss: 0.14215  Avg mIoU:  76.08  
[Epoch: 262] [Batch: 0201/0570] Loss: 0.14940  Avg Loss: 0.14135  Avg mIoU:  76.31  
[Epoch: 262] [Batch: 0251/0570] Loss: 0.16084  Avg Loss: 0.14199  Avg mIoU:  76.55  
[Epoch: 262] [Batch: 0301/0570] Loss: 0.11579  Avg Loss: 0.14237  Avg mIoU:  76.63  
[Epoch: 262] [Batch: 0351/0570] Loss: 0.15396  Avg Loss: 0.14386  Avg mIoU:  76.51  
[Epoch: 262] [Batch: 0401/0570] Loss: 0.13847  Avg Loss: 0.14366  Avg mIoU:  76.62  
[Epoch: 262] [Batch: 0451/0570] Loss: 0.16904  Avg Loss: 0.14313  Avg mIoU:  76.61  
[Epoch: 262] [Batch: 0501/0570] Loss: 0.16099  Avg Loss: 0.14338  Avg mIoU:  76.48  
[Epoch: 262] [Batch: 0551/0570] Loss: 0.16984  Avg Loss: 0.14350  Avg mIoU:  76.40  

*** Training [@Epoch 262] Avg Loss: 0.14359  Avg mIoU:  76.38  ***

[Epoch: 262] [Batch: 0001/0050] Loss: 0.12786  Avg Loss: 0.12786  Avg mIoU:  67.08  

*** Validation [@Epoch 262] Avg Loss: 0.18058  Avg mIoU:  64.63  ***

[Epoch: 263] [Batch: 0001/0570] Loss: 0.09331  Avg Loss: 0.09331  Avg mIoU:  54.51  
[Epoch: 263] [Batch: 0051/0570] Loss: 0.10237  Avg Loss: 0.14820  Avg mIoU:  75.61  
[Epoch: 263] [Batch: 0101/0570] Loss: 0.20030  Avg Loss: 0.14827  Avg mIoU:  76.43  
[Epoch: 263] [Batch: 0151/0570] Loss: 0.14645  Avg Loss: 0.14613  Avg mIoU:  76.58  
[Epoch: 263] [Batch: 0201/0570] Loss: 0.12048  Avg Loss: 0.14447  Avg mIoU:  76.82  
[Epoch: 263] [Batch: 0251/0570] Loss: 0.18821  Avg Loss: 0.14655  Avg mIoU:  76.46  
[Epoch: 263] [Batch: 0301/0570] Loss: 0.10883  Avg Loss: 0.14595  Avg mIoU:  76.41  
[Epoch: 263] [Batch: 0351/0570] Loss: 0.13315  Avg Loss: 0.14614  Avg mIoU:  76.36  
[Epoch: 263] [Batch: 0401/0570] Loss: 0.14910  Avg Loss: 0.14578  Avg mIoU:  76.35  
[Epoch: 263] [Batch: 0451/0570] Loss: 0.12469  Avg Loss: 0.14425  Avg mIoU:  76.68  
[Epoch: 263] [Batch: 0501/0570] Loss: 0.11150  Avg Loss: 0.14478  Avg mIoU:  76.46  
[Epoch: 263] [Batch: 0551/0570] Loss: 0.20262  Avg Loss: 0.14438  Avg mIoU:  76.38  

*** Training [@Epoch 263] Avg Loss: 0.14484  Avg mIoU:  76.29  ***

[Epoch: 263] [Batch: 0001/0050] Loss: 0.12576  Avg Loss: 0.12576  Avg mIoU:  65.89  

*** Validation [@Epoch 263] Avg Loss: 0.16097  Avg mIoU:  63.35  ***

[Epoch: 264] [Batch: 0001/0570] Loss: 0.14392  Avg Loss: 0.14392  Avg mIoU:  42.98  
[Epoch: 264] [Batch: 0051/0570] Loss: 0.13723  Avg Loss: 0.14648  Avg mIoU:  75.11  
[Epoch: 264] [Batch: 0101/0570] Loss: 0.16907  Avg Loss: 0.14821  Avg mIoU:  75.16  
[Epoch: 264] [Batch: 0151/0570] Loss: 0.13626  Avg Loss: 0.14544  Avg mIoU:  75.90  
[Epoch: 264] [Batch: 0201/0570] Loss: 0.17187  Avg Loss: 0.14397  Avg mIoU:  76.34  
[Epoch: 264] [Batch: 0251/0570] Loss: 0.17567  Avg Loss: 0.14359  Avg mIoU:  76.31  
[Epoch: 264] [Batch: 0301/0570] Loss: 0.16770  Avg Loss: 0.14231  Avg mIoU:  76.34  
[Epoch: 264] [Batch: 0351/0570] Loss: 0.15833  Avg Loss: 0.14341  Avg mIoU:  76.20  
[Epoch: 264] [Batch: 0401/0570] Loss: 0.13826  Avg Loss: 0.14360  Avg mIoU:  76.16  
[Epoch: 264] [Batch: 0451/0570] Loss: 0.13200  Avg Loss: 0.14327  Avg mIoU:  76.29  
[Epoch: 264] [Batch: 0501/0570] Loss: 0.12164  Avg Loss: 0.14460  Avg mIoU:  76.22  
[Epoch: 264] [Batch: 0551/0570] Loss: 0.12879  Avg Loss: 0.14437  Avg mIoU:  76.30  

*** Training [@Epoch 264] Avg Loss: 0.14444  Avg mIoU:  76.39  ***

[Epoch: 264] [Batch: 0001/0050] Loss: 0.16041  Avg Loss: 0.16041  Avg mIoU:  64.23  

*** Validation [@Epoch 264] Avg Loss: 0.19068  Avg mIoU:  64.49  ***

[Epoch: 265] [Batch: 0001/0570] Loss: 0.14002  Avg Loss: 0.14002  Avg mIoU:  62.11  
[Epoch: 265] [Batch: 0051/0570] Loss: 0.13289  Avg Loss: 0.13107  Avg mIoU:  79.04  
[Epoch: 265] [Batch: 0101/0570] Loss: 0.14802  Avg Loss: 0.13985  Avg mIoU:  77.14  
[Epoch: 265] [Batch: 0151/0570] Loss: 0.17838  Avg Loss: 0.13983  Avg mIoU:  77.11  
[Epoch: 265] [Batch: 0201/0570] Loss: 0.14891  Avg Loss: 0.13913  Avg mIoU:  77.07  
[Epoch: 265] [Batch: 0251/0570] Loss: 0.14987  Avg Loss: 0.14002  Avg mIoU:  77.09  
[Epoch: 265] [Batch: 0301/0570] Loss: 0.12041  Avg Loss: 0.14141  Avg mIoU:  76.85  
[Epoch: 265] [Batch: 0351/0570] Loss: 0.12409  Avg Loss: 0.14204  Avg mIoU:  76.85  
[Epoch: 265] [Batch: 0401/0570] Loss: 0.13138  Avg Loss: 0.14320  Avg mIoU:  76.60  
[Epoch: 265] [Batch: 0451/0570] Loss: 0.13792  Avg Loss: 0.14430  Avg mIoU:  76.48  
[Epoch: 265] [Batch: 0501/0570] Loss: 0.12048  Avg Loss: 0.14476  Avg mIoU:  76.35  
[Epoch: 265] [Batch: 0551/0570] Loss: 0.24930  Avg Loss: 0.14533  Avg mIoU:  76.52  

*** Training [@Epoch 265] Avg Loss: 0.14517  Avg mIoU:  76.49  ***

[Epoch: 265] [Batch: 0001/0050] Loss: 0.14265  Avg Loss: 0.14265  Avg mIoU:  64.81  

*** Validation [@Epoch 265] Avg Loss: 0.17362  Avg mIoU:  64.51  ***

[Epoch: 266] [Batch: 0001/0570] Loss: 0.16629  Avg Loss: 0.16629  Avg mIoU:  45.57  
[Epoch: 266] [Batch: 0051/0570] Loss: 0.11999  Avg Loss: 0.14158  Avg mIoU:  76.93  
[Epoch: 266] [Batch: 0101/0570] Loss: 0.14897  Avg Loss: 0.14552  Avg mIoU:  76.13  
[Epoch: 266] [Batch: 0151/0570] Loss: 0.20646  Avg Loss: 0.14344  Avg mIoU:  76.27  
[Epoch: 266] [Batch: 0201/0570] Loss: 0.11632  Avg Loss: 0.14578  Avg mIoU:  76.30  
[Epoch: 266] [Batch: 0251/0570] Loss: 0.17191  Avg Loss: 0.14699  Avg mIoU:  76.08  
[Epoch: 266] [Batch: 0301/0570] Loss: 0.09075  Avg Loss: 0.14479  Avg mIoU:  76.28  
[Epoch: 266] [Batch: 0351/0570] Loss: 0.10870  Avg Loss: 0.14442  Avg mIoU:  76.38  
[Epoch: 266] [Batch: 0401/0570] Loss: 0.24968  Avg Loss: 0.14367  Avg mIoU:  76.52  
[Epoch: 266] [Batch: 0451/0570] Loss: 0.18099  Avg Loss: 0.14241  Avg mIoU:  76.69  
[Epoch: 266] [Batch: 0501/0570] Loss: 0.12072  Avg Loss: 0.14266  Avg mIoU:  76.56  
[Epoch: 266] [Batch: 0551/0570] Loss: 0.09405  Avg Loss: 0.14300  Avg mIoU:  76.57  

*** Training [@Epoch 266] Avg Loss: 0.14315  Avg mIoU:  76.64  ***

[Epoch: 266] [Batch: 0001/0050] Loss: 0.13790  Avg Loss: 0.13790  Avg mIoU:  67.63  

*** Validation [@Epoch 266] Avg Loss: 0.19045  Avg mIoU:  63.68  ***

[Epoch: 267] [Batch: 0001/0570] Loss: 0.11978  Avg Loss: 0.11978  Avg mIoU:  36.39  
[Epoch: 267] [Batch: 0051/0570] Loss: 0.13703  Avg Loss: 0.14028  Avg mIoU:  76.08  
[Epoch: 267] [Batch: 0101/0570] Loss: 0.12315  Avg Loss: 0.14065  Avg mIoU:  76.76  
[Epoch: 267] [Batch: 0151/0570] Loss: 0.09728  Avg Loss: 0.13862  Avg mIoU:  76.83  
[Epoch: 267] [Batch: 0201/0570] Loss: 0.15188  Avg Loss: 0.14006  Avg mIoU:  76.40  
[Epoch: 267] [Batch: 0251/0570] Loss: 0.13089  Avg Loss: 0.14046  Avg mIoU:  76.76  
[Epoch: 267] [Batch: 0301/0570] Loss: 0.17642  Avg Loss: 0.13991  Avg mIoU:  76.77  
[Epoch: 267] [Batch: 0351/0570] Loss: 0.13188  Avg Loss: 0.14105  Avg mIoU:  76.60  
[Epoch: 267] [Batch: 0401/0570] Loss: 0.10112  Avg Loss: 0.14196  Avg mIoU:  76.57  
[Epoch: 267] [Batch: 0451/0570] Loss: 0.13952  Avg Loss: 0.14229  Avg mIoU:  76.54  
[Epoch: 267] [Batch: 0501/0570] Loss: 0.14949  Avg Loss: 0.14215  Avg mIoU:  76.50  
[Epoch: 267] [Batch: 0551/0570] Loss: 0.15789  Avg Loss: 0.14223  Avg mIoU:  76.59  

*** Training [@Epoch 267] Avg Loss: 0.14259  Avg mIoU:  76.52  ***

[Epoch: 267] [Batch: 0001/0050] Loss: 0.12343  Avg Loss: 0.12343  Avg mIoU:  68.77  

*** Validation [@Epoch 267] Avg Loss: 0.15629  Avg mIoU:  65.39  ***

[Epoch: 268] [Batch: 0001/0570] Loss: 0.13170  Avg Loss: 0.13170  Avg mIoU:  43.40  
[Epoch: 268] [Batch: 0051/0570] Loss: 0.10740  Avg Loss: 0.13116  Avg mIoU:  77.38  
[Epoch: 268] [Batch: 0101/0570] Loss: 0.13548  Avg Loss: 0.13545  Avg mIoU:  76.96  
[Epoch: 268] [Batch: 0151/0570] Loss: 0.19884  Avg Loss: 0.14062  Avg mIoU:  76.97  
[Epoch: 268] [Batch: 0201/0570] Loss: 0.14012  Avg Loss: 0.14119  Avg mIoU:  76.83  
[Epoch: 268] [Batch: 0251/0570] Loss: 0.12238  Avg Loss: 0.14236  Avg mIoU:  76.46  
[Epoch: 268] [Batch: 0301/0570] Loss: 0.21603  Avg Loss: 0.14232  Avg mIoU:  76.43  
[Epoch: 268] [Batch: 0351/0570] Loss: 0.16009  Avg Loss: 0.14249  Avg mIoU:  76.33  
[Epoch: 268] [Batch: 0401/0570] Loss: 0.13569  Avg Loss: 0.14286  Avg mIoU:  76.38  
[Epoch: 268] [Batch: 0451/0570] Loss: 0.14783  Avg Loss: 0.14364  Avg mIoU:  76.26  
[Epoch: 268] [Batch: 0501/0570] Loss: 0.09019  Avg Loss: 0.14322  Avg mIoU:  76.35  
[Epoch: 268] [Batch: 0551/0570] Loss: 0.19278  Avg Loss: 0.14423  Avg mIoU:  76.28  

*** Training [@Epoch 268] Avg Loss: 0.14421  Avg mIoU:  76.32  ***

[Epoch: 268] [Batch: 0001/0050] Loss: 0.13595  Avg Loss: 0.13595  Avg mIoU:  66.60  

*** Validation [@Epoch 268] Avg Loss: 0.17277  Avg mIoU:  66.02  ***

[Epoch: 269] [Batch: 0001/0570] Loss: 0.19606  Avg Loss: 0.19606  Avg mIoU:  40.83  
[Epoch: 269] [Batch: 0051/0570] Loss: 0.20812  Avg Loss: 0.14605  Avg mIoU:  76.52  
[Epoch: 269] [Batch: 0101/0570] Loss: 0.09144  Avg Loss: 0.14480  Avg mIoU:  76.62  
[Epoch: 269] [Batch: 0151/0570] Loss: 0.12721  Avg Loss: 0.14430  Avg mIoU:  76.41  
[Epoch: 269] [Batch: 0201/0570] Loss: 0.14097  Avg Loss: 0.14298  Avg mIoU:  76.74  
[Epoch: 269] [Batch: 0251/0570] Loss: 0.13608  Avg Loss: 0.14376  Avg mIoU:  76.70  
[Epoch: 269] [Batch: 0301/0570] Loss: 0.15788  Avg Loss: 0.14380  Avg mIoU:  76.52  
[Epoch: 269] [Batch: 0351/0570] Loss: 0.18654  Avg Loss: 0.14264  Avg mIoU:  76.69  
[Epoch: 269] [Batch: 0401/0570] Loss: 0.13633  Avg Loss: 0.14209  Avg mIoU:  76.87  
[Epoch: 269] [Batch: 0451/0570] Loss: 0.16721  Avg Loss: 0.14215  Avg mIoU:  76.72  
[Epoch: 269] [Batch: 0501/0570] Loss: 0.16046  Avg Loss: 0.14233  Avg mIoU:  76.79  
[Epoch: 269] [Batch: 0551/0570] Loss: 0.13970  Avg Loss: 0.14199  Avg mIoU:  76.70  

*** Training [@Epoch 269] Avg Loss: 0.14209  Avg mIoU:  76.69  ***

[Epoch: 269] [Batch: 0001/0050] Loss: 0.14209  Avg Loss: 0.14209  Avg mIoU:  64.90  

*** Validation [@Epoch 269] Avg Loss: 0.16894  Avg mIoU:  64.86  ***

[Epoch: 270] [Batch: 0001/0570] Loss: 0.13865  Avg Loss: 0.13865  Avg mIoU:  45.00  
[Epoch: 270] [Batch: 0051/0570] Loss: 0.14171  Avg Loss: 0.14233  Avg mIoU:  75.39  
[Epoch: 270] [Batch: 0101/0570] Loss: 0.10426  Avg Loss: 0.13711  Avg mIoU:  76.59  
[Epoch: 270] [Batch: 0151/0570] Loss: 0.11402  Avg Loss: 0.13673  Avg mIoU:  76.82  
[Epoch: 270] [Batch: 0201/0570] Loss: 0.17640  Avg Loss: 0.13872  Avg mIoU:  76.85  
[Epoch: 270] [Batch: 0251/0570] Loss: 0.17206  Avg Loss: 0.14033  Avg mIoU:  76.67  
[Epoch: 270] [Batch: 0301/0570] Loss: 0.11266  Avg Loss: 0.14080  Avg mIoU:  76.72  
[Epoch: 270] [Batch: 0351/0570] Loss: 0.19243  Avg Loss: 0.14194  Avg mIoU:  76.44  
[Epoch: 270] [Batch: 0401/0570] Loss: 0.14462  Avg Loss: 0.14216  Avg mIoU:  76.32  
[Epoch: 270] [Batch: 0451/0570] Loss: 0.15075  Avg Loss: 0.14219  Avg mIoU:  76.46  
[Epoch: 270] [Batch: 0501/0570] Loss: 0.15313  Avg Loss: 0.14241  Avg mIoU:  76.54  
[Epoch: 270] [Batch: 0551/0570] Loss: 0.12988  Avg Loss: 0.14222  Avg mIoU:  76.66  

*** Training [@Epoch 270] Avg Loss: 0.14212  Avg mIoU:  76.74  ***

[Epoch: 270] [Batch: 0001/0050] Loss: 0.11563  Avg Loss: 0.11563  Avg mIoU:  69.45  

*** Validation [@Epoch 270] Avg Loss: 0.15813  Avg mIoU:  65.49  ***

[Epoch: 271] [Batch: 0001/0570] Loss: 0.14060  Avg Loss: 0.14060  Avg mIoU:  33.10  
[Epoch: 271] [Batch: 0051/0570] Loss: 0.13309  Avg Loss: 0.14491  Avg mIoU:  76.01  
[Epoch: 271] [Batch: 0101/0570] Loss: 0.16985  Avg Loss: 0.14445  Avg mIoU:  76.36  
[Epoch: 271] [Batch: 0151/0570] Loss: 0.17696  Avg Loss: 0.14510  Avg mIoU:  76.27  
[Epoch: 271] [Batch: 0201/0570] Loss: 0.15063  Avg Loss: 0.14433  Avg mIoU:  76.24  
[Epoch: 271] [Batch: 0251/0570] Loss: 0.21067  Avg Loss: 0.14512  Avg mIoU:  76.16  
[Epoch: 271] [Batch: 0301/0570] Loss: 0.20674  Avg Loss: 0.14550  Avg mIoU:  76.12  
[Epoch: 271] [Batch: 0351/0570] Loss: 0.16547  Avg Loss: 0.14398  Avg mIoU:  76.27  
[Epoch: 271] [Batch: 0401/0570] Loss: 0.16149  Avg Loss: 0.14342  Avg mIoU:  76.31  
[Epoch: 271] [Batch: 0451/0570] Loss: 0.13635  Avg Loss: 0.14359  Avg mIoU:  76.33  
[Epoch: 271] [Batch: 0501/0570] Loss: 0.13337  Avg Loss: 0.14328  Avg mIoU:  76.38  
[Epoch: 271] [Batch: 0551/0570] Loss: 0.08923  Avg Loss: 0.14233  Avg mIoU:  76.50  

*** Training [@Epoch 271] Avg Loss: 0.14300  Avg mIoU:  76.42  ***

[Epoch: 271] [Batch: 0001/0050] Loss: 0.16046  Avg Loss: 0.16046  Avg mIoU:  65.21  

*** Validation [@Epoch 271] Avg Loss: 0.20348  Avg mIoU:  63.51  ***

[Epoch: 272] [Batch: 0001/0570] Loss: 0.10784  Avg Loss: 0.10784  Avg mIoU:  53.19  
[Epoch: 272] [Batch: 0051/0570] Loss: 0.10864  Avg Loss: 0.14110  Avg mIoU:  76.59  
[Epoch: 272] [Batch: 0101/0570] Loss: 0.15597  Avg Loss: 0.14027  Avg mIoU:  76.62  
[Epoch: 272] [Batch: 0151/0570] Loss: 0.15490  Avg Loss: 0.14145  Avg mIoU:  76.39  
[Epoch: 272] [Batch: 0201/0570] Loss: 0.15922  Avg Loss: 0.14307  Avg mIoU:  76.69  
[Epoch: 272] [Batch: 0251/0570] Loss: 0.16614  Avg Loss: 0.14183  Avg mIoU:  76.77  
[Epoch: 272] [Batch: 0301/0570] Loss: 0.11288  Avg Loss: 0.14200  Avg mIoU:  76.64  
[Epoch: 272] [Batch: 0351/0570] Loss: 0.13780  Avg Loss: 0.14205  Avg mIoU:  77.11  
[Epoch: 272] [Batch: 0401/0570] Loss: 0.09726  Avg Loss: 0.14266  Avg mIoU:  77.09  
[Epoch: 272] [Batch: 0451/0570] Loss: 0.12568  Avg Loss: 0.14288  Avg mIoU:  77.06  
[Epoch: 272] [Batch: 0501/0570] Loss: 0.14130  Avg Loss: 0.14256  Avg mIoU:  76.90  
[Epoch: 272] [Batch: 0551/0570] Loss: 0.16611  Avg Loss: 0.14282  Avg mIoU:  76.80  

*** Training [@Epoch 272] Avg Loss: 0.14270  Avg mIoU:  76.75  ***

[Epoch: 272] [Batch: 0001/0050] Loss: 0.12892  Avg Loss: 0.12892  Avg mIoU:  66.94  

*** Validation [@Epoch 272] Avg Loss: 0.16155  Avg mIoU:  65.28  ***

[Epoch: 273] [Batch: 0001/0570] Loss: 0.17339  Avg Loss: 0.17339  Avg mIoU:  34.85  
[Epoch: 273] [Batch: 0051/0570] Loss: 0.19156  Avg Loss: 0.13882  Avg mIoU:  76.27  
[Epoch: 273] [Batch: 0101/0570] Loss: 0.12176  Avg Loss: 0.13859  Avg mIoU:  77.23  
[Epoch: 273] [Batch: 0151/0570] Loss: 0.09579  Avg Loss: 0.13966  Avg mIoU:  77.11  
[Epoch: 273] [Batch: 0201/0570] Loss: 0.12012  Avg Loss: 0.14094  Avg mIoU:  76.77  
[Epoch: 273] [Batch: 0251/0570] Loss: 0.13254  Avg Loss: 0.13993  Avg mIoU:  76.89  
[Epoch: 273] [Batch: 0301/0570] Loss: 0.11564  Avg Loss: 0.13944  Avg mIoU:  76.88  
[Epoch: 273] [Batch: 0351/0570] Loss: 0.11910  Avg Loss: 0.14020  Avg mIoU:  76.88  
[Epoch: 273] [Batch: 0401/0570] Loss: 0.12619  Avg Loss: 0.14017  Avg mIoU:  76.83  
[Epoch: 273] [Batch: 0451/0570] Loss: 0.14592  Avg Loss: 0.14047  Avg mIoU:  76.80  
[Epoch: 273] [Batch: 0501/0570] Loss: 0.10926  Avg Loss: 0.14085  Avg mIoU:  76.79  
[Epoch: 273] [Batch: 0551/0570] Loss: 0.13771  Avg Loss: 0.14223  Avg mIoU:  76.57  

*** Training [@Epoch 273] Avg Loss: 0.14243  Avg mIoU:  76.48  ***

[Epoch: 273] [Batch: 0001/0050] Loss: 0.12228  Avg Loss: 0.12228  Avg mIoU:  67.11  

*** Validation [@Epoch 273] Avg Loss: 0.17820  Avg mIoU:  63.12  ***

[Epoch: 274] [Batch: 0001/0570] Loss: 0.15821  Avg Loss: 0.15821  Avg mIoU:  36.28  
[Epoch: 274] [Batch: 0051/0570] Loss: 0.11731  Avg Loss: 0.14138  Avg mIoU:  75.71  
[Epoch: 274] [Batch: 0101/0570] Loss: 0.21835  Avg Loss: 0.14102  Avg mIoU:  75.79  
[Epoch: 274] [Batch: 0151/0570] Loss: 0.14513  Avg Loss: 0.14411  Avg mIoU:  76.32  
[Epoch: 274] [Batch: 0201/0570] Loss: 0.12104  Avg Loss: 0.14418  Avg mIoU:  76.34  
[Epoch: 274] [Batch: 0251/0570] Loss: 0.12131  Avg Loss: 0.14439  Avg mIoU:  76.43  
[Epoch: 274] [Batch: 0301/0570] Loss: 0.15188  Avg Loss: 0.14397  Avg mIoU:  76.49  
[Epoch: 274] [Batch: 0351/0570] Loss: 0.16705  Avg Loss: 0.14523  Avg mIoU:  76.24  
[Epoch: 274] [Batch: 0401/0570] Loss: 0.14818  Avg Loss: 0.14532  Avg mIoU:  76.24  
[Epoch: 274] [Batch: 0451/0570] Loss: 0.12962  Avg Loss: 0.14352  Avg mIoU:  76.25  
[Epoch: 274] [Batch: 0501/0570] Loss: 0.07556  Avg Loss: 0.14288  Avg mIoU:  76.34  
[Epoch: 274] [Batch: 0551/0570] Loss: 0.09103  Avg Loss: 0.14258  Avg mIoU:  76.48  

*** Training [@Epoch 274] Avg Loss: 0.14264  Avg mIoU:  76.51  ***

[Epoch: 274] [Batch: 0001/0050] Loss: 0.13970  Avg Loss: 0.13970  Avg mIoU:  64.19  

*** Validation [@Epoch 274] Avg Loss: 0.18028  Avg mIoU:  63.75  ***

[Epoch: 275] [Batch: 0001/0570] Loss: 0.10216  Avg Loss: 0.10216  Avg mIoU:  58.30  
[Epoch: 275] [Batch: 0051/0570] Loss: 0.15006  Avg Loss: 0.13707  Avg mIoU:  77.77  
[Epoch: 275] [Batch: 0101/0570] Loss: 0.18258  Avg Loss: 0.14000  Avg mIoU:  77.17  
[Epoch: 275] [Batch: 0151/0570] Loss: 0.09497  Avg Loss: 0.14050  Avg mIoU:  76.79  
[Epoch: 275] [Batch: 0201/0570] Loss: 0.20705  Avg Loss: 0.14183  Avg mIoU:  76.49  
[Epoch: 275] [Batch: 0251/0570] Loss: 0.13594  Avg Loss: 0.14052  Avg mIoU:  76.76  
[Epoch: 275] [Batch: 0301/0570] Loss: 0.18059  Avg Loss: 0.14179  Avg mIoU:  76.47  
[Epoch: 275] [Batch: 0351/0570] Loss: 0.13013  Avg Loss: 0.14275  Avg mIoU:  76.15  
[Epoch: 275] [Batch: 0401/0570] Loss: 0.15050  Avg Loss: 0.14160  Avg mIoU:  76.34  
[Epoch: 275] [Batch: 0451/0570] Loss: 0.15607  Avg Loss: 0.14236  Avg mIoU:  76.46  
[Epoch: 275] [Batch: 0501/0570] Loss: 0.10508  Avg Loss: 0.14133  Avg mIoU:  76.55  
[Epoch: 275] [Batch: 0551/0570] Loss: 0.13639  Avg Loss: 0.14209  Avg mIoU:  76.47  

*** Training [@Epoch 275] Avg Loss: 0.14249  Avg mIoU:  76.43  ***

[Epoch: 275] [Batch: 0001/0050] Loss: 0.15637  Avg Loss: 0.15637  Avg mIoU:  62.38  

*** Validation [@Epoch 275] Avg Loss: 0.20075  Avg mIoU:  63.44  ***

[Epoch: 276] [Batch: 0001/0570] Loss: 0.11977  Avg Loss: 0.11977  Avg mIoU:  41.38  
[Epoch: 276] [Batch: 0051/0570] Loss: 0.16546  Avg Loss: 0.14728  Avg mIoU:  75.92  
[Epoch: 276] [Batch: 0101/0570] Loss: 0.13202  Avg Loss: 0.14432  Avg mIoU:  76.61  
[Epoch: 276] [Batch: 0151/0570] Loss: 0.13087  Avg Loss: 0.14270  Avg mIoU:  77.13  
[Epoch: 276] [Batch: 0201/0570] Loss: 0.14847  Avg Loss: 0.14386  Avg mIoU:  76.91  
[Epoch: 276] [Batch: 0251/0570] Loss: 0.15173  Avg Loss: 0.14256  Avg mIoU:  77.16  
[Epoch: 276] [Batch: 0301/0570] Loss: 0.18938  Avg Loss: 0.14144  Avg mIoU:  77.34  
[Epoch: 276] [Batch: 0351/0570] Loss: 0.09912  Avg Loss: 0.14077  Avg mIoU:  77.38  
[Epoch: 276] [Batch: 0401/0570] Loss: 0.12848  Avg Loss: 0.14262  Avg mIoU:  77.08  
[Epoch: 276] [Batch: 0451/0570] Loss: 0.07762  Avg Loss: 0.14166  Avg mIoU:  77.00  
[Epoch: 276] [Batch: 0501/0570] Loss: 0.18100  Avg Loss: 0.14160  Avg mIoU:  76.90  
[Epoch: 276] [Batch: 0551/0570] Loss: 0.14984  Avg Loss: 0.14179  Avg mIoU:  76.94  

*** Training [@Epoch 276] Avg Loss: 0.14155  Avg mIoU:  76.97  ***

[Epoch: 276] [Batch: 0001/0050] Loss: 0.12734  Avg Loss: 0.12734  Avg mIoU:  67.62  

*** Validation [@Epoch 276] Avg Loss: 0.17272  Avg mIoU:  66.27  ***

[Epoch: 277] [Batch: 0001/0570] Loss: 0.12819  Avg Loss: 0.12819  Avg mIoU:  56.32  
[Epoch: 277] [Batch: 0051/0570] Loss: 0.13528  Avg Loss: 0.14514  Avg mIoU:  75.66  
[Epoch: 277] [Batch: 0101/0570] Loss: 0.12777  Avg Loss: 0.14562  Avg mIoU:  76.22  
[Epoch: 277] [Batch: 0151/0570] Loss: 0.14297  Avg Loss: 0.14284  Avg mIoU:  77.14  
[Epoch: 277] [Batch: 0201/0570] Loss: 0.15064  Avg Loss: 0.14189  Avg mIoU:  76.77  
[Epoch: 277] [Batch: 0251/0570] Loss: 0.15843  Avg Loss: 0.14236  Avg mIoU:  76.79  
[Epoch: 277] [Batch: 0301/0570] Loss: 0.11307  Avg Loss: 0.14262  Avg mIoU:  76.79  
[Epoch: 277] [Batch: 0351/0570] Loss: 0.17675  Avg Loss: 0.14257  Avg mIoU:  76.77  
[Epoch: 277] [Batch: 0401/0570] Loss: 0.12242  Avg Loss: 0.14155  Avg mIoU:  76.91  
[Epoch: 277] [Batch: 0451/0570] Loss: 0.10525  Avg Loss: 0.14159  Avg mIoU:  76.85  
[Epoch: 277] [Batch: 0501/0570] Loss: 0.13476  Avg Loss: 0.14194  Avg mIoU:  76.86  
[Epoch: 277] [Batch: 0551/0570] Loss: 0.16373  Avg Loss: 0.14108  Avg mIoU:  76.85  

*** Training [@Epoch 277] Avg Loss: 0.14106  Avg mIoU:  76.81  ***

[Epoch: 277] [Batch: 0001/0050] Loss: 0.14430  Avg Loss: 0.14430  Avg mIoU:  65.31  

*** Validation [@Epoch 277] Avg Loss: 0.18595  Avg mIoU:  64.41  ***

[Epoch: 278] [Batch: 0001/0570] Loss: 0.13014  Avg Loss: 0.13014  Avg mIoU:  38.24  
[Epoch: 278] [Batch: 0051/0570] Loss: 0.17793  Avg Loss: 0.13521  Avg mIoU:  78.10  
[Epoch: 278] [Batch: 0101/0570] Loss: 0.14481  Avg Loss: 0.13961  Avg mIoU:  77.51  
[Epoch: 278] [Batch: 0151/0570] Loss: 0.08140  Avg Loss: 0.13753  Avg mIoU:  77.06  
[Epoch: 278] [Batch: 0201/0570] Loss: 0.12531  Avg Loss: 0.13715  Avg mIoU:  77.22  
[Epoch: 278] [Batch: 0251/0570] Loss: 0.13509  Avg Loss: 0.13885  Avg mIoU:  76.74  
[Epoch: 278] [Batch: 0301/0570] Loss: 0.13575  Avg Loss: 0.13925  Avg mIoU:  77.27  
[Epoch: 278] [Batch: 0351/0570] Loss: 0.10761  Avg Loss: 0.13990  Avg mIoU:  77.09  
[Epoch: 278] [Batch: 0401/0570] Loss: 0.15565  Avg Loss: 0.14014  Avg mIoU:  76.93  
[Epoch: 278] [Batch: 0451/0570] Loss: 0.17531  Avg Loss: 0.14095  Avg mIoU:  76.87  
[Epoch: 278] [Batch: 0501/0570] Loss: 0.11512  Avg Loss: 0.14137  Avg mIoU:  76.80  
[Epoch: 278] [Batch: 0551/0570] Loss: 0.15929  Avg Loss: 0.14123  Avg mIoU:  76.86  

*** Training [@Epoch 278] Avg Loss: 0.14101  Avg mIoU:  76.85  ***

[Epoch: 278] [Batch: 0001/0050] Loss: 0.15056  Avg Loss: 0.15056  Avg mIoU:  65.91  

*** Validation [@Epoch 278] Avg Loss: 0.19205  Avg mIoU:  64.93  ***

[Epoch: 279] [Batch: 0001/0570] Loss: 0.13875  Avg Loss: 0.13875  Avg mIoU:  41.66  
[Epoch: 279] [Batch: 0051/0570] Loss: 0.13692  Avg Loss: 0.13256  Avg mIoU:  77.11  
[Epoch: 279] [Batch: 0101/0570] Loss: 0.08988  Avg Loss: 0.13532  Avg mIoU:  77.63  
[Epoch: 279] [Batch: 0151/0570] Loss: 0.13679  Avg Loss: 0.13912  Avg mIoU:  77.53  
[Epoch: 279] [Batch: 0201/0570] Loss: 0.14400  Avg Loss: 0.13935  Avg mIoU:  77.24  
[Epoch: 279] [Batch: 0251/0570] Loss: 0.15793  Avg Loss: 0.14090  Avg mIoU:  77.08  
[Epoch: 279] [Batch: 0301/0570] Loss: 0.11098  Avg Loss: 0.14070  Avg mIoU:  76.92  
[Epoch: 279] [Batch: 0351/0570] Loss: 0.14530  Avg Loss: 0.14044  Avg mIoU:  77.06  
[Epoch: 279] [Batch: 0401/0570] Loss: 0.10990  Avg Loss: 0.14059  Avg mIoU:  77.09  
[Epoch: 279] [Batch: 0451/0570] Loss: 0.17187  Avg Loss: 0.14045  Avg mIoU:  77.00  
[Epoch: 279] [Batch: 0501/0570] Loss: 0.09978  Avg Loss: 0.14072  Avg mIoU:  76.95  
[Epoch: 279] [Batch: 0551/0570] Loss: 0.17898  Avg Loss: 0.14041  Avg mIoU:  77.01  

*** Training [@Epoch 279] Avg Loss: 0.14036  Avg mIoU:  77.00  ***

[Epoch: 279] [Batch: 0001/0050] Loss: 0.12565  Avg Loss: 0.12565  Avg mIoU:  67.77  

*** Validation [@Epoch 279] Avg Loss: 0.17369  Avg mIoU:  64.85  ***

[Epoch: 280] [Batch: 0001/0570] Loss: 0.12275  Avg Loss: 0.12275  Avg mIoU:  46.50  
[Epoch: 280] [Batch: 0051/0570] Loss: 0.13978  Avg Loss: 0.13988  Avg mIoU:  78.18  
[Epoch: 280] [Batch: 0101/0570] Loss: 0.11204  Avg Loss: 0.13557  Avg mIoU:  77.84  
[Epoch: 280] [Batch: 0151/0570] Loss: 0.19098  Avg Loss: 0.13796  Avg mIoU:  78.05  
[Epoch: 280] [Batch: 0201/0570] Loss: 0.10555  Avg Loss: 0.13785  Avg mIoU:  77.90  
[Epoch: 280] [Batch: 0251/0570] Loss: 0.10870  Avg Loss: 0.13916  Avg mIoU:  77.79  
[Epoch: 280] [Batch: 0301/0570] Loss: 0.09816  Avg Loss: 0.13909  Avg mIoU:  77.73  
[Epoch: 280] [Batch: 0351/0570] Loss: 0.10749  Avg Loss: 0.13874  Avg mIoU:  77.83  
[Epoch: 280] [Batch: 0401/0570] Loss: 0.10095  Avg Loss: 0.13971  Avg mIoU:  77.61  
[Epoch: 280] [Batch: 0451/0570] Loss: 0.15042  Avg Loss: 0.13989  Avg mIoU:  77.30  
[Epoch: 280] [Batch: 0501/0570] Loss: 0.21445  Avg Loss: 0.14067  Avg mIoU:  77.24  
[Epoch: 280] [Batch: 0551/0570] Loss: 0.07951  Avg Loss: 0.14073  Avg mIoU:  77.11  

*** Training [@Epoch 280] Avg Loss: 0.14088  Avg mIoU:  77.04  ***

[Epoch: 280] [Batch: 0001/0050] Loss: 0.12763  Avg Loss: 0.12763  Avg mIoU:  65.56  

*** Validation [@Epoch 280] Avg Loss: 0.15998  Avg mIoU:  64.21  ***

[Epoch: 281] [Batch: 0001/0570] Loss: 0.10763  Avg Loss: 0.10763  Avg mIoU:  41.13  
[Epoch: 281] [Batch: 0051/0570] Loss: 0.11941  Avg Loss: 0.13794  Avg mIoU:  76.27  
[Epoch: 281] [Batch: 0101/0570] Loss: 0.11038  Avg Loss: 0.14079  Avg mIoU:  76.16  
[Epoch: 281] [Batch: 0151/0570] Loss: 0.16576  Avg Loss: 0.13895  Avg mIoU:  76.60  
[Epoch: 281] [Batch: 0201/0570] Loss: 0.20601  Avg Loss: 0.13912  Avg mIoU:  77.27  
[Epoch: 281] [Batch: 0251/0570] Loss: 0.17041  Avg Loss: 0.13838  Avg mIoU:  77.34  
[Epoch: 281] [Batch: 0301/0570] Loss: 0.13928  Avg Loss: 0.13908  Avg mIoU:  77.28  
[Epoch: 281] [Batch: 0351/0570] Loss: 0.12416  Avg Loss: 0.13990  Avg mIoU:  77.26  
[Epoch: 281] [Batch: 0401/0570] Loss: 0.11215  Avg Loss: 0.13999  Avg mIoU:  77.10  
[Epoch: 281] [Batch: 0451/0570] Loss: 0.13832  Avg Loss: 0.13979  Avg mIoU:  77.09  
[Epoch: 281] [Batch: 0501/0570] Loss: 0.16003  Avg Loss: 0.13912  Avg mIoU:  77.26  
[Epoch: 281] [Batch: 0551/0570] Loss: 0.17804  Avg Loss: 0.14003  Avg mIoU:  77.12  

*** Training [@Epoch 281] Avg Loss: 0.13982  Avg mIoU:  77.21  ***

[Epoch: 281] [Batch: 0001/0050] Loss: 0.15353  Avg Loss: 0.15353  Avg mIoU:  64.01  

*** Validation [@Epoch 281] Avg Loss: 0.20664  Avg mIoU:  64.42  ***

[Epoch: 282] [Batch: 0001/0570] Loss: 0.08202  Avg Loss: 0.08202  Avg mIoU:  38.26  
[Epoch: 282] [Batch: 0051/0570] Loss: 0.15305  Avg Loss: 0.14051  Avg mIoU:  75.91  
[Epoch: 282] [Batch: 0101/0570] Loss: 0.16832  Avg Loss: 0.13837  Avg mIoU:  76.27  
[Epoch: 282] [Batch: 0151/0570] Loss: 0.10541  Avg Loss: 0.14109  Avg mIoU:  76.26  
[Epoch: 282] [Batch: 0201/0570] Loss: 0.11347  Avg Loss: 0.13944  Avg mIoU:  76.73  
[Epoch: 282] [Batch: 0251/0570] Loss: 0.15010  Avg Loss: 0.13975  Avg mIoU:  76.94  
[Epoch: 282] [Batch: 0301/0570] Loss: 0.16840  Avg Loss: 0.14162  Avg mIoU:  76.61  
[Epoch: 282] [Batch: 0351/0570] Loss: 0.13584  Avg Loss: 0.14114  Avg mIoU:  76.65  
[Epoch: 282] [Batch: 0401/0570] Loss: 0.13686  Avg Loss: 0.14145  Avg mIoU:  76.79  
[Epoch: 282] [Batch: 0451/0570] Loss: 0.18748  Avg Loss: 0.14163  Avg mIoU:  76.78  
[Epoch: 282] [Batch: 0501/0570] Loss: 0.12472  Avg Loss: 0.14144  Avg mIoU:  76.83  
[Epoch: 282] [Batch: 0551/0570] Loss: 0.13366  Avg Loss: 0.14111  Avg mIoU:  76.89  

*** Training [@Epoch 282] Avg Loss: 0.14142  Avg mIoU:  76.83  ***

[Epoch: 282] [Batch: 0001/0050] Loss: 0.12780  Avg Loss: 0.12780  Avg mIoU:  65.01  

*** Validation [@Epoch 282] Avg Loss: 0.16794  Avg mIoU:  62.26  ***

[Epoch: 283] [Batch: 0001/0570] Loss: 0.12749  Avg Loss: 0.12749  Avg mIoU:  45.50  
[Epoch: 283] [Batch: 0051/0570] Loss: 0.13228  Avg Loss: 0.13708  Avg mIoU:  77.72  
[Epoch: 283] [Batch: 0101/0570] Loss: 0.11366  Avg Loss: 0.13982  Avg mIoU:  77.50  
[Epoch: 283] [Batch: 0151/0570] Loss: 0.31457  Avg Loss: 0.14037  Avg mIoU:  77.43  
[Epoch: 283] [Batch: 0201/0570] Loss: 0.12569  Avg Loss: 0.14190  Avg mIoU:  76.83  
[Epoch: 283] [Batch: 0251/0570] Loss: 0.13808  Avg Loss: 0.14242  Avg mIoU:  76.88  
[Epoch: 283] [Batch: 0301/0570] Loss: 0.10995  Avg Loss: 0.14200  Avg mIoU:  76.71  
[Epoch: 283] [Batch: 0351/0570] Loss: 0.13387  Avg Loss: 0.14210  Avg mIoU:  76.60  
[Epoch: 283] [Batch: 0401/0570] Loss: 0.19538  Avg Loss: 0.14241  Avg mIoU:  76.65  
[Epoch: 283] [Batch: 0451/0570] Loss: 0.15018  Avg Loss: 0.14240  Avg mIoU:  76.66  
[Epoch: 283] [Batch: 0501/0570] Loss: 0.11954  Avg Loss: 0.14257  Avg mIoU:  76.79  
[Epoch: 283] [Batch: 0551/0570] Loss: 0.21846  Avg Loss: 0.14193  Avg mIoU:  76.88  

*** Training [@Epoch 283] Avg Loss: 0.14183  Avg mIoU:  76.87  ***

[Epoch: 283] [Batch: 0001/0050] Loss: 0.13365  Avg Loss: 0.13365  Avg mIoU:  68.37  

*** Validation [@Epoch 283] Avg Loss: 0.17991  Avg mIoU:  64.01  ***

[Epoch: 284] [Batch: 0001/0570] Loss: 0.10775  Avg Loss: 0.10775  Avg mIoU:  51.05  
[Epoch: 284] [Batch: 0051/0570] Loss: 0.18371  Avg Loss: 0.14756  Avg mIoU:  76.24  
[Epoch: 284] [Batch: 0101/0570] Loss: 0.12940  Avg Loss: 0.14179  Avg mIoU:  77.68  
[Epoch: 284] [Batch: 0151/0570] Loss: 0.16921  Avg Loss: 0.13830  Avg mIoU:  77.49  
[Epoch: 284] [Batch: 0201/0570] Loss: 0.09737  Avg Loss: 0.14015  Avg mIoU:  77.39  
[Epoch: 284] [Batch: 0251/0570] Loss: 0.18954  Avg Loss: 0.14089  Avg mIoU:  77.26  
[Epoch: 284] [Batch: 0301/0570] Loss: 0.12767  Avg Loss: 0.13974  Avg mIoU:  77.34  
[Epoch: 284] [Batch: 0351/0570] Loss: 0.12098  Avg Loss: 0.14001  Avg mIoU:  77.18  
[Epoch: 284] [Batch: 0401/0570] Loss: 0.11719  Avg Loss: 0.14079  Avg mIoU:  77.15  
[Epoch: 284] [Batch: 0451/0570] Loss: 0.16463  Avg Loss: 0.14095  Avg mIoU:  76.95  
[Epoch: 284] [Batch: 0501/0570] Loss: 0.13521  Avg Loss: 0.14038  Avg mIoU:  76.91  
[Epoch: 284] [Batch: 0551/0570] Loss: 0.09844  Avg Loss: 0.14088  Avg mIoU:  76.88  

*** Training [@Epoch 284] Avg Loss: 0.14086  Avg mIoU:  76.85  ***

[Epoch: 284] [Batch: 0001/0050] Loss: 0.13920  Avg Loss: 0.13920  Avg mIoU:  66.48  

*** Validation [@Epoch 284] Avg Loss: 0.18159  Avg mIoU:  64.89  ***

[Epoch: 285] [Batch: 0001/0570] Loss: 0.12910  Avg Loss: 0.12910  Avg mIoU:  32.90  
[Epoch: 285] [Batch: 0051/0570] Loss: 0.12937  Avg Loss: 0.13879  Avg mIoU:  78.47  
[Epoch: 285] [Batch: 0101/0570] Loss: 0.09960  Avg Loss: 0.13939  Avg mIoU:  78.11  
[Epoch: 285] [Batch: 0151/0570] Loss: 0.16456  Avg Loss: 0.14055  Avg mIoU:  77.62  
[Epoch: 285] [Batch: 0201/0570] Loss: 0.12764  Avg Loss: 0.13943  Avg mIoU:  77.58  
[Epoch: 285] [Batch: 0251/0570] Loss: 0.10633  Avg Loss: 0.13951  Avg mIoU:  77.46  
[Epoch: 285] [Batch: 0301/0570] Loss: 0.15248  Avg Loss: 0.14048  Avg mIoU:  77.39  
[Epoch: 285] [Batch: 0351/0570] Loss: 0.15714  Avg Loss: 0.14015  Avg mIoU:  77.26  
[Epoch: 285] [Batch: 0401/0570] Loss: 0.12858  Avg Loss: 0.13995  Avg mIoU:  77.18  
[Epoch: 285] [Batch: 0451/0570] Loss: 0.15114  Avg Loss: 0.14009  Avg mIoU:  77.18  
[Epoch: 285] [Batch: 0501/0570] Loss: 0.17893  Avg Loss: 0.14032  Avg mIoU:  77.19  
[Epoch: 285] [Batch: 0551/0570] Loss: 0.15152  Avg Loss: 0.14032  Avg mIoU:  77.07  

*** Training [@Epoch 285] Avg Loss: 0.14008  Avg mIoU:  77.09  ***

[Epoch: 285] [Batch: 0001/0050] Loss: 0.15097  Avg Loss: 0.15097  Avg mIoU:  65.10  

*** Validation [@Epoch 285] Avg Loss: 0.18293  Avg mIoU:  64.39  ***

[Epoch: 286] [Batch: 0001/0570] Loss: 0.18088  Avg Loss: 0.18088  Avg mIoU:  50.64  
[Epoch: 286] [Batch: 0051/0570] Loss: 0.17245  Avg Loss: 0.13783  Avg mIoU:  76.71  
[Epoch: 286] [Batch: 0101/0570] Loss: 0.18245  Avg Loss: 0.14094  Avg mIoU:  77.20  
[Epoch: 286] [Batch: 0151/0570] Loss: 0.09489  Avg Loss: 0.14124  Avg mIoU:  77.70  
[Epoch: 286] [Batch: 0201/0570] Loss: 0.15779  Avg Loss: 0.13957  Avg mIoU:  77.74  
[Epoch: 286] [Batch: 0251/0570] Loss: 0.09527  Avg Loss: 0.14099  Avg mIoU:  77.27  
[Epoch: 286] [Batch: 0301/0570] Loss: 0.14609  Avg Loss: 0.14085  Avg mIoU:  77.27  
[Epoch: 286] [Batch: 0351/0570] Loss: 0.13276  Avg Loss: 0.14001  Avg mIoU:  77.22  
[Epoch: 286] [Batch: 0401/0570] Loss: 0.13375  Avg Loss: 0.14026  Avg mIoU:  77.08  
[Epoch: 286] [Batch: 0451/0570] Loss: 0.15038  Avg Loss: 0.14127  Avg mIoU:  77.01  
[Epoch: 286] [Batch: 0501/0570] Loss: 0.13440  Avg Loss: 0.14143  Avg mIoU:  76.87  
[Epoch: 286] [Batch: 0551/0570] Loss: 0.11665  Avg Loss: 0.14099  Avg mIoU:  76.86  

*** Training [@Epoch 286] Avg Loss: 0.14102  Avg mIoU:  76.86  ***

[Epoch: 286] [Batch: 0001/0050] Loss: 0.13250  Avg Loss: 0.13250  Avg mIoU:  68.02  

*** Validation [@Epoch 286] Avg Loss: 0.18597  Avg mIoU:  66.28  ***

[Epoch: 287] [Batch: 0001/0570] Loss: 0.14133  Avg Loss: 0.14133  Avg mIoU:  40.33  
[Epoch: 287] [Batch: 0051/0570] Loss: 0.15568  Avg Loss: 0.13813  Avg mIoU:  78.32  
[Epoch: 287] [Batch: 0101/0570] Loss: 0.13812  Avg Loss: 0.13923  Avg mIoU:  78.02  
[Epoch: 287] [Batch: 0151/0570] Loss: 0.13246  Avg Loss: 0.13967  Avg mIoU:  77.38  
[Epoch: 287] [Batch: 0201/0570] Loss: 0.13692  Avg Loss: 0.13808  Avg mIoU:  77.40  
[Epoch: 287] [Batch: 0251/0570] Loss: 0.13294  Avg Loss: 0.13861  Avg mIoU:  77.46  
[Epoch: 287] [Batch: 0301/0570] Loss: 0.13109  Avg Loss: 0.13794  Avg mIoU:  77.38  
[Epoch: 287] [Batch: 0351/0570] Loss: 0.09164  Avg Loss: 0.13811  Avg mIoU:  77.40  
[Epoch: 287] [Batch: 0401/0570] Loss: 0.13577  Avg Loss: 0.13868  Avg mIoU:  77.28  
[Epoch: 287] [Batch: 0451/0570] Loss: 0.14183  Avg Loss: 0.13940  Avg mIoU:  77.12  
[Epoch: 287] [Batch: 0501/0570] Loss: 0.16473  Avg Loss: 0.13969  Avg mIoU:  77.15  
[Epoch: 287] [Batch: 0551/0570] Loss: 0.14033  Avg Loss: 0.13956  Avg mIoU:  77.09  

*** Training [@Epoch 287] Avg Loss: 0.13958  Avg mIoU:  77.02  ***

[Epoch: 287] [Batch: 0001/0050] Loss: 0.13374  Avg Loss: 0.13374  Avg mIoU:  65.46  

*** Validation [@Epoch 287] Avg Loss: 0.18026  Avg mIoU:  65.92  ***

[Epoch: 288] [Batch: 0001/0570] Loss: 0.12896  Avg Loss: 0.12896  Avg mIoU:  56.88  
[Epoch: 288] [Batch: 0051/0570] Loss: 0.09541  Avg Loss: 0.14153  Avg mIoU:  76.89  
[Epoch: 288] [Batch: 0101/0570] Loss: 0.16638  Avg Loss: 0.13791  Avg mIoU:  77.21  
[Epoch: 288] [Batch: 0151/0570] Loss: 0.21527  Avg Loss: 0.13797  Avg mIoU:  77.73  
[Epoch: 288] [Batch: 0201/0570] Loss: 0.12488  Avg Loss: 0.13682  Avg mIoU:  78.03  
[Epoch: 288] [Batch: 0251/0570] Loss: 0.10831  Avg Loss: 0.13575  Avg mIoU:  77.88  
[Epoch: 288] [Batch: 0301/0570] Loss: 0.12764  Avg Loss: 0.13507  Avg mIoU:  78.07  
[Epoch: 288] [Batch: 0351/0570] Loss: 0.14790  Avg Loss: 0.13553  Avg mIoU:  78.11  
[Epoch: 288] [Batch: 0401/0570] Loss: 0.18659  Avg Loss: 0.13621  Avg mIoU:  77.73  
[Epoch: 288] [Batch: 0451/0570] Loss: 0.10429  Avg Loss: 0.13599  Avg mIoU:  77.73  
[Epoch: 288] [Batch: 0501/0570] Loss: 0.16981  Avg Loss: 0.13691  Avg mIoU:  77.61  
[Epoch: 288] [Batch: 0551/0570] Loss: 0.16558  Avg Loss: 0.13707  Avg mIoU:  77.61  

*** Training [@Epoch 288] Avg Loss: 0.13700  Avg mIoU:  77.54  ***

[Epoch: 288] [Batch: 0001/0050] Loss: 0.13292  Avg Loss: 0.13292  Avg mIoU:  65.99  

*** Validation [@Epoch 288] Avg Loss: 0.18120  Avg mIoU:  65.47  ***

[Epoch: 289] [Batch: 0001/0570] Loss: 0.19204  Avg Loss: 0.19204  Avg mIoU:  52.57  
[Epoch: 289] [Batch: 0051/0570] Loss: 0.13391  Avg Loss: 0.14351  Avg mIoU:  77.08  
[Epoch: 289] [Batch: 0101/0570] Loss: 0.10727  Avg Loss: 0.14155  Avg mIoU:  77.34  
[Epoch: 289] [Batch: 0151/0570] Loss: 0.12883  Avg Loss: 0.13830  Avg mIoU:  77.53  
[Epoch: 289] [Batch: 0201/0570] Loss: 0.16577  Avg Loss: 0.13881  Avg mIoU:  77.35  
[Epoch: 289] [Batch: 0251/0570] Loss: 0.13593  Avg Loss: 0.13915  Avg mIoU:  77.66  
[Epoch: 289] [Batch: 0301/0570] Loss: 0.20292  Avg Loss: 0.13973  Avg mIoU:  77.30  
[Epoch: 289] [Batch: 0351/0570] Loss: 0.11342  Avg Loss: 0.14048  Avg mIoU:  77.17  
[Epoch: 289] [Batch: 0401/0570] Loss: 0.12339  Avg Loss: 0.14007  Avg mIoU:  77.07  
[Epoch: 289] [Batch: 0451/0570] Loss: 0.12968  Avg Loss: 0.14023  Avg mIoU:  77.24  
[Epoch: 289] [Batch: 0501/0570] Loss: 0.15408  Avg Loss: 0.13931  Avg mIoU:  77.42  
[Epoch: 289] [Batch: 0551/0570] Loss: 0.10697  Avg Loss: 0.13891  Avg mIoU:  77.25  

*** Training [@Epoch 289] Avg Loss: 0.13967  Avg mIoU:  77.16  ***

[Epoch: 289] [Batch: 0001/0050] Loss: 0.12359  Avg Loss: 0.12359  Avg mIoU:  64.61  

*** Validation [@Epoch 289] Avg Loss: 0.15387  Avg mIoU:  65.21  ***

[Epoch: 290] [Batch: 0001/0570] Loss: 0.19347  Avg Loss: 0.19347  Avg mIoU:  33.13  
[Epoch: 290] [Batch: 0051/0570] Loss: 0.16657  Avg Loss: 0.13500  Avg mIoU:  77.36  
[Epoch: 290] [Batch: 0101/0570] Loss: 0.09806  Avg Loss: 0.13753  Avg mIoU:  77.03  
[Epoch: 290] [Batch: 0151/0570] Loss: 0.12916  Avg Loss: 0.13762  Avg mIoU:  77.79  
[Epoch: 290] [Batch: 0201/0570] Loss: 0.10163  Avg Loss: 0.13915  Avg mIoU:  77.76  
[Epoch: 290] [Batch: 0251/0570] Loss: 0.13385  Avg Loss: 0.13995  Avg mIoU:  77.50  
[Epoch: 290] [Batch: 0301/0570] Loss: 0.20132  Avg Loss: 0.14059  Avg mIoU:  77.27  
[Epoch: 290] [Batch: 0351/0570] Loss: 0.11281  Avg Loss: 0.14106  Avg mIoU:  77.25  
[Epoch: 290] [Batch: 0401/0570] Loss: 0.14317  Avg Loss: 0.14112  Avg mIoU:  77.19  
[Epoch: 290] [Batch: 0451/0570] Loss: 0.16325  Avg Loss: 0.14119  Avg mIoU:  77.11  
[Epoch: 290] [Batch: 0501/0570] Loss: 0.14626  Avg Loss: 0.13995  Avg mIoU:  77.32  
[Epoch: 290] [Batch: 0551/0570] Loss: 0.15350  Avg Loss: 0.13880  Avg mIoU:  77.43  

*** Training [@Epoch 290] Avg Loss: 0.13926  Avg mIoU:  77.36  ***

[Epoch: 290] [Batch: 0001/0050] Loss: 0.13010  Avg Loss: 0.13010  Avg mIoU:  65.78  

*** Validation [@Epoch 290] Avg Loss: 0.17596  Avg mIoU:  61.86  ***

[Epoch: 291] [Batch: 0001/0570] Loss: 0.14394  Avg Loss: 0.14394  Avg mIoU:  58.28  
[Epoch: 291] [Batch: 0051/0570] Loss: 0.12431  Avg Loss: 0.13369  Avg mIoU:  77.07  
[Epoch: 291] [Batch: 0101/0570] Loss: 0.21057  Avg Loss: 0.13753  Avg mIoU:  76.85  
[Epoch: 291] [Batch: 0151/0570] Loss: 0.18611  Avg Loss: 0.13945  Avg mIoU:  76.63  
[Epoch: 291] [Batch: 0201/0570] Loss: 0.11730  Avg Loss: 0.14001  Avg mIoU:  76.64  
[Epoch: 291] [Batch: 0251/0570] Loss: 0.11165  Avg Loss: 0.13973  Avg mIoU:  76.73  
[Epoch: 291] [Batch: 0301/0570] Loss: 0.19488  Avg Loss: 0.13928  Avg mIoU:  76.85  
[Epoch: 291] [Batch: 0351/0570] Loss: 0.16654  Avg Loss: 0.13947  Avg mIoU:  76.77  
[Epoch: 291] [Batch: 0401/0570] Loss: 0.11134  Avg Loss: 0.13837  Avg mIoU:  76.79  
[Epoch: 291] [Batch: 0451/0570] Loss: 0.09794  Avg Loss: 0.13840  Avg mIoU:  76.75  
[Epoch: 291] [Batch: 0501/0570] Loss: 0.23132  Avg Loss: 0.13857  Avg mIoU:  76.89  
[Epoch: 291] [Batch: 0551/0570] Loss: 0.15982  Avg Loss: 0.13917  Avg mIoU:  76.75  

*** Training [@Epoch 291] Avg Loss: 0.13963  Avg mIoU:  76.70  ***

[Epoch: 291] [Batch: 0001/0050] Loss: 0.12638  Avg Loss: 0.12638  Avg mIoU:  64.30  

*** Validation [@Epoch 291] Avg Loss: 0.16050  Avg mIoU:  64.16  ***

[Epoch: 292] [Batch: 0001/0570] Loss: 0.16141  Avg Loss: 0.16141  Avg mIoU:  28.27  
[Epoch: 292] [Batch: 0051/0570] Loss: 0.15004  Avg Loss: 0.14284  Avg mIoU:  75.00  
[Epoch: 292] [Batch: 0101/0570] Loss: 0.15697  Avg Loss: 0.14490  Avg mIoU:  74.96  
[Epoch: 292] [Batch: 0151/0570] Loss: 0.20403  Avg Loss: 0.14636  Avg mIoU:  76.00  
[Epoch: 292] [Batch: 0201/0570] Loss: 0.14938  Avg Loss: 0.14423  Avg mIoU:  76.41  
[Epoch: 292] [Batch: 0251/0570] Loss: 0.12218  Avg Loss: 0.14178  Avg mIoU:  76.68  
[Epoch: 292] [Batch: 0301/0570] Loss: 0.08937  Avg Loss: 0.14114  Avg mIoU:  76.98  
[Epoch: 292] [Batch: 0351/0570] Loss: 0.15720  Avg Loss: 0.14139  Avg mIoU:  76.89  
[Epoch: 292] [Batch: 0401/0570] Loss: 0.16238  Avg Loss: 0.14072  Avg mIoU:  76.79  
[Epoch: 292] [Batch: 0451/0570] Loss: 0.15153  Avg Loss: 0.14062  Avg mIoU:  76.80  
[Epoch: 292] [Batch: 0501/0570] Loss: 0.13903  Avg Loss: 0.14128  Avg mIoU:  76.64  
[Epoch: 292] [Batch: 0551/0570] Loss: 0.15438  Avg Loss: 0.14081  Avg mIoU:  76.66  

*** Training [@Epoch 292] Avg Loss: 0.14065  Avg mIoU:  76.73  ***

[Epoch: 292] [Batch: 0001/0050] Loss: 0.12044  Avg Loss: 0.12044  Avg mIoU:  67.65  

*** Validation [@Epoch 292] Avg Loss: 0.17650  Avg mIoU:  64.44  ***

[Epoch: 293] [Batch: 0001/0570] Loss: 0.13040  Avg Loss: 0.13040  Avg mIoU:  53.36  
[Epoch: 293] [Batch: 0051/0570] Loss: 0.11620  Avg Loss: 0.14302  Avg mIoU:  76.75  
[Epoch: 293] [Batch: 0101/0570] Loss: 0.18068  Avg Loss: 0.14006  Avg mIoU:  77.32  
[Epoch: 293] [Batch: 0151/0570] Loss: 0.13525  Avg Loss: 0.13786  Avg mIoU:  77.70  
[Epoch: 293] [Batch: 0201/0570] Loss: 0.14541  Avg Loss: 0.13627  Avg mIoU:  77.98  
[Epoch: 293] [Batch: 0251/0570] Loss: 0.15631  Avg Loss: 0.13965  Avg mIoU:  77.73  
[Epoch: 293] [Batch: 0301/0570] Loss: 0.16870  Avg Loss: 0.14025  Avg mIoU:  77.58  
[Epoch: 293] [Batch: 0351/0570] Loss: 0.10094  Avg Loss: 0.13913  Avg mIoU:  77.66  
[Epoch: 293] [Batch: 0401/0570] Loss: 0.10935  Avg Loss: 0.13935  Avg mIoU:  77.56  
[Epoch: 293] [Batch: 0451/0570] Loss: 0.12828  Avg Loss: 0.13848  Avg mIoU:  77.57  
[Epoch: 293] [Batch: 0501/0570] Loss: 0.16575  Avg Loss: 0.13754  Avg mIoU:  77.60  
[Epoch: 293] [Batch: 0551/0570] Loss: 0.17572  Avg Loss: 0.13728  Avg mIoU:  77.53  

*** Training [@Epoch 293] Avg Loss: 0.13732  Avg mIoU:  77.60  ***

[Epoch: 293] [Batch: 0001/0050] Loss: 0.13223  Avg Loss: 0.13223  Avg mIoU:  64.50  

*** Validation [@Epoch 293] Avg Loss: 0.16978  Avg mIoU:  62.14  ***

[Epoch: 294] [Batch: 0001/0570] Loss: 0.21611  Avg Loss: 0.21611  Avg mIoU:  24.57  
[Epoch: 294] [Batch: 0051/0570] Loss: 0.09824  Avg Loss: 0.13915  Avg mIoU:  78.61  
[Epoch: 294] [Batch: 0101/0570] Loss: 0.12393  Avg Loss: 0.13994  Avg mIoU:  77.28  
[Epoch: 294] [Batch: 0151/0570] Loss: 0.15676  Avg Loss: 0.14066  Avg mIoU:  77.04  
[Epoch: 294] [Batch: 0201/0570] Loss: 0.18389  Avg Loss: 0.14019  Avg mIoU:  76.81  
[Epoch: 294] [Batch: 0251/0570] Loss: 0.16579  Avg Loss: 0.14041  Avg mIoU:  76.91  
[Epoch: 294] [Batch: 0301/0570] Loss: 0.12443  Avg Loss: 0.13908  Avg mIoU:  77.27  
[Epoch: 294] [Batch: 0351/0570] Loss: 0.13805  Avg Loss: 0.13854  Avg mIoU:  77.51  
[Epoch: 294] [Batch: 0401/0570] Loss: 0.13086  Avg Loss: 0.13988  Avg mIoU:  77.25  
[Epoch: 294] [Batch: 0451/0570] Loss: 0.17389  Avg Loss: 0.13989  Avg mIoU:  77.18  
[Epoch: 294] [Batch: 0501/0570] Loss: 0.14992  Avg Loss: 0.13972  Avg mIoU:  77.21  
[Epoch: 294] [Batch: 0551/0570] Loss: 0.09402  Avg Loss: 0.13973  Avg mIoU:  77.11  

*** Training [@Epoch 294] Avg Loss: 0.13981  Avg mIoU:  77.15  ***

[Epoch: 294] [Batch: 0001/0050] Loss: 0.17071  Avg Loss: 0.17071  Avg mIoU:  62.41  

*** Validation [@Epoch 294] Avg Loss: 0.19800  Avg mIoU:  65.86  ***

[Epoch: 295] [Batch: 0001/0570] Loss: 0.10456  Avg Loss: 0.10456  Avg mIoU:  47.07  
[Epoch: 295] [Batch: 0051/0570] Loss: 0.09566  Avg Loss: 0.14009  Avg mIoU:  76.89  
[Epoch: 295] [Batch: 0101/0570] Loss: 0.16919  Avg Loss: 0.14182  Avg mIoU:  77.33  
[Epoch: 295] [Batch: 0151/0570] Loss: 0.20342  Avg Loss: 0.13996  Avg mIoU:  77.53  
[Epoch: 295] [Batch: 0201/0570] Loss: 0.09980  Avg Loss: 0.14100  Avg mIoU:  77.25  
[Epoch: 295] [Batch: 0251/0570] Loss: 0.16254  Avg Loss: 0.13981  Avg mIoU:  77.15  
[Epoch: 295] [Batch: 0301/0570] Loss: 0.12846  Avg Loss: 0.13927  Avg mIoU:  77.12  
[Epoch: 295] [Batch: 0351/0570] Loss: 0.16168  Avg Loss: 0.13937  Avg mIoU:  77.12  
[Epoch: 295] [Batch: 0401/0570] Loss: 0.14051  Avg Loss: 0.13919  Avg mIoU:  77.13  
[Epoch: 295] [Batch: 0451/0570] Loss: 0.10929  Avg Loss: 0.13935  Avg mIoU:  77.24  
[Epoch: 295] [Batch: 0501/0570] Loss: 0.12998  Avg Loss: 0.13855  Avg mIoU:  77.40  
[Epoch: 295] [Batch: 0551/0570] Loss: 0.15558  Avg Loss: 0.13897  Avg mIoU:  77.37  

*** Training [@Epoch 295] Avg Loss: 0.13843  Avg mIoU:  77.37  ***

[Epoch: 295] [Batch: 0001/0050] Loss: 0.12817  Avg Loss: 0.12817  Avg mIoU:  66.96  

*** Validation [@Epoch 295] Avg Loss: 0.16899  Avg mIoU:  65.63  ***

[Epoch: 296] [Batch: 0001/0570] Loss: 0.12669  Avg Loss: 0.12669  Avg mIoU:  43.55  
[Epoch: 296] [Batch: 0051/0570] Loss: 0.14152  Avg Loss: 0.13245  Avg mIoU:  78.26  
[Epoch: 296] [Batch: 0101/0570] Loss: 0.12172  Avg Loss: 0.13396  Avg mIoU:  77.91  
[Epoch: 296] [Batch: 0151/0570] Loss: 0.09839  Avg Loss: 0.13611  Avg mIoU:  78.23  
[Epoch: 296] [Batch: 0201/0570] Loss: 0.12718  Avg Loss: 0.13743  Avg mIoU:  77.91  
[Epoch: 296] [Batch: 0251/0570] Loss: 0.15177  Avg Loss: 0.13733  Avg mIoU:  77.81  
[Epoch: 296] [Batch: 0301/0570] Loss: 0.13479  Avg Loss: 0.13724  Avg mIoU:  77.47  
[Epoch: 296] [Batch: 0351/0570] Loss: 0.10992  Avg Loss: 0.13734  Avg mIoU:  77.41  
[Epoch: 296] [Batch: 0401/0570] Loss: 0.15114  Avg Loss: 0.13815  Avg mIoU:  77.19  
[Epoch: 296] [Batch: 0451/0570] Loss: 0.12727  Avg Loss: 0.13780  Avg mIoU:  77.24  
[Epoch: 296] [Batch: 0501/0570] Loss: 0.11744  Avg Loss: 0.13752  Avg mIoU:  77.11  
[Epoch: 296] [Batch: 0551/0570] Loss: 0.13040  Avg Loss: 0.13819  Avg mIoU:  77.18  

*** Training [@Epoch 296] Avg Loss: 0.13844  Avg mIoU:  77.15  ***

[Epoch: 296] [Batch: 0001/0050] Loss: 0.12554  Avg Loss: 0.12554  Avg mIoU:  65.44  

*** Validation [@Epoch 296] Avg Loss: 0.15562  Avg mIoU:  63.79  ***

[Epoch: 297] [Batch: 0001/0570] Loss: 0.12727  Avg Loss: 0.12727  Avg mIoU:  55.29  
[Epoch: 297] [Batch: 0051/0570] Loss: 0.09536  Avg Loss: 0.13025  Avg mIoU:  77.74  
[Epoch: 297] [Batch: 0101/0570] Loss: 0.22088  Avg Loss: 0.13387  Avg mIoU:  77.35  
[Epoch: 297] [Batch: 0151/0570] Loss: 0.14717  Avg Loss: 0.13385  Avg mIoU:  76.81  
[Epoch: 297] [Batch: 0201/0570] Loss: 0.11191  Avg Loss: 0.13652  Avg mIoU:  77.20  
[Epoch: 297] [Batch: 0251/0570] Loss: 0.16778  Avg Loss: 0.13600  Avg mIoU:  77.21  
[Epoch: 297] [Batch: 0301/0570] Loss: 0.19592  Avg Loss: 0.13729  Avg mIoU:  77.19  
[Epoch: 297] [Batch: 0351/0570] Loss: 0.11891  Avg Loss: 0.13722  Avg mIoU:  77.26  
[Epoch: 297] [Batch: 0401/0570] Loss: 0.14075  Avg Loss: 0.13685  Avg mIoU:  77.27  
[Epoch: 297] [Batch: 0451/0570] Loss: 0.15960  Avg Loss: 0.13749  Avg mIoU:  77.24  
[Epoch: 297] [Batch: 0501/0570] Loss: 0.19418  Avg Loss: 0.13899  Avg mIoU:  77.14  
[Epoch: 297] [Batch: 0551/0570] Loss: 0.13057  Avg Loss: 0.13892  Avg mIoU:  77.15  

*** Training [@Epoch 297] Avg Loss: 0.13887  Avg mIoU:  77.09  ***

[Epoch: 297] [Batch: 0001/0050] Loss: 0.12556  Avg Loss: 0.12556  Avg mIoU:  68.11  

*** Validation [@Epoch 297] Avg Loss: 0.17474  Avg mIoU:  61.81  ***

[Epoch: 298] [Batch: 0001/0570] Loss: 0.17796  Avg Loss: 0.17796  Avg mIoU:  50.09  
[Epoch: 298] [Batch: 0051/0570] Loss: 0.10928  Avg Loss: 0.13601  Avg mIoU:  77.38  
[Epoch: 298] [Batch: 0101/0570] Loss: 0.11798  Avg Loss: 0.13617  Avg mIoU:  77.12  
[Epoch: 298] [Batch: 0151/0570] Loss: 0.12125  Avg Loss: 0.13797  Avg mIoU:  77.27  
[Epoch: 298] [Batch: 0201/0570] Loss: 0.12513  Avg Loss: 0.13724  Avg mIoU:  77.45  
[Epoch: 298] [Batch: 0251/0570] Loss: 0.11500  Avg Loss: 0.13876  Avg mIoU:  77.64  
[Epoch: 298] [Batch: 0301/0570] Loss: 0.15365  Avg Loss: 0.13979  Avg mIoU:  77.40  
[Epoch: 298] [Batch: 0351/0570] Loss: 0.10109  Avg Loss: 0.13859  Avg mIoU:  77.34  
[Epoch: 298] [Batch: 0401/0570] Loss: 0.10213  Avg Loss: 0.13881  Avg mIoU:  77.27  
[Epoch: 298] [Batch: 0451/0570] Loss: 0.19673  Avg Loss: 0.13919  Avg mIoU:  77.10  
[Epoch: 298] [Batch: 0501/0570] Loss: 0.09790  Avg Loss: 0.13928  Avg mIoU:  77.14  
[Epoch: 298] [Batch: 0551/0570] Loss: 0.07767  Avg Loss: 0.13878  Avg mIoU:  77.28  

*** Training [@Epoch 298] Avg Loss: 0.13884  Avg mIoU:  77.27  ***

[Epoch: 298] [Batch: 0001/0050] Loss: 0.12383  Avg Loss: 0.12383  Avg mIoU:  68.45  

*** Validation [@Epoch 298] Avg Loss: 0.17220  Avg mIoU:  64.79  ***

[Epoch: 299] [Batch: 0001/0570] Loss: 0.11377  Avg Loss: 0.11377  Avg mIoU:  44.25  
[Epoch: 299] [Batch: 0051/0570] Loss: 0.16901  Avg Loss: 0.13709  Avg mIoU:  78.42  
[Epoch: 299] [Batch: 0101/0570] Loss: 0.14473  Avg Loss: 0.13556  Avg mIoU:  78.55  
[Epoch: 299] [Batch: 0151/0570] Loss: 0.18610  Avg Loss: 0.13395  Avg mIoU:  78.24  
[Epoch: 299] [Batch: 0201/0570] Loss: 0.09467  Avg Loss: 0.13473  Avg mIoU:  78.21  
[Epoch: 299] [Batch: 0251/0570] Loss: 0.13903  Avg Loss: 0.13513  Avg mIoU:  78.00  
[Epoch: 299] [Batch: 0301/0570] Loss: 0.10359  Avg Loss: 0.13634  Avg mIoU:  77.70  
[Epoch: 299] [Batch: 0351/0570] Loss: 0.12635  Avg Loss: 0.13739  Avg mIoU:  77.52  
[Epoch: 299] [Batch: 0401/0570] Loss: 0.09325  Avg Loss: 0.13701  Avg mIoU:  77.44  
[Epoch: 299] [Batch: 0451/0570] Loss: 0.14417  Avg Loss: 0.13795  Avg mIoU:  77.43  
[Epoch: 299] [Batch: 0501/0570] Loss: 0.13415  Avg Loss: 0.13753  Avg mIoU:  77.54  
[Epoch: 299] [Batch: 0551/0570] Loss: 0.11172  Avg Loss: 0.13749  Avg mIoU:  77.56  

*** Training [@Epoch 299] Avg Loss: 0.13736  Avg mIoU:  77.54  ***

[Epoch: 299] [Batch: 0001/0050] Loss: 0.13229  Avg Loss: 0.13229  Avg mIoU:  67.25  

*** Validation [@Epoch 299] Avg Loss: 0.17179  Avg mIoU:  64.33  ***

[Epoch: 300] [Batch: 0001/0570] Loss: 0.10267  Avg Loss: 0.10267  Avg mIoU:  42.57  
[Epoch: 300] [Batch: 0051/0570] Loss: 0.13304  Avg Loss: 0.13358  Avg mIoU:  75.94  
[Epoch: 300] [Batch: 0101/0570] Loss: 0.21180  Avg Loss: 0.13575  Avg mIoU:  76.97  
[Epoch: 300] [Batch: 0151/0570] Loss: 0.16609  Avg Loss: 0.13727  Avg mIoU:  76.61  
[Epoch: 300] [Batch: 0201/0570] Loss: 0.18590  Avg Loss: 0.13915  Avg mIoU:  76.89  
[Epoch: 300] [Batch: 0251/0570] Loss: 0.09893  Avg Loss: 0.13948  Avg mIoU:  77.01  
[Epoch: 300] [Batch: 0301/0570] Loss: 0.12731  Avg Loss: 0.13930  Avg mIoU:  76.84  
[Epoch: 300] [Batch: 0351/0570] Loss: 0.13724  Avg Loss: 0.13983  Avg mIoU:  76.77  
[Epoch: 300] [Batch: 0401/0570] Loss: 0.15630  Avg Loss: 0.13929  Avg mIoU:  76.76  
[Epoch: 300] [Batch: 0451/0570] Loss: 0.09801  Avg Loss: 0.13837  Avg mIoU:  77.10  
[Epoch: 300] [Batch: 0501/0570] Loss: 0.18080  Avg Loss: 0.13796  Avg mIoU:  77.34  
[Epoch: 300] [Batch: 0551/0570] Loss: 0.15985  Avg Loss: 0.13812  Avg mIoU:  77.34  

*** Training [@Epoch 300] Avg Loss: 0.13780  Avg mIoU:  77.45  ***

[Epoch: 300] [Batch: 0001/0050] Loss: 0.13209  Avg Loss: 0.13209  Avg mIoU:  66.95  

*** Validation [@Epoch 300] Avg Loss: 0.18587  Avg mIoU:  64.84  ***

[Epoch: 301] [Batch: 0001/0570] Loss: 0.11199  Avg Loss: 0.11199  Avg mIoU:  48.13  
[Epoch: 301] [Batch: 0051/0570] Loss: 0.15761  Avg Loss: 0.13125  Avg mIoU:  77.73  
[Epoch: 301] [Batch: 0101/0570] Loss: 0.11773  Avg Loss: 0.13204  Avg mIoU:  77.74  
[Epoch: 301] [Batch: 0151/0570] Loss: 0.14340  Avg Loss: 0.13316  Avg mIoU:  78.02  
[Epoch: 301] [Batch: 0201/0570] Loss: 0.13965  Avg Loss: 0.13542  Avg mIoU:  77.69  
[Epoch: 301] [Batch: 0251/0570] Loss: 0.12999  Avg Loss: 0.13475  Avg mIoU:  77.30  
[Epoch: 301] [Batch: 0301/0570] Loss: 0.16058  Avg Loss: 0.13505  Avg mIoU:  77.69  
[Epoch: 301] [Batch: 0351/0570] Loss: 0.19147  Avg Loss: 0.13644  Avg mIoU:  77.52  
[Epoch: 301] [Batch: 0401/0570] Loss: 0.12126  Avg Loss: 0.13601  Avg mIoU:  77.48  
[Epoch: 301] [Batch: 0451/0570] Loss: 0.14323  Avg Loss: 0.13644  Avg mIoU:  77.41  
[Epoch: 301] [Batch: 0501/0570] Loss: 0.10839  Avg Loss: 0.13705  Avg mIoU:  77.33  
[Epoch: 301] [Batch: 0551/0570] Loss: 0.14634  Avg Loss: 0.13734  Avg mIoU:  77.23  

*** Training [@Epoch 301] Avg Loss: 0.13741  Avg mIoU:  77.24  ***

[Epoch: 301] [Batch: 0001/0050] Loss: 0.12619  Avg Loss: 0.12619  Avg mIoU:  68.40  

*** Validation [@Epoch 301] Avg Loss: 0.17014  Avg mIoU:  65.23  ***

[Epoch: 302] [Batch: 0001/0570] Loss: 0.11096  Avg Loss: 0.11096  Avg mIoU:  53.33  
[Epoch: 302] [Batch: 0051/0570] Loss: 0.13459  Avg Loss: 0.13423  Avg mIoU:  77.39  
[Epoch: 302] [Batch: 0101/0570] Loss: 0.17493  Avg Loss: 0.13927  Avg mIoU:  76.90  
[Epoch: 302] [Batch: 0151/0570] Loss: 0.10326  Avg Loss: 0.14017  Avg mIoU:  77.33  
[Epoch: 302] [Batch: 0201/0570] Loss: 0.21033  Avg Loss: 0.14025  Avg mIoU:  77.43  
[Epoch: 302] [Batch: 0251/0570] Loss: 0.13026  Avg Loss: 0.14087  Avg mIoU:  77.26  
[Epoch: 302] [Batch: 0301/0570] Loss: 0.14091  Avg Loss: 0.13919  Avg mIoU:  77.25  
[Epoch: 302] [Batch: 0351/0570] Loss: 0.15141  Avg Loss: 0.13898  Avg mIoU:  77.29  
[Epoch: 302] [Batch: 0401/0570] Loss: 0.11812  Avg Loss: 0.13825  Avg mIoU:  77.46  
[Epoch: 302] [Batch: 0451/0570] Loss: 0.13807  Avg Loss: 0.13859  Avg mIoU:  77.38  
[Epoch: 302] [Batch: 0501/0570] Loss: 0.13847  Avg Loss: 0.13808  Avg mIoU:  77.53  
[Epoch: 302] [Batch: 0551/0570] Loss: 0.10808  Avg Loss: 0.13836  Avg mIoU:  77.37  

*** Training [@Epoch 302] Avg Loss: 0.13828  Avg mIoU:  77.33  ***

[Epoch: 302] [Batch: 0001/0050] Loss: 0.14100  Avg Loss: 0.14100  Avg mIoU:  67.53  

*** Validation [@Epoch 302] Avg Loss: 0.19347  Avg mIoU:  64.33  ***

[Epoch: 303] [Batch: 0001/0570] Loss: 0.11284  Avg Loss: 0.11284  Avg mIoU:  53.29  
[Epoch: 303] [Batch: 0051/0570] Loss: 0.16921  Avg Loss: 0.14296  Avg mIoU:  78.16  
[Epoch: 303] [Batch: 0101/0570] Loss: 0.14336  Avg Loss: 0.14234  Avg mIoU:  77.47  
[Epoch: 303] [Batch: 0151/0570] Loss: 0.10061  Avg Loss: 0.14049  Avg mIoU:  77.35  
[Epoch: 303] [Batch: 0201/0570] Loss: 0.13979  Avg Loss: 0.13895  Avg mIoU:  77.19  
[Epoch: 303] [Batch: 0251/0570] Loss: 0.08977  Avg Loss: 0.13990  Avg mIoU:  77.22  
[Epoch: 303] [Batch: 0301/0570] Loss: 0.09174  Avg Loss: 0.13776  Avg mIoU:  77.32  
[Epoch: 303] [Batch: 0351/0570] Loss: 0.10415  Avg Loss: 0.13758  Avg mIoU:  77.40  
[Epoch: 303] [Batch: 0401/0570] Loss: 0.17317  Avg Loss: 0.13768  Avg mIoU:  77.60  
[Epoch: 303] [Batch: 0451/0570] Loss: 0.14313  Avg Loss: 0.13755  Avg mIoU:  77.51  
[Epoch: 303] [Batch: 0501/0570] Loss: 0.13344  Avg Loss: 0.13684  Avg mIoU:  77.59  
[Epoch: 303] [Batch: 0551/0570] Loss: 0.15786  Avg Loss: 0.13713  Avg mIoU:  77.58  

*** Training [@Epoch 303] Avg Loss: 0.13718  Avg mIoU:  77.59  ***

[Epoch: 303] [Batch: 0001/0050] Loss: 0.15473  Avg Loss: 0.15473  Avg mIoU:  64.88  

*** Validation [@Epoch 303] Avg Loss: 0.19530  Avg mIoU:  64.98  ***

[Epoch: 304] [Batch: 0001/0570] Loss: 0.14303  Avg Loss: 0.14303  Avg mIoU:  35.71  
[Epoch: 304] [Batch: 0051/0570] Loss: 0.09560  Avg Loss: 0.12980  Avg mIoU:  77.43  
[Epoch: 304] [Batch: 0101/0570] Loss: 0.14001  Avg Loss: 0.13090  Avg mIoU:  77.60  
[Epoch: 304] [Batch: 0151/0570] Loss: 0.14832  Avg Loss: 0.13198  Avg mIoU:  77.75  
[Epoch: 304] [Batch: 0201/0570] Loss: 0.15081  Avg Loss: 0.13084  Avg mIoU:  77.98  
[Epoch: 304] [Batch: 0251/0570] Loss: 0.13889  Avg Loss: 0.13213  Avg mIoU:  77.94  
[Epoch: 304] [Batch: 0301/0570] Loss: 0.14385  Avg Loss: 0.13281  Avg mIoU:  78.08  
[Epoch: 304] [Batch: 0351/0570] Loss: 0.10160  Avg Loss: 0.13501  Avg mIoU:  77.81  
[Epoch: 304] [Batch: 0401/0570] Loss: 0.11318  Avg Loss: 0.13515  Avg mIoU:  77.63  
[Epoch: 304] [Batch: 0451/0570] Loss: 0.15086  Avg Loss: 0.13608  Avg mIoU:  77.58  
[Epoch: 304] [Batch: 0501/0570] Loss: 0.14915  Avg Loss: 0.13577  Avg mIoU:  77.61  
[Epoch: 304] [Batch: 0551/0570] Loss: 0.13599  Avg Loss: 0.13520  Avg mIoU:  77.80  

*** Training [@Epoch 304] Avg Loss: 0.13548  Avg mIoU:  77.77  ***

[Epoch: 304] [Batch: 0001/0050] Loss: 0.11397  Avg Loss: 0.11397  Avg mIoU:  66.86  

*** Validation [@Epoch 304] Avg Loss: 0.15440  Avg mIoU:  64.61  ***

[Epoch: 305] [Batch: 0001/0570] Loss: 0.17577  Avg Loss: 0.17577  Avg mIoU:  54.62  
[Epoch: 305] [Batch: 0051/0570] Loss: 0.36410  Avg Loss: 0.14451  Avg mIoU:  77.75  
[Epoch: 305] [Batch: 0101/0570] Loss: 0.10830  Avg Loss: 0.14050  Avg mIoU:  77.68  
[Epoch: 305] [Batch: 0151/0570] Loss: 0.12914  Avg Loss: 0.13648  Avg mIoU:  77.86  
[Epoch: 305] [Batch: 0201/0570] Loss: 0.11253  Avg Loss: 0.13696  Avg mIoU:  77.75  
[Epoch: 305] [Batch: 0251/0570] Loss: 0.21547  Avg Loss: 0.13734  Avg mIoU:  77.81  
[Epoch: 305] [Batch: 0301/0570] Loss: 0.12362  Avg Loss: 0.13783  Avg mIoU:  77.81  
[Epoch: 305] [Batch: 0351/0570] Loss: 0.13576  Avg Loss: 0.13764  Avg mIoU:  77.72  
[Epoch: 305] [Batch: 0401/0570] Loss: 0.09810  Avg Loss: 0.13684  Avg mIoU:  77.80  
[Epoch: 305] [Batch: 0451/0570] Loss: 0.12773  Avg Loss: 0.13707  Avg mIoU:  77.57  
[Epoch: 305] [Batch: 0501/0570] Loss: 0.14832  Avg Loss: 0.13762  Avg mIoU:  77.52  
[Epoch: 305] [Batch: 0551/0570] Loss: 0.14710  Avg Loss: 0.13756  Avg mIoU:  77.29  

*** Training [@Epoch 305] Avg Loss: 0.13781  Avg mIoU:  77.27  ***

[Epoch: 305] [Batch: 0001/0050] Loss: 0.14921  Avg Loss: 0.14921  Avg mIoU:  64.06  

*** Validation [@Epoch 305] Avg Loss: 0.16915  Avg mIoU:  64.58  ***

[Epoch: 306] [Batch: 0001/0570] Loss: 0.13882  Avg Loss: 0.13882  Avg mIoU:  44.98  
[Epoch: 306] [Batch: 0051/0570] Loss: 0.11474  Avg Loss: 0.14383  Avg mIoU:  75.52  
[Epoch: 306] [Batch: 0101/0570] Loss: 0.14092  Avg Loss: 0.13688  Avg mIoU:  77.10  
[Epoch: 306] [Batch: 0151/0570] Loss: 0.13624  Avg Loss: 0.13741  Avg mIoU:  76.29  
[Epoch: 306] [Batch: 0201/0570] Loss: 0.14019  Avg Loss: 0.13629  Avg mIoU:  76.92  
[Epoch: 306] [Batch: 0251/0570] Loss: 0.07719  Avg Loss: 0.13760  Avg mIoU:  77.02  
[Epoch: 306] [Batch: 0301/0570] Loss: 0.18101  Avg Loss: 0.13622  Avg mIoU:  77.00  
[Epoch: 306] [Batch: 0351/0570] Loss: 0.14615  Avg Loss: 0.13626  Avg mIoU:  77.06  
[Epoch: 306] [Batch: 0401/0570] Loss: 0.13490  Avg Loss: 0.13598  Avg mIoU:  77.37  
[Epoch: 306] [Batch: 0451/0570] Loss: 0.11383  Avg Loss: 0.13590  Avg mIoU:  77.42  
[Epoch: 306] [Batch: 0501/0570] Loss: 0.11379  Avg Loss: 0.13621  Avg mIoU:  77.44  
[Epoch: 306] [Batch: 0551/0570] Loss: 0.14104  Avg Loss: 0.13684  Avg mIoU:  77.38  

*** Training [@Epoch 306] Avg Loss: 0.13705  Avg mIoU:  77.34  ***

[Epoch: 306] [Batch: 0001/0050] Loss: 0.13688  Avg Loss: 0.13688  Avg mIoU:  65.34  

*** Validation [@Epoch 306] Avg Loss: 0.17135  Avg mIoU:  65.45  ***

[Epoch: 307] [Batch: 0001/0570] Loss: 0.13951  Avg Loss: 0.13951  Avg mIoU:  52.22  
[Epoch: 307] [Batch: 0051/0570] Loss: 0.17391  Avg Loss: 0.13939  Avg mIoU:  78.16  
[Epoch: 307] [Batch: 0101/0570] Loss: 0.14195  Avg Loss: 0.13500  Avg mIoU:  78.19  
[Epoch: 307] [Batch: 0151/0570] Loss: 0.16010  Avg Loss: 0.13534  Avg mIoU:  78.44  
[Epoch: 307] [Batch: 0201/0570] Loss: 0.12234  Avg Loss: 0.13514  Avg mIoU:  77.99  
[Epoch: 307] [Batch: 0251/0570] Loss: 0.13438  Avg Loss: 0.13596  Avg mIoU:  77.75  
[Epoch: 307] [Batch: 0301/0570] Loss: 0.16352  Avg Loss: 0.13561  Avg mIoU:  77.71  
[Epoch: 307] [Batch: 0351/0570] Loss: 0.08067  Avg Loss: 0.13592  Avg mIoU:  77.71  
[Epoch: 307] [Batch: 0401/0570] Loss: 0.11055  Avg Loss: 0.13575  Avg mIoU:  77.95  
[Epoch: 307] [Batch: 0451/0570] Loss: 0.14212  Avg Loss: 0.13620  Avg mIoU:  77.81  
[Epoch: 307] [Batch: 0501/0570] Loss: 0.15502  Avg Loss: 0.13694  Avg mIoU:  77.77  
[Epoch: 307] [Batch: 0551/0570] Loss: 0.14344  Avg Loss: 0.13704  Avg mIoU:  77.74  

*** Training [@Epoch 307] Avg Loss: 0.13677  Avg mIoU:  77.77  ***

[Epoch: 307] [Batch: 0001/0050] Loss: 0.12751  Avg Loss: 0.12751  Avg mIoU:  67.47  

*** Validation [@Epoch 307] Avg Loss: 0.17571  Avg mIoU:  64.77  ***

[Epoch: 308] [Batch: 0001/0570] Loss: 0.15688  Avg Loss: 0.15688  Avg mIoU:  39.51  
[Epoch: 308] [Batch: 0051/0570] Loss: 0.11546  Avg Loss: 0.13684  Avg mIoU:  77.43  
[Epoch: 308] [Batch: 0101/0570] Loss: 0.14082  Avg Loss: 0.13573  Avg mIoU:  77.44  
[Epoch: 308] [Batch: 0151/0570] Loss: 0.13821  Avg Loss: 0.13878  Avg mIoU:  76.57  
[Epoch: 308] [Batch: 0201/0570] Loss: 0.13669  Avg Loss: 0.13934  Avg mIoU:  76.74  
[Epoch: 308] [Batch: 0251/0570] Loss: 0.11494  Avg Loss: 0.14008  Avg mIoU:  76.83  
[Epoch: 308] [Batch: 0301/0570] Loss: 0.15615  Avg Loss: 0.14016  Avg mIoU:  76.74  
[Epoch: 308] [Batch: 0351/0570] Loss: 0.08937  Avg Loss: 0.13820  Avg mIoU:  76.92  
[Epoch: 308] [Batch: 0401/0570] Loss: 0.11558  Avg Loss: 0.13705  Avg mIoU:  77.10  
[Epoch: 308] [Batch: 0451/0570] Loss: 0.11651  Avg Loss: 0.13617  Avg mIoU:  77.42  
[Epoch: 308] [Batch: 0501/0570] Loss: 0.14840  Avg Loss: 0.13652  Avg mIoU:  77.42  
[Epoch: 308] [Batch: 0551/0570] Loss: 0.11407  Avg Loss: 0.13639  Avg mIoU:  77.56  

*** Training [@Epoch 308] Avg Loss: 0.13619  Avg mIoU:  77.59  ***

[Epoch: 308] [Batch: 0001/0050] Loss: 0.14219  Avg Loss: 0.14219  Avg mIoU:  67.28  

*** Validation [@Epoch 308] Avg Loss: 0.19805  Avg mIoU:  64.26  ***

[Epoch: 309] [Batch: 0001/0570] Loss: 0.13336  Avg Loss: 0.13336  Avg mIoU:  51.49  
[Epoch: 309] [Batch: 0051/0570] Loss: 0.39367  Avg Loss: 0.14044  Avg mIoU:  77.85  
[Epoch: 309] [Batch: 0101/0570] Loss: 0.14562  Avg Loss: 0.14062  Avg mIoU:  77.99  
[Epoch: 309] [Batch: 0151/0570] Loss: 0.10038  Avg Loss: 0.14085  Avg mIoU:  77.63  
[Epoch: 309] [Batch: 0201/0570] Loss: 0.15027  Avg Loss: 0.13975  Avg mIoU:  77.50  
[Epoch: 309] [Batch: 0251/0570] Loss: 0.13003  Avg Loss: 0.13752  Avg mIoU:  77.26  
[Epoch: 309] [Batch: 0301/0570] Loss: 0.15475  Avg Loss: 0.13804  Avg mIoU:  77.07  
[Epoch: 309] [Batch: 0351/0570] Loss: 0.10665  Avg Loss: 0.13700  Avg mIoU:  77.30  
[Epoch: 309] [Batch: 0401/0570] Loss: 0.15677  Avg Loss: 0.13622  Avg mIoU:  77.36  
[Epoch: 309] [Batch: 0451/0570] Loss: 0.14289  Avg Loss: 0.13633  Avg mIoU:  77.33  
[Epoch: 309] [Batch: 0501/0570] Loss: 0.16610  Avg Loss: 0.13622  Avg mIoU:  77.42  
[Epoch: 309] [Batch: 0551/0570] Loss: 0.09431  Avg Loss: 0.13565  Avg mIoU:  77.62  

*** Training [@Epoch 309] Avg Loss: 0.13558  Avg mIoU:  77.65  ***

[Epoch: 309] [Batch: 0001/0050] Loss: 0.13611  Avg Loss: 0.13611  Avg mIoU:  67.35  

*** Validation [@Epoch 309] Avg Loss: 0.20482  Avg mIoU:  64.76  ***

[Epoch: 310] [Batch: 0001/0570] Loss: 0.16944  Avg Loss: 0.16944  Avg mIoU:  31.99  
[Epoch: 310] [Batch: 0051/0570] Loss: 0.11678  Avg Loss: 0.13934  Avg mIoU:  76.80  
[Epoch: 310] [Batch: 0101/0570] Loss: 0.18570  Avg Loss: 0.13990  Avg mIoU:  76.71  
[Epoch: 310] [Batch: 0151/0570] Loss: 0.13399  Avg Loss: 0.14005  Avg mIoU:  76.56  
[Epoch: 310] [Batch: 0201/0570] Loss: 0.13407  Avg Loss: 0.13895  Avg mIoU:  76.95  
[Epoch: 310] [Batch: 0251/0570] Loss: 0.18479  Avg Loss: 0.13957  Avg mIoU:  77.11  
[Epoch: 310] [Batch: 0301/0570] Loss: 0.15394  Avg Loss: 0.13865  Avg mIoU:  77.31  
[Epoch: 310] [Batch: 0351/0570] Loss: 0.15317  Avg Loss: 0.13854  Avg mIoU:  77.51  
[Epoch: 310] [Batch: 0401/0570] Loss: 0.12133  Avg Loss: 0.13831  Avg mIoU:  77.42  
[Epoch: 310] [Batch: 0451/0570] Loss: 0.15002  Avg Loss: 0.13793  Avg mIoU:  77.42  
[Epoch: 310] [Batch: 0501/0570] Loss: 0.10447  Avg Loss: 0.13730  Avg mIoU:  77.41  
[Epoch: 310] [Batch: 0551/0570] Loss: 0.16178  Avg Loss: 0.13714  Avg mIoU:  77.43  

*** Training [@Epoch 310] Avg Loss: 0.13708  Avg mIoU:  77.40  ***

[Epoch: 310] [Batch: 0001/0050] Loss: 0.13811  Avg Loss: 0.13811  Avg mIoU:  65.72  

*** Validation [@Epoch 310] Avg Loss: 0.18373  Avg mIoU:  63.65  ***

[Epoch: 311] [Batch: 0001/0570] Loss: 0.10860  Avg Loss: 0.10860  Avg mIoU:  61.03  
